{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from torch._functorch.aot_autograd import aot_module_simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 15 files: 100%|██████████| 15/15 [00:00<00:00, 17100.99it/s]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float16\n",
    ")\n",
    "pipe.safety_checker = None\n",
    "pipe.feature_extrator = None\n",
    "pipe = pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backend(gm: torch.fx.GraphModule, sample_inp):\n",
    "    def compiler(gm, sample_inp):\n",
    "        gm.print_readable()\n",
    "        return gm.forward\n",
    "    return aot_module_simplified(gm, sample_inp, fw_compiler=compiler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch._dynamo.reset()\n",
    "\n",
    "@torch.compile(backend=backend)\n",
    "def inference_func(promt):\n",
    "    image = pipe(prompt, num_inference_steps=10).images[0]\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[49408, 768], arg1_1: f16[77, 768], arg2_1: f16[768], arg3_1: f16[768], arg4_1: f16[768, 768], arg5_1: f16[768], arg6_1: f16[768, 768], arg7_1: f16[768], arg8_1: f16[768, 768], arg9_1: f16[768], arg10_1: f16[768, 768], arg11_1: f16[768], arg12_1: f16[768], arg13_1: f16[768], arg14_1: f16[3072, 768], arg15_1: f16[3072], arg16_1: f16[768, 3072], arg17_1: f16[768], arg18_1: f16[768], arg19_1: f16[768], arg20_1: f16[768, 768], arg21_1: f16[768], arg22_1: f16[768, 768], arg23_1: f16[768], arg24_1: f16[768, 768], arg25_1: f16[768], arg26_1: f16[768, 768], arg27_1: f16[768], arg28_1: f16[768], arg29_1: f16[768], arg30_1: f16[3072, 768], arg31_1: f16[3072], arg32_1: f16[768, 3072], arg33_1: f16[768], arg34_1: f16[768], arg35_1: f16[768], arg36_1: f16[768, 768], arg37_1: f16[768], arg38_1: f16[768, 768], arg39_1: f16[768], arg40_1: f16[768, 768], arg41_1: f16[768], arg42_1: f16[768, 768], arg43_1: f16[768], arg44_1: f16[768], arg45_1: f16[768], arg46_1: f16[3072, 768], arg47_1: f16[3072], arg48_1: f16[768, 3072], arg49_1: f16[768], arg50_1: f16[768], arg51_1: f16[768], arg52_1: f16[768, 768], arg53_1: f16[768], arg54_1: f16[768, 768], arg55_1: f16[768], arg56_1: f16[768, 768], arg57_1: f16[768], arg58_1: f16[768, 768], arg59_1: f16[768], arg60_1: f16[768], arg61_1: f16[768], arg62_1: f16[3072, 768], arg63_1: f16[3072], arg64_1: f16[768, 3072], arg65_1: f16[768], arg66_1: f16[768], arg67_1: f16[768], arg68_1: f16[768, 768], arg69_1: f16[768], arg70_1: f16[768, 768], arg71_1: f16[768], arg72_1: f16[768, 768], arg73_1: f16[768], arg74_1: f16[768, 768], arg75_1: f16[768], arg76_1: f16[768], arg77_1: f16[768], arg78_1: f16[3072, 768], arg79_1: f16[3072], arg80_1: f16[768, 3072], arg81_1: f16[768], arg82_1: f16[768], arg83_1: f16[768], arg84_1: f16[768, 768], arg85_1: f16[768], arg86_1: f16[768, 768], arg87_1: f16[768], arg88_1: f16[768, 768], arg89_1: f16[768], arg90_1: f16[768, 768], arg91_1: f16[768], arg92_1: f16[768], arg93_1: f16[768], arg94_1: f16[3072, 768], arg95_1: f16[3072], arg96_1: f16[768, 3072], arg97_1: f16[768], arg98_1: f16[768], arg99_1: f16[768], arg100_1: f16[768, 768], arg101_1: f16[768], arg102_1: f16[768, 768], arg103_1: f16[768], arg104_1: f16[768, 768], arg105_1: f16[768], arg106_1: f16[768, 768], arg107_1: f16[768], arg108_1: f16[768], arg109_1: f16[768], arg110_1: f16[3072, 768], arg111_1: f16[3072], arg112_1: f16[768, 3072], arg113_1: f16[768], arg114_1: f16[768], arg115_1: f16[768], arg116_1: f16[768, 768], arg117_1: f16[768], arg118_1: f16[768, 768], arg119_1: f16[768], arg120_1: f16[768, 768], arg121_1: f16[768], arg122_1: f16[768, 768], arg123_1: f16[768], arg124_1: f16[768], arg125_1: f16[768], arg126_1: f16[3072, 768], arg127_1: f16[3072], arg128_1: f16[768, 3072], arg129_1: f16[768], arg130_1: f16[768], arg131_1: f16[768], arg132_1: f16[768, 768], arg133_1: f16[768], arg134_1: f16[768, 768], arg135_1: f16[768], arg136_1: f16[768, 768], arg137_1: f16[768], arg138_1: f16[768, 768], arg139_1: f16[768], arg140_1: f16[768], arg141_1: f16[768], arg142_1: f16[3072, 768], arg143_1: f16[3072], arg144_1: f16[768, 3072], arg145_1: f16[768], arg146_1: f16[768], arg147_1: f16[768], arg148_1: f16[768, 768], arg149_1: f16[768], arg150_1: f16[768, 768], arg151_1: f16[768], arg152_1: f16[768, 768], arg153_1: f16[768], arg154_1: f16[768, 768], arg155_1: f16[768], arg156_1: f16[768], arg157_1: f16[768], arg158_1: f16[3072, 768], arg159_1: f16[3072], arg160_1: f16[768, 3072], arg161_1: f16[768], arg162_1: f16[768], arg163_1: f16[768], arg164_1: f16[768, 768], arg165_1: f16[768], arg166_1: f16[768, 768], arg167_1: f16[768], arg168_1: f16[768, 768], arg169_1: f16[768], arg170_1: f16[768, 768], arg171_1: f16[768], arg172_1: f16[768], arg173_1: f16[768], arg174_1: f16[3072, 768], arg175_1: f16[3072], arg176_1: f16[768, 3072], arg177_1: f16[768], arg178_1: f16[768], arg179_1: f16[768], arg180_1: f16[768, 768], arg181_1: f16[768], arg182_1: f16[768, 768], arg183_1: f16[768], arg184_1: f16[768, 768], arg185_1: f16[768], arg186_1: f16[768, 768], arg187_1: f16[768], arg188_1: f16[768], arg189_1: f16[768], arg190_1: f16[3072, 768], arg191_1: f16[3072], arg192_1: f16[768, 3072], arg193_1: f16[768], arg194_1: f16[768], arg195_1: f16[768], arg196_1: i64[1, 77], arg197_1: i64[1, 77]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:266, code: text_input_ids.to(device),\n",
      "        _to_copy: i64[1, 77] = torch.ops.aten._to_copy.default(arg197_1, dtype = torch.int64, layout = torch.strided, device = device(type='cuda', index=0));  arg197_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:706, code: input_ids = input_ids.view(-1, input_shape[-1])\n",
      "        view: i64[1, 77] = torch.ops.aten.view.default(_to_copy, [-1, 77]);  _to_copy = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:220, code: position_ids = self.position_ids[:, :seq_length]\n",
      "        slice_1: i64[1, 77] = torch.ops.aten.slice.Tensor(arg196_1, 0, 0, 9223372036854775807);  arg196_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:223, code: inputs_embeds = self.token_embedding(input_ids)\n",
      "        embedding: f16[1, 77, 768] = torch.ops.aten.embedding.default(arg0_1, view);  arg0_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:225, code: position_embeddings = self.position_embedding(position_ids)\n",
      "        embedding_1: f16[1, 77, 768] = torch.ops.aten.embedding.default(arg1_1, slice_1);  arg1_1 = slice_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:226, code: embeddings = inputs_embeds + position_embeddings\n",
      "        add: f16[1, 77, 768] = torch.ops.aten.add.Tensor(embedding, embedding_1);  embedding = embedding_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:753, code: mask = torch.empty(bsz, seq_len, seq_len, dtype=dtype)\n",
      "        empty: f16[1, 77, 77] = torch.ops.aten.empty.memory_format([1, 77, 77], dtype = torch.float16, device = device(type='cpu'), pin_memory = False)\n",
      "        \n",
      "        # No stacktrace found for following nodes\n",
      "        _tensor_constant0 = self._tensor_constant0\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:754, code: mask.fill_(torch.tensor(torch.finfo(dtype).min))\n",
      "        lift_fresh_copy: f32[] = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None\n",
      "        fill_: f16[1, 77, 77] = torch.ops.aten.fill_.Tensor(empty, lift_fresh_copy);  empty = lift_fresh_copy = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:755, code: mask.triu_(1)  # zero out the lower diagonal\n",
      "        triu_: f16[1, 77, 77] = torch.ops.aten.triu_.default(fill_, 1);  fill_ = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:756, code: mask = mask.unsqueeze(1)  # expand mask\n",
      "        unsqueeze: f16[1, 1, 77, 77] = torch.ops.aten.unsqueeze.default(triu_, 1);  triu_ = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:713, code: causal_attention_mask = self._build_causal_attention_mask(bsz, seq_len, hidden_states.dtype).to(\n",
      "        _to_copy_1: f16[1, 1, 77, 77] = torch.ops.aten._to_copy.default(unsqueeze, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0));  unsqueeze = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm = torch.ops.aten.native_layer_norm.default(add, [768], arg2_1, arg3_1, 1e-05);  arg2_1 = arg3_1 = None\n",
      "        getitem: f16[1, 77, 768] = native_layer_norm[0]\n",
      "        getitem_1: f32[1, 77, 1] = native_layer_norm[1]\n",
      "        getitem_2: f32[1, 77, 1] = native_layer_norm[2];  native_layer_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t: f16[768, 768] = torch.ops.aten.t.default(arg4_1);  arg4_1 = None\n",
      "        view_1: f16[77, 768] = torch.ops.aten.view.default(getitem, [77, 768])\n",
      "        addmm: f16[77, 768] = torch.ops.aten.addmm.default(arg5_1, view_1, t);  arg5_1 = view_1 = t = None\n",
      "        view_2: f16[1, 77, 768] = torch.ops.aten.view.default(addmm, [1, 77, 768]);  addmm = None\n",
      "        mul: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_2, 0.125);  view_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_1: f16[768, 768] = torch.ops.aten.t.default(arg6_1);  arg6_1 = None\n",
      "        view_3: f16[77, 768] = torch.ops.aten.view.default(getitem, [77, 768])\n",
      "        addmm_1: f16[77, 768] = torch.ops.aten.addmm.default(arg7_1, view_3, t_1);  arg7_1 = view_3 = t_1 = None\n",
      "        view_4: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_1, [1, 77, 768]);  addmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_5: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_4, [1, -1, 12, 64]);  view_4 = None\n",
      "        transpose: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_5, 1, 2);  view_5 = None\n",
      "        clone: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose, memory_format = torch.contiguous_format);  transpose = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_2: f16[768, 768] = torch.ops.aten.t.default(arg8_1);  arg8_1 = None\n",
      "        view_6: f16[77, 768] = torch.ops.aten.view.default(getitem, [77, 768]);  getitem = None\n",
      "        addmm_2: f16[77, 768] = torch.ops.aten.addmm.default(arg9_1, view_6, t_2);  arg9_1 = view_6 = t_2 = None\n",
      "        view_7: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_2, [1, 77, 768]);  addmm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_8: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_7, [1, -1, 12, 64]);  view_7 = None\n",
      "        transpose_1: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_8, 1, 2);  view_8 = None\n",
      "        clone_1: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_1, memory_format = torch.contiguous_format);  transpose_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_9: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul, [1, 77, 12, 64]);  mul = None\n",
      "        transpose_2: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_9, 1, 2);  view_9 = None\n",
      "        clone_2: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_2, memory_format = torch.contiguous_format);  transpose_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_10: f16[12, 77, 64] = torch.ops.aten.view.default(clone_2, [12, -1, 64]);  clone_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_11: f16[12, 77, 64] = torch.ops.aten.view.default(clone, [12, -1, 64]);  clone = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_12: f16[12, 77, 64] = torch.ops.aten.view.default(clone_1, [12, -1, 64]);  clone_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_3: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_11, 1, 2);  view_11 = None\n",
      "        bmm: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_10, transpose_3);  view_10 = transpose_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_13: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm, [1, 12, 77, 77]);  bmm = None\n",
      "        add_1: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_13, _to_copy_1);  view_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_14: f16[12, 77, 77] = torch.ops.aten.view.default(add_1, [12, 77, 77]);  add_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_14, -1, False);  view_14 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_1: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax, view_12);  _softmax = view_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_15: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_1, [1, 12, 77, 64]);  bmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_4: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_15, 1, 2);  view_15 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_3: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_4, memory_format = torch.contiguous_format);  transpose_4 = None\n",
      "        _unsafe_view: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_3, [1, 77, 768]);  clone_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_3: f16[768, 768] = torch.ops.aten.t.default(arg10_1);  arg10_1 = None\n",
      "        view_16: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view, [77, 768]);  _unsafe_view = None\n",
      "        addmm_3: f16[77, 768] = torch.ops.aten.addmm.default(arg11_1, view_16, t_3);  arg11_1 = view_16 = t_3 = None\n",
      "        view_17: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_3, [1, 77, 768]);  addmm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_2: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add, view_17);  add = view_17 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_1 = torch.ops.aten.native_layer_norm.default(add_2, [768], arg12_1, arg13_1, 1e-05);  arg12_1 = arg13_1 = None\n",
      "        getitem_3: f16[1, 77, 768] = native_layer_norm_1[0]\n",
      "        getitem_4: f32[1, 77, 1] = native_layer_norm_1[1]\n",
      "        getitem_5: f32[1, 77, 1] = native_layer_norm_1[2];  native_layer_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_4: f16[768, 3072] = torch.ops.aten.t.default(arg14_1);  arg14_1 = None\n",
      "        view_18: f16[77, 768] = torch.ops.aten.view.default(getitem_3, [77, 768]);  getitem_3 = None\n",
      "        addmm_4: f16[77, 3072] = torch.ops.aten.addmm.default(arg15_1, view_18, t_4);  arg15_1 = view_18 = t_4 = None\n",
      "        view_19: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_4, [1, 77, 3072]);  addmm_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_1: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_19, 1.702)\n",
      "        sigmoid: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_1);  mul_1 = None\n",
      "        mul_2: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_19, sigmoid);  view_19 = sigmoid = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_5: f16[3072, 768] = torch.ops.aten.t.default(arg16_1);  arg16_1 = None\n",
      "        view_20: f16[77, 3072] = torch.ops.aten.view.default(mul_2, [77, 3072]);  mul_2 = None\n",
      "        addmm_5: f16[77, 768] = torch.ops.aten.addmm.default(arg17_1, view_20, t_5);  arg17_1 = view_20 = t_5 = None\n",
      "        view_21: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_5, [1, 77, 768]);  addmm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_3: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_2, view_21);  add_2 = view_21 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm_2 = torch.ops.aten.native_layer_norm.default(add_3, [768], arg18_1, arg19_1, 1e-05);  arg18_1 = arg19_1 = None\n",
      "        getitem_6: f16[1, 77, 768] = native_layer_norm_2[0]\n",
      "        getitem_7: f32[1, 77, 1] = native_layer_norm_2[1]\n",
      "        getitem_8: f32[1, 77, 1] = native_layer_norm_2[2];  native_layer_norm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t_6: f16[768, 768] = torch.ops.aten.t.default(arg20_1);  arg20_1 = None\n",
      "        view_22: f16[77, 768] = torch.ops.aten.view.default(getitem_6, [77, 768])\n",
      "        addmm_6: f16[77, 768] = torch.ops.aten.addmm.default(arg21_1, view_22, t_6);  arg21_1 = view_22 = t_6 = None\n",
      "        view_23: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_6, [1, 77, 768]);  addmm_6 = None\n",
      "        mul_3: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_23, 0.125);  view_23 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_7: f16[768, 768] = torch.ops.aten.t.default(arg22_1);  arg22_1 = None\n",
      "        view_24: f16[77, 768] = torch.ops.aten.view.default(getitem_6, [77, 768])\n",
      "        addmm_7: f16[77, 768] = torch.ops.aten.addmm.default(arg23_1, view_24, t_7);  arg23_1 = view_24 = t_7 = None\n",
      "        view_25: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_7, [1, 77, 768]);  addmm_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_26: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_25, [1, -1, 12, 64]);  view_25 = None\n",
      "        transpose_5: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_26, 1, 2);  view_26 = None\n",
      "        clone_4: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_5, memory_format = torch.contiguous_format);  transpose_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_8: f16[768, 768] = torch.ops.aten.t.default(arg24_1);  arg24_1 = None\n",
      "        view_27: f16[77, 768] = torch.ops.aten.view.default(getitem_6, [77, 768]);  getitem_6 = None\n",
      "        addmm_8: f16[77, 768] = torch.ops.aten.addmm.default(arg25_1, view_27, t_8);  arg25_1 = view_27 = t_8 = None\n",
      "        view_28: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_8, [1, 77, 768]);  addmm_8 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_29: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_28, [1, -1, 12, 64]);  view_28 = None\n",
      "        transpose_6: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_29, 1, 2);  view_29 = None\n",
      "        clone_5: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_6, memory_format = torch.contiguous_format);  transpose_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_30: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul_3, [1, 77, 12, 64]);  mul_3 = None\n",
      "        transpose_7: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_30, 1, 2);  view_30 = None\n",
      "        clone_6: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_7, memory_format = torch.contiguous_format);  transpose_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_31: f16[12, 77, 64] = torch.ops.aten.view.default(clone_6, [12, -1, 64]);  clone_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_32: f16[12, 77, 64] = torch.ops.aten.view.default(clone_4, [12, -1, 64]);  clone_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_33: f16[12, 77, 64] = torch.ops.aten.view.default(clone_5, [12, -1, 64]);  clone_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_8: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_32, 1, 2);  view_32 = None\n",
      "        bmm_2: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_31, transpose_8);  view_31 = transpose_8 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_34: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm_2, [1, 12, 77, 77]);  bmm_2 = None\n",
      "        add_4: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_34, _to_copy_1);  view_34 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_35: f16[12, 77, 77] = torch.ops.aten.view.default(add_4, [12, 77, 77]);  add_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax_1: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_35, -1, False);  view_35 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_3: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax_1, view_33);  _softmax_1 = view_33 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_36: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_3, [1, 12, 77, 64]);  bmm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_9: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_36, 1, 2);  view_36 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_7: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_9, memory_format = torch.contiguous_format);  transpose_9 = None\n",
      "        _unsafe_view_1: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_7, [1, 77, 768]);  clone_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_9: f16[768, 768] = torch.ops.aten.t.default(arg26_1);  arg26_1 = None\n",
      "        view_37: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view_1, [77, 768]);  _unsafe_view_1 = None\n",
      "        addmm_9: f16[77, 768] = torch.ops.aten.addmm.default(arg27_1, view_37, t_9);  arg27_1 = view_37 = t_9 = None\n",
      "        view_38: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_9, [1, 77, 768]);  addmm_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_5: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_3, view_38);  add_3 = view_38 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_3 = torch.ops.aten.native_layer_norm.default(add_5, [768], arg28_1, arg29_1, 1e-05);  arg28_1 = arg29_1 = None\n",
      "        getitem_9: f16[1, 77, 768] = native_layer_norm_3[0]\n",
      "        getitem_10: f32[1, 77, 1] = native_layer_norm_3[1]\n",
      "        getitem_11: f32[1, 77, 1] = native_layer_norm_3[2];  native_layer_norm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_10: f16[768, 3072] = torch.ops.aten.t.default(arg30_1);  arg30_1 = None\n",
      "        view_39: f16[77, 768] = torch.ops.aten.view.default(getitem_9, [77, 768]);  getitem_9 = None\n",
      "        addmm_10: f16[77, 3072] = torch.ops.aten.addmm.default(arg31_1, view_39, t_10);  arg31_1 = view_39 = t_10 = None\n",
      "        view_40: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_10, [1, 77, 3072]);  addmm_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_4: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_40, 1.702)\n",
      "        sigmoid_1: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_4);  mul_4 = None\n",
      "        mul_5: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_40, sigmoid_1);  view_40 = sigmoid_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_11: f16[3072, 768] = torch.ops.aten.t.default(arg32_1);  arg32_1 = None\n",
      "        view_41: f16[77, 3072] = torch.ops.aten.view.default(mul_5, [77, 3072]);  mul_5 = None\n",
      "        addmm_11: f16[77, 768] = torch.ops.aten.addmm.default(arg33_1, view_41, t_11);  arg33_1 = view_41 = t_11 = None\n",
      "        view_42: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_11, [1, 77, 768]);  addmm_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_6: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_5, view_42);  add_5 = view_42 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm_4 = torch.ops.aten.native_layer_norm.default(add_6, [768], arg34_1, arg35_1, 1e-05);  arg34_1 = arg35_1 = None\n",
      "        getitem_12: f16[1, 77, 768] = native_layer_norm_4[0]\n",
      "        getitem_13: f32[1, 77, 1] = native_layer_norm_4[1]\n",
      "        getitem_14: f32[1, 77, 1] = native_layer_norm_4[2];  native_layer_norm_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t_12: f16[768, 768] = torch.ops.aten.t.default(arg36_1);  arg36_1 = None\n",
      "        view_43: f16[77, 768] = torch.ops.aten.view.default(getitem_12, [77, 768])\n",
      "        addmm_12: f16[77, 768] = torch.ops.aten.addmm.default(arg37_1, view_43, t_12);  arg37_1 = view_43 = t_12 = None\n",
      "        view_44: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_12, [1, 77, 768]);  addmm_12 = None\n",
      "        mul_6: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_44, 0.125);  view_44 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_13: f16[768, 768] = torch.ops.aten.t.default(arg38_1);  arg38_1 = None\n",
      "        view_45: f16[77, 768] = torch.ops.aten.view.default(getitem_12, [77, 768])\n",
      "        addmm_13: f16[77, 768] = torch.ops.aten.addmm.default(arg39_1, view_45, t_13);  arg39_1 = view_45 = t_13 = None\n",
      "        view_46: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_13, [1, 77, 768]);  addmm_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_47: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_46, [1, -1, 12, 64]);  view_46 = None\n",
      "        transpose_10: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_47, 1, 2);  view_47 = None\n",
      "        clone_8: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_10, memory_format = torch.contiguous_format);  transpose_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_14: f16[768, 768] = torch.ops.aten.t.default(arg40_1);  arg40_1 = None\n",
      "        view_48: f16[77, 768] = torch.ops.aten.view.default(getitem_12, [77, 768]);  getitem_12 = None\n",
      "        addmm_14: f16[77, 768] = torch.ops.aten.addmm.default(arg41_1, view_48, t_14);  arg41_1 = view_48 = t_14 = None\n",
      "        view_49: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_14, [1, 77, 768]);  addmm_14 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_50: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_49, [1, -1, 12, 64]);  view_49 = None\n",
      "        transpose_11: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_50, 1, 2);  view_50 = None\n",
      "        clone_9: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_11, memory_format = torch.contiguous_format);  transpose_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_51: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul_6, [1, 77, 12, 64]);  mul_6 = None\n",
      "        transpose_12: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_51, 1, 2);  view_51 = None\n",
      "        clone_10: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_12, memory_format = torch.contiguous_format);  transpose_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_52: f16[12, 77, 64] = torch.ops.aten.view.default(clone_10, [12, -1, 64]);  clone_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_53: f16[12, 77, 64] = torch.ops.aten.view.default(clone_8, [12, -1, 64]);  clone_8 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_54: f16[12, 77, 64] = torch.ops.aten.view.default(clone_9, [12, -1, 64]);  clone_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_13: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_53, 1, 2);  view_53 = None\n",
      "        bmm_4: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_52, transpose_13);  view_52 = transpose_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_55: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm_4, [1, 12, 77, 77]);  bmm_4 = None\n",
      "        add_7: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_55, _to_copy_1);  view_55 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_56: f16[12, 77, 77] = torch.ops.aten.view.default(add_7, [12, 77, 77]);  add_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax_2: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_56, -1, False);  view_56 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_5: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax_2, view_54);  _softmax_2 = view_54 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_57: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_5, [1, 12, 77, 64]);  bmm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_14: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_57, 1, 2);  view_57 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_11: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_14, memory_format = torch.contiguous_format);  transpose_14 = None\n",
      "        _unsafe_view_2: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_11, [1, 77, 768]);  clone_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_15: f16[768, 768] = torch.ops.aten.t.default(arg42_1);  arg42_1 = None\n",
      "        view_58: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view_2, [77, 768]);  _unsafe_view_2 = None\n",
      "        addmm_15: f16[77, 768] = torch.ops.aten.addmm.default(arg43_1, view_58, t_15);  arg43_1 = view_58 = t_15 = None\n",
      "        view_59: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_15, [1, 77, 768]);  addmm_15 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_8: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_6, view_59);  add_6 = view_59 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_5 = torch.ops.aten.native_layer_norm.default(add_8, [768], arg44_1, arg45_1, 1e-05);  arg44_1 = arg45_1 = None\n",
      "        getitem_15: f16[1, 77, 768] = native_layer_norm_5[0]\n",
      "        getitem_16: f32[1, 77, 1] = native_layer_norm_5[1]\n",
      "        getitem_17: f32[1, 77, 1] = native_layer_norm_5[2];  native_layer_norm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_16: f16[768, 3072] = torch.ops.aten.t.default(arg46_1);  arg46_1 = None\n",
      "        view_60: f16[77, 768] = torch.ops.aten.view.default(getitem_15, [77, 768]);  getitem_15 = None\n",
      "        addmm_16: f16[77, 3072] = torch.ops.aten.addmm.default(arg47_1, view_60, t_16);  arg47_1 = view_60 = t_16 = None\n",
      "        view_61: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_16, [1, 77, 3072]);  addmm_16 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_7: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_61, 1.702)\n",
      "        sigmoid_2: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_7);  mul_7 = None\n",
      "        mul_8: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_61, sigmoid_2);  view_61 = sigmoid_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_17: f16[3072, 768] = torch.ops.aten.t.default(arg48_1);  arg48_1 = None\n",
      "        view_62: f16[77, 3072] = torch.ops.aten.view.default(mul_8, [77, 3072]);  mul_8 = None\n",
      "        addmm_17: f16[77, 768] = torch.ops.aten.addmm.default(arg49_1, view_62, t_17);  arg49_1 = view_62 = t_17 = None\n",
      "        view_63: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_17, [1, 77, 768]);  addmm_17 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_9: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_8, view_63);  add_8 = view_63 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm_6 = torch.ops.aten.native_layer_norm.default(add_9, [768], arg50_1, arg51_1, 1e-05);  arg50_1 = arg51_1 = None\n",
      "        getitem_18: f16[1, 77, 768] = native_layer_norm_6[0]\n",
      "        getitem_19: f32[1, 77, 1] = native_layer_norm_6[1]\n",
      "        getitem_20: f32[1, 77, 1] = native_layer_norm_6[2];  native_layer_norm_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t_18: f16[768, 768] = torch.ops.aten.t.default(arg52_1);  arg52_1 = None\n",
      "        view_64: f16[77, 768] = torch.ops.aten.view.default(getitem_18, [77, 768])\n",
      "        addmm_18: f16[77, 768] = torch.ops.aten.addmm.default(arg53_1, view_64, t_18);  arg53_1 = view_64 = t_18 = None\n",
      "        view_65: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_18, [1, 77, 768]);  addmm_18 = None\n",
      "        mul_9: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_65, 0.125);  view_65 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_19: f16[768, 768] = torch.ops.aten.t.default(arg54_1);  arg54_1 = None\n",
      "        view_66: f16[77, 768] = torch.ops.aten.view.default(getitem_18, [77, 768])\n",
      "        addmm_19: f16[77, 768] = torch.ops.aten.addmm.default(arg55_1, view_66, t_19);  arg55_1 = view_66 = t_19 = None\n",
      "        view_67: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_19, [1, 77, 768]);  addmm_19 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_68: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_67, [1, -1, 12, 64]);  view_67 = None\n",
      "        transpose_15: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_68, 1, 2);  view_68 = None\n",
      "        clone_12: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_15, memory_format = torch.contiguous_format);  transpose_15 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_20: f16[768, 768] = torch.ops.aten.t.default(arg56_1);  arg56_1 = None\n",
      "        view_69: f16[77, 768] = torch.ops.aten.view.default(getitem_18, [77, 768]);  getitem_18 = None\n",
      "        addmm_20: f16[77, 768] = torch.ops.aten.addmm.default(arg57_1, view_69, t_20);  arg57_1 = view_69 = t_20 = None\n",
      "        view_70: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_20, [1, 77, 768]);  addmm_20 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_71: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_70, [1, -1, 12, 64]);  view_70 = None\n",
      "        transpose_16: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_71, 1, 2);  view_71 = None\n",
      "        clone_13: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_16, memory_format = torch.contiguous_format);  transpose_16 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_72: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul_9, [1, 77, 12, 64]);  mul_9 = None\n",
      "        transpose_17: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_72, 1, 2);  view_72 = None\n",
      "        clone_14: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_17, memory_format = torch.contiguous_format);  transpose_17 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_73: f16[12, 77, 64] = torch.ops.aten.view.default(clone_14, [12, -1, 64]);  clone_14 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_74: f16[12, 77, 64] = torch.ops.aten.view.default(clone_12, [12, -1, 64]);  clone_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_75: f16[12, 77, 64] = torch.ops.aten.view.default(clone_13, [12, -1, 64]);  clone_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_18: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_74, 1, 2);  view_74 = None\n",
      "        bmm_6: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_73, transpose_18);  view_73 = transpose_18 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_76: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm_6, [1, 12, 77, 77]);  bmm_6 = None\n",
      "        add_10: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_76, _to_copy_1);  view_76 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_77: f16[12, 77, 77] = torch.ops.aten.view.default(add_10, [12, 77, 77]);  add_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax_3: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_77, -1, False);  view_77 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_7: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax_3, view_75);  _softmax_3 = view_75 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_78: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_7, [1, 12, 77, 64]);  bmm_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_19: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_78, 1, 2);  view_78 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_15: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_19, memory_format = torch.contiguous_format);  transpose_19 = None\n",
      "        _unsafe_view_3: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_15, [1, 77, 768]);  clone_15 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_21: f16[768, 768] = torch.ops.aten.t.default(arg58_1);  arg58_1 = None\n",
      "        view_79: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view_3, [77, 768]);  _unsafe_view_3 = None\n",
      "        addmm_21: f16[77, 768] = torch.ops.aten.addmm.default(arg59_1, view_79, t_21);  arg59_1 = view_79 = t_21 = None\n",
      "        view_80: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_21, [1, 77, 768]);  addmm_21 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_11: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_9, view_80);  add_9 = view_80 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_7 = torch.ops.aten.native_layer_norm.default(add_11, [768], arg60_1, arg61_1, 1e-05);  arg60_1 = arg61_1 = None\n",
      "        getitem_21: f16[1, 77, 768] = native_layer_norm_7[0]\n",
      "        getitem_22: f32[1, 77, 1] = native_layer_norm_7[1]\n",
      "        getitem_23: f32[1, 77, 1] = native_layer_norm_7[2];  native_layer_norm_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_22: f16[768, 3072] = torch.ops.aten.t.default(arg62_1);  arg62_1 = None\n",
      "        view_81: f16[77, 768] = torch.ops.aten.view.default(getitem_21, [77, 768]);  getitem_21 = None\n",
      "        addmm_22: f16[77, 3072] = torch.ops.aten.addmm.default(arg63_1, view_81, t_22);  arg63_1 = view_81 = t_22 = None\n",
      "        view_82: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_22, [1, 77, 3072]);  addmm_22 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_10: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_82, 1.702)\n",
      "        sigmoid_3: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_10);  mul_10 = None\n",
      "        mul_11: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_82, sigmoid_3);  view_82 = sigmoid_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_23: f16[3072, 768] = torch.ops.aten.t.default(arg64_1);  arg64_1 = None\n",
      "        view_83: f16[77, 3072] = torch.ops.aten.view.default(mul_11, [77, 3072]);  mul_11 = None\n",
      "        addmm_23: f16[77, 768] = torch.ops.aten.addmm.default(arg65_1, view_83, t_23);  arg65_1 = view_83 = t_23 = None\n",
      "        view_84: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_23, [1, 77, 768]);  addmm_23 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_12: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_11, view_84);  add_11 = view_84 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm_8 = torch.ops.aten.native_layer_norm.default(add_12, [768], arg66_1, arg67_1, 1e-05);  arg66_1 = arg67_1 = None\n",
      "        getitem_24: f16[1, 77, 768] = native_layer_norm_8[0]\n",
      "        getitem_25: f32[1, 77, 1] = native_layer_norm_8[1]\n",
      "        getitem_26: f32[1, 77, 1] = native_layer_norm_8[2];  native_layer_norm_8 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t_24: f16[768, 768] = torch.ops.aten.t.default(arg68_1);  arg68_1 = None\n",
      "        view_85: f16[77, 768] = torch.ops.aten.view.default(getitem_24, [77, 768])\n",
      "        addmm_24: f16[77, 768] = torch.ops.aten.addmm.default(arg69_1, view_85, t_24);  arg69_1 = view_85 = t_24 = None\n",
      "        view_86: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_24, [1, 77, 768]);  addmm_24 = None\n",
      "        mul_12: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_86, 0.125);  view_86 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_25: f16[768, 768] = torch.ops.aten.t.default(arg70_1);  arg70_1 = None\n",
      "        view_87: f16[77, 768] = torch.ops.aten.view.default(getitem_24, [77, 768])\n",
      "        addmm_25: f16[77, 768] = torch.ops.aten.addmm.default(arg71_1, view_87, t_25);  arg71_1 = view_87 = t_25 = None\n",
      "        view_88: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_25, [1, 77, 768]);  addmm_25 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_89: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_88, [1, -1, 12, 64]);  view_88 = None\n",
      "        transpose_20: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_89, 1, 2);  view_89 = None\n",
      "        clone_16: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_20, memory_format = torch.contiguous_format);  transpose_20 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_26: f16[768, 768] = torch.ops.aten.t.default(arg72_1);  arg72_1 = None\n",
      "        view_90: f16[77, 768] = torch.ops.aten.view.default(getitem_24, [77, 768]);  getitem_24 = None\n",
      "        addmm_26: f16[77, 768] = torch.ops.aten.addmm.default(arg73_1, view_90, t_26);  arg73_1 = view_90 = t_26 = None\n",
      "        view_91: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_26, [1, 77, 768]);  addmm_26 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_92: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_91, [1, -1, 12, 64]);  view_91 = None\n",
      "        transpose_21: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_92, 1, 2);  view_92 = None\n",
      "        clone_17: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_21, memory_format = torch.contiguous_format);  transpose_21 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_93: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul_12, [1, 77, 12, 64]);  mul_12 = None\n",
      "        transpose_22: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_93, 1, 2);  view_93 = None\n",
      "        clone_18: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_22, memory_format = torch.contiguous_format);  transpose_22 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_94: f16[12, 77, 64] = torch.ops.aten.view.default(clone_18, [12, -1, 64]);  clone_18 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_95: f16[12, 77, 64] = torch.ops.aten.view.default(clone_16, [12, -1, 64]);  clone_16 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_96: f16[12, 77, 64] = torch.ops.aten.view.default(clone_17, [12, -1, 64]);  clone_17 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_23: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_95, 1, 2);  view_95 = None\n",
      "        bmm_8: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_94, transpose_23);  view_94 = transpose_23 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_97: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm_8, [1, 12, 77, 77]);  bmm_8 = None\n",
      "        add_13: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_97, _to_copy_1);  view_97 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_98: f16[12, 77, 77] = torch.ops.aten.view.default(add_13, [12, 77, 77]);  add_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax_4: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_98, -1, False);  view_98 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_9: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax_4, view_96);  _softmax_4 = view_96 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_99: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_9, [1, 12, 77, 64]);  bmm_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_24: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_99, 1, 2);  view_99 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_19: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_24, memory_format = torch.contiguous_format);  transpose_24 = None\n",
      "        _unsafe_view_4: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_19, [1, 77, 768]);  clone_19 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_27: f16[768, 768] = torch.ops.aten.t.default(arg74_1);  arg74_1 = None\n",
      "        view_100: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view_4, [77, 768]);  _unsafe_view_4 = None\n",
      "        addmm_27: f16[77, 768] = torch.ops.aten.addmm.default(arg75_1, view_100, t_27);  arg75_1 = view_100 = t_27 = None\n",
      "        view_101: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_27, [1, 77, 768]);  addmm_27 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_14: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_12, view_101);  add_12 = view_101 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_9 = torch.ops.aten.native_layer_norm.default(add_14, [768], arg76_1, arg77_1, 1e-05);  arg76_1 = arg77_1 = None\n",
      "        getitem_27: f16[1, 77, 768] = native_layer_norm_9[0]\n",
      "        getitem_28: f32[1, 77, 1] = native_layer_norm_9[1]\n",
      "        getitem_29: f32[1, 77, 1] = native_layer_norm_9[2];  native_layer_norm_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_28: f16[768, 3072] = torch.ops.aten.t.default(arg78_1);  arg78_1 = None\n",
      "        view_102: f16[77, 768] = torch.ops.aten.view.default(getitem_27, [77, 768]);  getitem_27 = None\n",
      "        addmm_28: f16[77, 3072] = torch.ops.aten.addmm.default(arg79_1, view_102, t_28);  arg79_1 = view_102 = t_28 = None\n",
      "        view_103: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_28, [1, 77, 3072]);  addmm_28 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_13: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_103, 1.702)\n",
      "        sigmoid_4: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_13);  mul_13 = None\n",
      "        mul_14: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_103, sigmoid_4);  view_103 = sigmoid_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_29: f16[3072, 768] = torch.ops.aten.t.default(arg80_1);  arg80_1 = None\n",
      "        view_104: f16[77, 3072] = torch.ops.aten.view.default(mul_14, [77, 3072]);  mul_14 = None\n",
      "        addmm_29: f16[77, 768] = torch.ops.aten.addmm.default(arg81_1, view_104, t_29);  arg81_1 = view_104 = t_29 = None\n",
      "        view_105: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_29, [1, 77, 768]);  addmm_29 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_15: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_14, view_105);  add_14 = view_105 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm_10 = torch.ops.aten.native_layer_norm.default(add_15, [768], arg82_1, arg83_1, 1e-05);  arg82_1 = arg83_1 = None\n",
      "        getitem_30: f16[1, 77, 768] = native_layer_norm_10[0]\n",
      "        getitem_31: f32[1, 77, 1] = native_layer_norm_10[1]\n",
      "        getitem_32: f32[1, 77, 1] = native_layer_norm_10[2];  native_layer_norm_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t_30: f16[768, 768] = torch.ops.aten.t.default(arg84_1);  arg84_1 = None\n",
      "        view_106: f16[77, 768] = torch.ops.aten.view.default(getitem_30, [77, 768])\n",
      "        addmm_30: f16[77, 768] = torch.ops.aten.addmm.default(arg85_1, view_106, t_30);  arg85_1 = view_106 = t_30 = None\n",
      "        view_107: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_30, [1, 77, 768]);  addmm_30 = None\n",
      "        mul_15: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_107, 0.125);  view_107 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_31: f16[768, 768] = torch.ops.aten.t.default(arg86_1);  arg86_1 = None\n",
      "        view_108: f16[77, 768] = torch.ops.aten.view.default(getitem_30, [77, 768])\n",
      "        addmm_31: f16[77, 768] = torch.ops.aten.addmm.default(arg87_1, view_108, t_31);  arg87_1 = view_108 = t_31 = None\n",
      "        view_109: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_31, [1, 77, 768]);  addmm_31 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_110: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_109, [1, -1, 12, 64]);  view_109 = None\n",
      "        transpose_25: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_110, 1, 2);  view_110 = None\n",
      "        clone_20: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_25, memory_format = torch.contiguous_format);  transpose_25 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_32: f16[768, 768] = torch.ops.aten.t.default(arg88_1);  arg88_1 = None\n",
      "        view_111: f16[77, 768] = torch.ops.aten.view.default(getitem_30, [77, 768]);  getitem_30 = None\n",
      "        addmm_32: f16[77, 768] = torch.ops.aten.addmm.default(arg89_1, view_111, t_32);  arg89_1 = view_111 = t_32 = None\n",
      "        view_112: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_32, [1, 77, 768]);  addmm_32 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_113: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_112, [1, -1, 12, 64]);  view_112 = None\n",
      "        transpose_26: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_113, 1, 2);  view_113 = None\n",
      "        clone_21: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_26, memory_format = torch.contiguous_format);  transpose_26 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_114: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul_15, [1, 77, 12, 64]);  mul_15 = None\n",
      "        transpose_27: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_114, 1, 2);  view_114 = None\n",
      "        clone_22: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_27, memory_format = torch.contiguous_format);  transpose_27 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_115: f16[12, 77, 64] = torch.ops.aten.view.default(clone_22, [12, -1, 64]);  clone_22 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_116: f16[12, 77, 64] = torch.ops.aten.view.default(clone_20, [12, -1, 64]);  clone_20 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_117: f16[12, 77, 64] = torch.ops.aten.view.default(clone_21, [12, -1, 64]);  clone_21 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_28: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_116, 1, 2);  view_116 = None\n",
      "        bmm_10: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_115, transpose_28);  view_115 = transpose_28 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_118: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm_10, [1, 12, 77, 77]);  bmm_10 = None\n",
      "        add_16: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_118, _to_copy_1);  view_118 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_119: f16[12, 77, 77] = torch.ops.aten.view.default(add_16, [12, 77, 77]);  add_16 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax_5: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_119, -1, False);  view_119 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_11: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax_5, view_117);  _softmax_5 = view_117 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_120: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_11, [1, 12, 77, 64]);  bmm_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_29: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_120, 1, 2);  view_120 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_23: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_29, memory_format = torch.contiguous_format);  transpose_29 = None\n",
      "        _unsafe_view_5: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_23, [1, 77, 768]);  clone_23 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_33: f16[768, 768] = torch.ops.aten.t.default(arg90_1);  arg90_1 = None\n",
      "        view_121: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view_5, [77, 768]);  _unsafe_view_5 = None\n",
      "        addmm_33: f16[77, 768] = torch.ops.aten.addmm.default(arg91_1, view_121, t_33);  arg91_1 = view_121 = t_33 = None\n",
      "        view_122: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_33, [1, 77, 768]);  addmm_33 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_17: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_15, view_122);  add_15 = view_122 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_11 = torch.ops.aten.native_layer_norm.default(add_17, [768], arg92_1, arg93_1, 1e-05);  arg92_1 = arg93_1 = None\n",
      "        getitem_33: f16[1, 77, 768] = native_layer_norm_11[0]\n",
      "        getitem_34: f32[1, 77, 1] = native_layer_norm_11[1]\n",
      "        getitem_35: f32[1, 77, 1] = native_layer_norm_11[2];  native_layer_norm_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_34: f16[768, 3072] = torch.ops.aten.t.default(arg94_1);  arg94_1 = None\n",
      "        view_123: f16[77, 768] = torch.ops.aten.view.default(getitem_33, [77, 768]);  getitem_33 = None\n",
      "        addmm_34: f16[77, 3072] = torch.ops.aten.addmm.default(arg95_1, view_123, t_34);  arg95_1 = view_123 = t_34 = None\n",
      "        view_124: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_34, [1, 77, 3072]);  addmm_34 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_16: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_124, 1.702)\n",
      "        sigmoid_5: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_16);  mul_16 = None\n",
      "        mul_17: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_124, sigmoid_5);  view_124 = sigmoid_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_35: f16[3072, 768] = torch.ops.aten.t.default(arg96_1);  arg96_1 = None\n",
      "        view_125: f16[77, 3072] = torch.ops.aten.view.default(mul_17, [77, 3072]);  mul_17 = None\n",
      "        addmm_35: f16[77, 768] = torch.ops.aten.addmm.default(arg97_1, view_125, t_35);  arg97_1 = view_125 = t_35 = None\n",
      "        view_126: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_35, [1, 77, 768]);  addmm_35 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_18: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_17, view_126);  add_17 = view_126 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm_12 = torch.ops.aten.native_layer_norm.default(add_18, [768], arg98_1, arg99_1, 1e-05);  arg98_1 = arg99_1 = None\n",
      "        getitem_36: f16[1, 77, 768] = native_layer_norm_12[0]\n",
      "        getitem_37: f32[1, 77, 1] = native_layer_norm_12[1]\n",
      "        getitem_38: f32[1, 77, 1] = native_layer_norm_12[2];  native_layer_norm_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t_36: f16[768, 768] = torch.ops.aten.t.default(arg100_1);  arg100_1 = None\n",
      "        view_127: f16[77, 768] = torch.ops.aten.view.default(getitem_36, [77, 768])\n",
      "        addmm_36: f16[77, 768] = torch.ops.aten.addmm.default(arg101_1, view_127, t_36);  arg101_1 = view_127 = t_36 = None\n",
      "        view_128: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_36, [1, 77, 768]);  addmm_36 = None\n",
      "        mul_18: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_128, 0.125);  view_128 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_37: f16[768, 768] = torch.ops.aten.t.default(arg102_1);  arg102_1 = None\n",
      "        view_129: f16[77, 768] = torch.ops.aten.view.default(getitem_36, [77, 768])\n",
      "        addmm_37: f16[77, 768] = torch.ops.aten.addmm.default(arg103_1, view_129, t_37);  arg103_1 = view_129 = t_37 = None\n",
      "        view_130: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_37, [1, 77, 768]);  addmm_37 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_131: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_130, [1, -1, 12, 64]);  view_130 = None\n",
      "        transpose_30: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_131, 1, 2);  view_131 = None\n",
      "        clone_24: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_30, memory_format = torch.contiguous_format);  transpose_30 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_38: f16[768, 768] = torch.ops.aten.t.default(arg104_1);  arg104_1 = None\n",
      "        view_132: f16[77, 768] = torch.ops.aten.view.default(getitem_36, [77, 768]);  getitem_36 = None\n",
      "        addmm_38: f16[77, 768] = torch.ops.aten.addmm.default(arg105_1, view_132, t_38);  arg105_1 = view_132 = t_38 = None\n",
      "        view_133: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_38, [1, 77, 768]);  addmm_38 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_134: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_133, [1, -1, 12, 64]);  view_133 = None\n",
      "        transpose_31: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_134, 1, 2);  view_134 = None\n",
      "        clone_25: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_31, memory_format = torch.contiguous_format);  transpose_31 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_135: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul_18, [1, 77, 12, 64]);  mul_18 = None\n",
      "        transpose_32: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_135, 1, 2);  view_135 = None\n",
      "        clone_26: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_32, memory_format = torch.contiguous_format);  transpose_32 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_136: f16[12, 77, 64] = torch.ops.aten.view.default(clone_26, [12, -1, 64]);  clone_26 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_137: f16[12, 77, 64] = torch.ops.aten.view.default(clone_24, [12, -1, 64]);  clone_24 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_138: f16[12, 77, 64] = torch.ops.aten.view.default(clone_25, [12, -1, 64]);  clone_25 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_33: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_137, 1, 2);  view_137 = None\n",
      "        bmm_12: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_136, transpose_33);  view_136 = transpose_33 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_139: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm_12, [1, 12, 77, 77]);  bmm_12 = None\n",
      "        add_19: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_139, _to_copy_1);  view_139 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_140: f16[12, 77, 77] = torch.ops.aten.view.default(add_19, [12, 77, 77]);  add_19 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax_6: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_140, -1, False);  view_140 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_13: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax_6, view_138);  _softmax_6 = view_138 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_141: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_13, [1, 12, 77, 64]);  bmm_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_34: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_141, 1, 2);  view_141 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_27: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_34, memory_format = torch.contiguous_format);  transpose_34 = None\n",
      "        _unsafe_view_6: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_27, [1, 77, 768]);  clone_27 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_39: f16[768, 768] = torch.ops.aten.t.default(arg106_1);  arg106_1 = None\n",
      "        view_142: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view_6, [77, 768]);  _unsafe_view_6 = None\n",
      "        addmm_39: f16[77, 768] = torch.ops.aten.addmm.default(arg107_1, view_142, t_39);  arg107_1 = view_142 = t_39 = None\n",
      "        view_143: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_39, [1, 77, 768]);  addmm_39 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_20: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_18, view_143);  add_18 = view_143 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_13 = torch.ops.aten.native_layer_norm.default(add_20, [768], arg108_1, arg109_1, 1e-05);  arg108_1 = arg109_1 = None\n",
      "        getitem_39: f16[1, 77, 768] = native_layer_norm_13[0]\n",
      "        getitem_40: f32[1, 77, 1] = native_layer_norm_13[1]\n",
      "        getitem_41: f32[1, 77, 1] = native_layer_norm_13[2];  native_layer_norm_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_40: f16[768, 3072] = torch.ops.aten.t.default(arg110_1);  arg110_1 = None\n",
      "        view_144: f16[77, 768] = torch.ops.aten.view.default(getitem_39, [77, 768]);  getitem_39 = None\n",
      "        addmm_40: f16[77, 3072] = torch.ops.aten.addmm.default(arg111_1, view_144, t_40);  arg111_1 = view_144 = t_40 = None\n",
      "        view_145: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_40, [1, 77, 3072]);  addmm_40 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_19: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_145, 1.702)\n",
      "        sigmoid_6: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_19);  mul_19 = None\n",
      "        mul_20: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_145, sigmoid_6);  view_145 = sigmoid_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_41: f16[3072, 768] = torch.ops.aten.t.default(arg112_1);  arg112_1 = None\n",
      "        view_146: f16[77, 3072] = torch.ops.aten.view.default(mul_20, [77, 3072]);  mul_20 = None\n",
      "        addmm_41: f16[77, 768] = torch.ops.aten.addmm.default(arg113_1, view_146, t_41);  arg113_1 = view_146 = t_41 = None\n",
      "        view_147: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_41, [1, 77, 768]);  addmm_41 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_21: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_20, view_147);  add_20 = view_147 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm_14 = torch.ops.aten.native_layer_norm.default(add_21, [768], arg114_1, arg115_1, 1e-05);  arg114_1 = arg115_1 = None\n",
      "        getitem_42: f16[1, 77, 768] = native_layer_norm_14[0]\n",
      "        getitem_43: f32[1, 77, 1] = native_layer_norm_14[1]\n",
      "        getitem_44: f32[1, 77, 1] = native_layer_norm_14[2];  native_layer_norm_14 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t_42: f16[768, 768] = torch.ops.aten.t.default(arg116_1);  arg116_1 = None\n",
      "        view_148: f16[77, 768] = torch.ops.aten.view.default(getitem_42, [77, 768])\n",
      "        addmm_42: f16[77, 768] = torch.ops.aten.addmm.default(arg117_1, view_148, t_42);  arg117_1 = view_148 = t_42 = None\n",
      "        view_149: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_42, [1, 77, 768]);  addmm_42 = None\n",
      "        mul_21: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_149, 0.125);  view_149 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_43: f16[768, 768] = torch.ops.aten.t.default(arg118_1);  arg118_1 = None\n",
      "        view_150: f16[77, 768] = torch.ops.aten.view.default(getitem_42, [77, 768])\n",
      "        addmm_43: f16[77, 768] = torch.ops.aten.addmm.default(arg119_1, view_150, t_43);  arg119_1 = view_150 = t_43 = None\n",
      "        view_151: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_43, [1, 77, 768]);  addmm_43 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_152: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_151, [1, -1, 12, 64]);  view_151 = None\n",
      "        transpose_35: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_152, 1, 2);  view_152 = None\n",
      "        clone_28: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_35, memory_format = torch.contiguous_format);  transpose_35 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_44: f16[768, 768] = torch.ops.aten.t.default(arg120_1);  arg120_1 = None\n",
      "        view_153: f16[77, 768] = torch.ops.aten.view.default(getitem_42, [77, 768]);  getitem_42 = None\n",
      "        addmm_44: f16[77, 768] = torch.ops.aten.addmm.default(arg121_1, view_153, t_44);  arg121_1 = view_153 = t_44 = None\n",
      "        view_154: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_44, [1, 77, 768]);  addmm_44 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_155: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_154, [1, -1, 12, 64]);  view_154 = None\n",
      "        transpose_36: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_155, 1, 2);  view_155 = None\n",
      "        clone_29: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_36, memory_format = torch.contiguous_format);  transpose_36 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_156: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul_21, [1, 77, 12, 64]);  mul_21 = None\n",
      "        transpose_37: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_156, 1, 2);  view_156 = None\n",
      "        clone_30: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_37, memory_format = torch.contiguous_format);  transpose_37 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_157: f16[12, 77, 64] = torch.ops.aten.view.default(clone_30, [12, -1, 64]);  clone_30 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_158: f16[12, 77, 64] = torch.ops.aten.view.default(clone_28, [12, -1, 64]);  clone_28 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_159: f16[12, 77, 64] = torch.ops.aten.view.default(clone_29, [12, -1, 64]);  clone_29 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_38: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_158, 1, 2);  view_158 = None\n",
      "        bmm_14: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_157, transpose_38);  view_157 = transpose_38 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_160: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm_14, [1, 12, 77, 77]);  bmm_14 = None\n",
      "        add_22: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_160, _to_copy_1);  view_160 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_161: f16[12, 77, 77] = torch.ops.aten.view.default(add_22, [12, 77, 77]);  add_22 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax_7: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_161, -1, False);  view_161 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_15: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax_7, view_159);  _softmax_7 = view_159 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_162: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_15, [1, 12, 77, 64]);  bmm_15 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_39: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_162, 1, 2);  view_162 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_31: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_39, memory_format = torch.contiguous_format);  transpose_39 = None\n",
      "        _unsafe_view_7: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_31, [1, 77, 768]);  clone_31 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_45: f16[768, 768] = torch.ops.aten.t.default(arg122_1);  arg122_1 = None\n",
      "        view_163: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view_7, [77, 768]);  _unsafe_view_7 = None\n",
      "        addmm_45: f16[77, 768] = torch.ops.aten.addmm.default(arg123_1, view_163, t_45);  arg123_1 = view_163 = t_45 = None\n",
      "        view_164: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_45, [1, 77, 768]);  addmm_45 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_23: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_21, view_164);  add_21 = view_164 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_15 = torch.ops.aten.native_layer_norm.default(add_23, [768], arg124_1, arg125_1, 1e-05);  arg124_1 = arg125_1 = None\n",
      "        getitem_45: f16[1, 77, 768] = native_layer_norm_15[0]\n",
      "        getitem_46: f32[1, 77, 1] = native_layer_norm_15[1]\n",
      "        getitem_47: f32[1, 77, 1] = native_layer_norm_15[2];  native_layer_norm_15 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_46: f16[768, 3072] = torch.ops.aten.t.default(arg126_1);  arg126_1 = None\n",
      "        view_165: f16[77, 768] = torch.ops.aten.view.default(getitem_45, [77, 768]);  getitem_45 = None\n",
      "        addmm_46: f16[77, 3072] = torch.ops.aten.addmm.default(arg127_1, view_165, t_46);  arg127_1 = view_165 = t_46 = None\n",
      "        view_166: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_46, [1, 77, 3072]);  addmm_46 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_22: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_166, 1.702)\n",
      "        sigmoid_7: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_22);  mul_22 = None\n",
      "        mul_23: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_166, sigmoid_7);  view_166 = sigmoid_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_47: f16[3072, 768] = torch.ops.aten.t.default(arg128_1);  arg128_1 = None\n",
      "        view_167: f16[77, 3072] = torch.ops.aten.view.default(mul_23, [77, 3072]);  mul_23 = None\n",
      "        addmm_47: f16[77, 768] = torch.ops.aten.addmm.default(arg129_1, view_167, t_47);  arg129_1 = view_167 = t_47 = None\n",
      "        view_168: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_47, [1, 77, 768]);  addmm_47 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_24: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_23, view_168);  add_23 = view_168 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm_16 = torch.ops.aten.native_layer_norm.default(add_24, [768], arg130_1, arg131_1, 1e-05);  arg130_1 = arg131_1 = None\n",
      "        getitem_48: f16[1, 77, 768] = native_layer_norm_16[0]\n",
      "        getitem_49: f32[1, 77, 1] = native_layer_norm_16[1]\n",
      "        getitem_50: f32[1, 77, 1] = native_layer_norm_16[2];  native_layer_norm_16 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t_48: f16[768, 768] = torch.ops.aten.t.default(arg132_1);  arg132_1 = None\n",
      "        view_169: f16[77, 768] = torch.ops.aten.view.default(getitem_48, [77, 768])\n",
      "        addmm_48: f16[77, 768] = torch.ops.aten.addmm.default(arg133_1, view_169, t_48);  arg133_1 = view_169 = t_48 = None\n",
      "        view_170: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_48, [1, 77, 768]);  addmm_48 = None\n",
      "        mul_24: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_170, 0.125);  view_170 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_49: f16[768, 768] = torch.ops.aten.t.default(arg134_1);  arg134_1 = None\n",
      "        view_171: f16[77, 768] = torch.ops.aten.view.default(getitem_48, [77, 768])\n",
      "        addmm_49: f16[77, 768] = torch.ops.aten.addmm.default(arg135_1, view_171, t_49);  arg135_1 = view_171 = t_49 = None\n",
      "        view_172: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_49, [1, 77, 768]);  addmm_49 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_173: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_172, [1, -1, 12, 64]);  view_172 = None\n",
      "        transpose_40: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_173, 1, 2);  view_173 = None\n",
      "        clone_32: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_40, memory_format = torch.contiguous_format);  transpose_40 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_50: f16[768, 768] = torch.ops.aten.t.default(arg136_1);  arg136_1 = None\n",
      "        view_174: f16[77, 768] = torch.ops.aten.view.default(getitem_48, [77, 768]);  getitem_48 = None\n",
      "        addmm_50: f16[77, 768] = torch.ops.aten.addmm.default(arg137_1, view_174, t_50);  arg137_1 = view_174 = t_50 = None\n",
      "        view_175: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_50, [1, 77, 768]);  addmm_50 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_176: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_175, [1, -1, 12, 64]);  view_175 = None\n",
      "        transpose_41: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_176, 1, 2);  view_176 = None\n",
      "        clone_33: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_41, memory_format = torch.contiguous_format);  transpose_41 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_177: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul_24, [1, 77, 12, 64]);  mul_24 = None\n",
      "        transpose_42: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_177, 1, 2);  view_177 = None\n",
      "        clone_34: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_42, memory_format = torch.contiguous_format);  transpose_42 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_178: f16[12, 77, 64] = torch.ops.aten.view.default(clone_34, [12, -1, 64]);  clone_34 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_179: f16[12, 77, 64] = torch.ops.aten.view.default(clone_32, [12, -1, 64]);  clone_32 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_180: f16[12, 77, 64] = torch.ops.aten.view.default(clone_33, [12, -1, 64]);  clone_33 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_43: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_179, 1, 2);  view_179 = None\n",
      "        bmm_16: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_178, transpose_43);  view_178 = transpose_43 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_181: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm_16, [1, 12, 77, 77]);  bmm_16 = None\n",
      "        add_25: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_181, _to_copy_1);  view_181 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_182: f16[12, 77, 77] = torch.ops.aten.view.default(add_25, [12, 77, 77]);  add_25 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax_8: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_182, -1, False);  view_182 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_17: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax_8, view_180);  _softmax_8 = view_180 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_183: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_17, [1, 12, 77, 64]);  bmm_17 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_44: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_183, 1, 2);  view_183 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_35: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_44, memory_format = torch.contiguous_format);  transpose_44 = None\n",
      "        _unsafe_view_8: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_35, [1, 77, 768]);  clone_35 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_51: f16[768, 768] = torch.ops.aten.t.default(arg138_1);  arg138_1 = None\n",
      "        view_184: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view_8, [77, 768]);  _unsafe_view_8 = None\n",
      "        addmm_51: f16[77, 768] = torch.ops.aten.addmm.default(arg139_1, view_184, t_51);  arg139_1 = view_184 = t_51 = None\n",
      "        view_185: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_51, [1, 77, 768]);  addmm_51 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_26: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_24, view_185);  add_24 = view_185 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_17 = torch.ops.aten.native_layer_norm.default(add_26, [768], arg140_1, arg141_1, 1e-05);  arg140_1 = arg141_1 = None\n",
      "        getitem_51: f16[1, 77, 768] = native_layer_norm_17[0]\n",
      "        getitem_52: f32[1, 77, 1] = native_layer_norm_17[1]\n",
      "        getitem_53: f32[1, 77, 1] = native_layer_norm_17[2];  native_layer_norm_17 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_52: f16[768, 3072] = torch.ops.aten.t.default(arg142_1);  arg142_1 = None\n",
      "        view_186: f16[77, 768] = torch.ops.aten.view.default(getitem_51, [77, 768]);  getitem_51 = None\n",
      "        addmm_52: f16[77, 3072] = torch.ops.aten.addmm.default(arg143_1, view_186, t_52);  arg143_1 = view_186 = t_52 = None\n",
      "        view_187: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_52, [1, 77, 3072]);  addmm_52 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_25: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_187, 1.702)\n",
      "        sigmoid_8: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_25);  mul_25 = None\n",
      "        mul_26: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_187, sigmoid_8);  view_187 = sigmoid_8 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_53: f16[3072, 768] = torch.ops.aten.t.default(arg144_1);  arg144_1 = None\n",
      "        view_188: f16[77, 3072] = torch.ops.aten.view.default(mul_26, [77, 3072]);  mul_26 = None\n",
      "        addmm_53: f16[77, 768] = torch.ops.aten.addmm.default(arg145_1, view_188, t_53);  arg145_1 = view_188 = t_53 = None\n",
      "        view_189: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_53, [1, 77, 768]);  addmm_53 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_27: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_26, view_189);  add_26 = view_189 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm_18 = torch.ops.aten.native_layer_norm.default(add_27, [768], arg146_1, arg147_1, 1e-05);  arg146_1 = arg147_1 = None\n",
      "        getitem_54: f16[1, 77, 768] = native_layer_norm_18[0]\n",
      "        getitem_55: f32[1, 77, 1] = native_layer_norm_18[1]\n",
      "        getitem_56: f32[1, 77, 1] = native_layer_norm_18[2];  native_layer_norm_18 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t_54: f16[768, 768] = torch.ops.aten.t.default(arg148_1);  arg148_1 = None\n",
      "        view_190: f16[77, 768] = torch.ops.aten.view.default(getitem_54, [77, 768])\n",
      "        addmm_54: f16[77, 768] = torch.ops.aten.addmm.default(arg149_1, view_190, t_54);  arg149_1 = view_190 = t_54 = None\n",
      "        view_191: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_54, [1, 77, 768]);  addmm_54 = None\n",
      "        mul_27: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_191, 0.125);  view_191 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_55: f16[768, 768] = torch.ops.aten.t.default(arg150_1);  arg150_1 = None\n",
      "        view_192: f16[77, 768] = torch.ops.aten.view.default(getitem_54, [77, 768])\n",
      "        addmm_55: f16[77, 768] = torch.ops.aten.addmm.default(arg151_1, view_192, t_55);  arg151_1 = view_192 = t_55 = None\n",
      "        view_193: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_55, [1, 77, 768]);  addmm_55 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_194: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_193, [1, -1, 12, 64]);  view_193 = None\n",
      "        transpose_45: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_194, 1, 2);  view_194 = None\n",
      "        clone_36: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_45, memory_format = torch.contiguous_format);  transpose_45 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_56: f16[768, 768] = torch.ops.aten.t.default(arg152_1);  arg152_1 = None\n",
      "        view_195: f16[77, 768] = torch.ops.aten.view.default(getitem_54, [77, 768]);  getitem_54 = None\n",
      "        addmm_56: f16[77, 768] = torch.ops.aten.addmm.default(arg153_1, view_195, t_56);  arg153_1 = view_195 = t_56 = None\n",
      "        view_196: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_56, [1, 77, 768]);  addmm_56 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_197: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_196, [1, -1, 12, 64]);  view_196 = None\n",
      "        transpose_46: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_197, 1, 2);  view_197 = None\n",
      "        clone_37: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_46, memory_format = torch.contiguous_format);  transpose_46 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_198: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul_27, [1, 77, 12, 64]);  mul_27 = None\n",
      "        transpose_47: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_198, 1, 2);  view_198 = None\n",
      "        clone_38: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_47, memory_format = torch.contiguous_format);  transpose_47 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_199: f16[12, 77, 64] = torch.ops.aten.view.default(clone_38, [12, -1, 64]);  clone_38 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_200: f16[12, 77, 64] = torch.ops.aten.view.default(clone_36, [12, -1, 64]);  clone_36 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_201: f16[12, 77, 64] = torch.ops.aten.view.default(clone_37, [12, -1, 64]);  clone_37 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_48: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_200, 1, 2);  view_200 = None\n",
      "        bmm_18: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_199, transpose_48);  view_199 = transpose_48 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_202: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm_18, [1, 12, 77, 77]);  bmm_18 = None\n",
      "        add_28: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_202, _to_copy_1);  view_202 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_203: f16[12, 77, 77] = torch.ops.aten.view.default(add_28, [12, 77, 77]);  add_28 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax_9: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_203, -1, False);  view_203 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_19: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax_9, view_201);  _softmax_9 = view_201 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_204: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_19, [1, 12, 77, 64]);  bmm_19 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_49: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_204, 1, 2);  view_204 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_39: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_49, memory_format = torch.contiguous_format);  transpose_49 = None\n",
      "        _unsafe_view_9: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_39, [1, 77, 768]);  clone_39 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_57: f16[768, 768] = torch.ops.aten.t.default(arg154_1);  arg154_1 = None\n",
      "        view_205: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view_9, [77, 768]);  _unsafe_view_9 = None\n",
      "        addmm_57: f16[77, 768] = torch.ops.aten.addmm.default(arg155_1, view_205, t_57);  arg155_1 = view_205 = t_57 = None\n",
      "        view_206: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_57, [1, 77, 768]);  addmm_57 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_29: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_27, view_206);  add_27 = view_206 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_19 = torch.ops.aten.native_layer_norm.default(add_29, [768], arg156_1, arg157_1, 1e-05);  arg156_1 = arg157_1 = None\n",
      "        getitem_57: f16[1, 77, 768] = native_layer_norm_19[0]\n",
      "        getitem_58: f32[1, 77, 1] = native_layer_norm_19[1]\n",
      "        getitem_59: f32[1, 77, 1] = native_layer_norm_19[2];  native_layer_norm_19 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_58: f16[768, 3072] = torch.ops.aten.t.default(arg158_1);  arg158_1 = None\n",
      "        view_207: f16[77, 768] = torch.ops.aten.view.default(getitem_57, [77, 768]);  getitem_57 = None\n",
      "        addmm_58: f16[77, 3072] = torch.ops.aten.addmm.default(arg159_1, view_207, t_58);  arg159_1 = view_207 = t_58 = None\n",
      "        view_208: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_58, [1, 77, 3072]);  addmm_58 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_28: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_208, 1.702)\n",
      "        sigmoid_9: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_28);  mul_28 = None\n",
      "        mul_29: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_208, sigmoid_9);  view_208 = sigmoid_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_59: f16[3072, 768] = torch.ops.aten.t.default(arg160_1);  arg160_1 = None\n",
      "        view_209: f16[77, 3072] = torch.ops.aten.view.default(mul_29, [77, 3072]);  mul_29 = None\n",
      "        addmm_59: f16[77, 768] = torch.ops.aten.addmm.default(arg161_1, view_209, t_59);  arg161_1 = view_209 = t_59 = None\n",
      "        view_210: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_59, [1, 77, 768]);  addmm_59 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_30: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_29, view_210);  add_29 = view_210 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm_20 = torch.ops.aten.native_layer_norm.default(add_30, [768], arg162_1, arg163_1, 1e-05);  arg162_1 = arg163_1 = None\n",
      "        getitem_60: f16[1, 77, 768] = native_layer_norm_20[0]\n",
      "        getitem_61: f32[1, 77, 1] = native_layer_norm_20[1]\n",
      "        getitem_62: f32[1, 77, 1] = native_layer_norm_20[2];  native_layer_norm_20 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t_60: f16[768, 768] = torch.ops.aten.t.default(arg164_1);  arg164_1 = None\n",
      "        view_211: f16[77, 768] = torch.ops.aten.view.default(getitem_60, [77, 768])\n",
      "        addmm_60: f16[77, 768] = torch.ops.aten.addmm.default(arg165_1, view_211, t_60);  arg165_1 = view_211 = t_60 = None\n",
      "        view_212: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_60, [1, 77, 768]);  addmm_60 = None\n",
      "        mul_30: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_212, 0.125);  view_212 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_61: f16[768, 768] = torch.ops.aten.t.default(arg166_1);  arg166_1 = None\n",
      "        view_213: f16[77, 768] = torch.ops.aten.view.default(getitem_60, [77, 768])\n",
      "        addmm_61: f16[77, 768] = torch.ops.aten.addmm.default(arg167_1, view_213, t_61);  arg167_1 = view_213 = t_61 = None\n",
      "        view_214: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_61, [1, 77, 768]);  addmm_61 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_215: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_214, [1, -1, 12, 64]);  view_214 = None\n",
      "        transpose_50: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_215, 1, 2);  view_215 = None\n",
      "        clone_40: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_50, memory_format = torch.contiguous_format);  transpose_50 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_62: f16[768, 768] = torch.ops.aten.t.default(arg168_1);  arg168_1 = None\n",
      "        view_216: f16[77, 768] = torch.ops.aten.view.default(getitem_60, [77, 768]);  getitem_60 = None\n",
      "        addmm_62: f16[77, 768] = torch.ops.aten.addmm.default(arg169_1, view_216, t_62);  arg169_1 = view_216 = t_62 = None\n",
      "        view_217: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_62, [1, 77, 768]);  addmm_62 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_218: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_217, [1, -1, 12, 64]);  view_217 = None\n",
      "        transpose_51: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_218, 1, 2);  view_218 = None\n",
      "        clone_41: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_51, memory_format = torch.contiguous_format);  transpose_51 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_219: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul_30, [1, 77, 12, 64]);  mul_30 = None\n",
      "        transpose_52: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_219, 1, 2);  view_219 = None\n",
      "        clone_42: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_52, memory_format = torch.contiguous_format);  transpose_52 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_220: f16[12, 77, 64] = torch.ops.aten.view.default(clone_42, [12, -1, 64]);  clone_42 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_221: f16[12, 77, 64] = torch.ops.aten.view.default(clone_40, [12, -1, 64]);  clone_40 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_222: f16[12, 77, 64] = torch.ops.aten.view.default(clone_41, [12, -1, 64]);  clone_41 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_53: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_221, 1, 2);  view_221 = None\n",
      "        bmm_20: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_220, transpose_53);  view_220 = transpose_53 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_223: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm_20, [1, 12, 77, 77]);  bmm_20 = None\n",
      "        add_31: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_223, _to_copy_1);  view_223 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_224: f16[12, 77, 77] = torch.ops.aten.view.default(add_31, [12, 77, 77]);  add_31 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax_10: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_224, -1, False);  view_224 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_21: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax_10, view_222);  _softmax_10 = view_222 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_225: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_21, [1, 12, 77, 64]);  bmm_21 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_54: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_225, 1, 2);  view_225 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_43: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_54, memory_format = torch.contiguous_format);  transpose_54 = None\n",
      "        _unsafe_view_10: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_43, [1, 77, 768]);  clone_43 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_63: f16[768, 768] = torch.ops.aten.t.default(arg170_1);  arg170_1 = None\n",
      "        view_226: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view_10, [77, 768]);  _unsafe_view_10 = None\n",
      "        addmm_63: f16[77, 768] = torch.ops.aten.addmm.default(arg171_1, view_226, t_63);  arg171_1 = view_226 = t_63 = None\n",
      "        view_227: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_63, [1, 77, 768]);  addmm_63 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_32: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_30, view_227);  add_30 = view_227 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_21 = torch.ops.aten.native_layer_norm.default(add_32, [768], arg172_1, arg173_1, 1e-05);  arg172_1 = arg173_1 = None\n",
      "        getitem_63: f16[1, 77, 768] = native_layer_norm_21[0]\n",
      "        getitem_64: f32[1, 77, 1] = native_layer_norm_21[1]\n",
      "        getitem_65: f32[1, 77, 1] = native_layer_norm_21[2];  native_layer_norm_21 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_64: f16[768, 3072] = torch.ops.aten.t.default(arg174_1);  arg174_1 = None\n",
      "        view_228: f16[77, 768] = torch.ops.aten.view.default(getitem_63, [77, 768]);  getitem_63 = None\n",
      "        addmm_64: f16[77, 3072] = torch.ops.aten.addmm.default(arg175_1, view_228, t_64);  arg175_1 = view_228 = t_64 = None\n",
      "        view_229: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_64, [1, 77, 3072]);  addmm_64 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_31: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_229, 1.702)\n",
      "        sigmoid_10: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_31);  mul_31 = None\n",
      "        mul_32: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_229, sigmoid_10);  view_229 = sigmoid_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_65: f16[3072, 768] = torch.ops.aten.t.default(arg176_1);  arg176_1 = None\n",
      "        view_230: f16[77, 3072] = torch.ops.aten.view.default(mul_32, [77, 3072]);  mul_32 = None\n",
      "        addmm_65: f16[77, 768] = torch.ops.aten.addmm.default(arg177_1, view_230, t_65);  arg177_1 = view_230 = t_65 = None\n",
      "        view_231: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_65, [1, 77, 768]);  addmm_65 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_33: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_32, view_231);  add_32 = view_231 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm_22 = torch.ops.aten.native_layer_norm.default(add_33, [768], arg178_1, arg179_1, 1e-05);  arg178_1 = arg179_1 = None\n",
      "        getitem_66: f16[1, 77, 768] = native_layer_norm_22[0]\n",
      "        getitem_67: f32[1, 77, 1] = native_layer_norm_22[1]\n",
      "        getitem_68: f32[1, 77, 1] = native_layer_norm_22[2];  native_layer_norm_22 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t_66: f16[768, 768] = torch.ops.aten.t.default(arg180_1);  arg180_1 = None\n",
      "        view_232: f16[77, 768] = torch.ops.aten.view.default(getitem_66, [77, 768])\n",
      "        addmm_66: f16[77, 768] = torch.ops.aten.addmm.default(arg181_1, view_232, t_66);  arg181_1 = view_232 = t_66 = None\n",
      "        view_233: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_66, [1, 77, 768]);  addmm_66 = None\n",
      "        mul_33: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_233, 0.125);  view_233 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_67: f16[768, 768] = torch.ops.aten.t.default(arg182_1);  arg182_1 = None\n",
      "        view_234: f16[77, 768] = torch.ops.aten.view.default(getitem_66, [77, 768])\n",
      "        addmm_67: f16[77, 768] = torch.ops.aten.addmm.default(arg183_1, view_234, t_67);  arg183_1 = view_234 = t_67 = None\n",
      "        view_235: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_67, [1, 77, 768]);  addmm_67 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_236: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_235, [1, -1, 12, 64]);  view_235 = None\n",
      "        transpose_55: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_236, 1, 2);  view_236 = None\n",
      "        clone_44: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_55, memory_format = torch.contiguous_format);  transpose_55 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_68: f16[768, 768] = torch.ops.aten.t.default(arg184_1);  arg184_1 = None\n",
      "        view_237: f16[77, 768] = torch.ops.aten.view.default(getitem_66, [77, 768]);  getitem_66 = None\n",
      "        addmm_68: f16[77, 768] = torch.ops.aten.addmm.default(arg185_1, view_237, t_68);  arg185_1 = view_237 = t_68 = None\n",
      "        view_238: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_68, [1, 77, 768]);  addmm_68 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_239: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_238, [1, -1, 12, 64]);  view_238 = None\n",
      "        transpose_56: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_239, 1, 2);  view_239 = None\n",
      "        clone_45: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_56, memory_format = torch.contiguous_format);  transpose_56 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_240: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul_33, [1, 77, 12, 64]);  mul_33 = None\n",
      "        transpose_57: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_240, 1, 2);  view_240 = None\n",
      "        clone_46: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_57, memory_format = torch.contiguous_format);  transpose_57 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_241: f16[12, 77, 64] = torch.ops.aten.view.default(clone_46, [12, -1, 64]);  clone_46 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_242: f16[12, 77, 64] = torch.ops.aten.view.default(clone_44, [12, -1, 64]);  clone_44 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_243: f16[12, 77, 64] = torch.ops.aten.view.default(clone_45, [12, -1, 64]);  clone_45 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_58: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_242, 1, 2);  view_242 = None\n",
      "        bmm_22: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_241, transpose_58);  view_241 = transpose_58 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_244: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm_22, [1, 12, 77, 77]);  bmm_22 = None\n",
      "        add_34: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_244, _to_copy_1);  view_244 = _to_copy_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_245: f16[12, 77, 77] = torch.ops.aten.view.default(add_34, [12, 77, 77]);  add_34 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax_11: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_245, -1, False);  view_245 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_23: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax_11, view_243);  _softmax_11 = view_243 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_246: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_23, [1, 12, 77, 64]);  bmm_23 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_59: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_246, 1, 2);  view_246 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_47: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_59, memory_format = torch.contiguous_format);  transpose_59 = None\n",
      "        _unsafe_view_11: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_47, [1, 77, 768]);  clone_47 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_69: f16[768, 768] = torch.ops.aten.t.default(arg186_1);  arg186_1 = None\n",
      "        view_247: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view_11, [77, 768]);  _unsafe_view_11 = None\n",
      "        addmm_69: f16[77, 768] = torch.ops.aten.addmm.default(arg187_1, view_247, t_69);  arg187_1 = view_247 = t_69 = None\n",
      "        view_248: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_69, [1, 77, 768]);  addmm_69 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_35: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_33, view_248);  add_33 = view_248 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_23 = torch.ops.aten.native_layer_norm.default(add_35, [768], arg188_1, arg189_1, 1e-05);  arg188_1 = arg189_1 = None\n",
      "        getitem_69: f16[1, 77, 768] = native_layer_norm_23[0]\n",
      "        getitem_70: f32[1, 77, 1] = native_layer_norm_23[1]\n",
      "        getitem_71: f32[1, 77, 1] = native_layer_norm_23[2];  native_layer_norm_23 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_70: f16[768, 3072] = torch.ops.aten.t.default(arg190_1);  arg190_1 = None\n",
      "        view_249: f16[77, 768] = torch.ops.aten.view.default(getitem_69, [77, 768]);  getitem_69 = None\n",
      "        addmm_70: f16[77, 3072] = torch.ops.aten.addmm.default(arg191_1, view_249, t_70);  arg191_1 = view_249 = t_70 = None\n",
      "        view_250: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_70, [1, 77, 3072]);  addmm_70 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_34: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_250, 1.702)\n",
      "        sigmoid_11: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_34);  mul_34 = None\n",
      "        mul_35: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_250, sigmoid_11);  view_250 = sigmoid_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_71: f16[3072, 768] = torch.ops.aten.t.default(arg192_1);  arg192_1 = None\n",
      "        view_251: f16[77, 3072] = torch.ops.aten.view.default(mul_35, [77, 3072]);  mul_35 = None\n",
      "        addmm_71: f16[77, 768] = torch.ops.aten.addmm.default(arg193_1, view_251, t_71);  arg193_1 = view_251 = t_71 = None\n",
      "        view_252: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_71, [1, 77, 768]);  addmm_71 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_36: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_35, view_252);  add_35 = view_252 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:731, code: last_hidden_state = self.final_layer_norm(last_hidden_state)\n",
      "        native_layer_norm_24 = torch.ops.aten.native_layer_norm.default(add_36, [768], arg194_1, arg195_1, 1e-05);  add_36 = arg194_1 = arg195_1 = None\n",
      "        getitem_72: f16[1, 77, 768] = native_layer_norm_24[0]\n",
      "        getitem_73: f32[1, 77, 1] = native_layer_norm_24[1]\n",
      "        getitem_74: f32[1, 77, 1] = native_layer_norm_24[2];  native_layer_norm_24 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:737, code: torch.arange(last_hidden_state.shape[0], device=input_ids.device), input_ids.to(torch.int).argmax(dim=-1)\n",
      "        arange: i64[1] = torch.ops.aten.arange.default(1, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        _to_copy_2: i32[1, 77] = torch.ops.aten._to_copy.default(view, dtype = torch.int32);  view = None\n",
      "        argmax: i64[1] = torch.ops.aten.argmax.default(_to_copy_2, -1);  _to_copy_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:273, code: text_embeddings = text_embeddings.repeat(1, num_images_per_prompt, 1)\n",
      "        repeat: f16[1, 77, 768] = torch.ops.aten.repeat.default(getitem_72, [1, 1, 1]);  getitem_72 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:274, code: text_embeddings = text_embeddings.view(bs_embed * num_images_per_prompt, seq_len, -1)\n",
      "        view_253: f16[1, 77, 768] = torch.ops.aten.view.default(repeat, [1, 77, -1]);  repeat = None\n",
      "        return (view_253,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[49408, 768], arg1_1: f16[77, 768], arg2_1: f16[768], arg3_1: f16[768], arg4_1: f16[768, 768], arg5_1: f16[768], arg6_1: f16[768, 768], arg7_1: f16[768], arg8_1: f16[768, 768], arg9_1: f16[768], arg10_1: f16[768, 768], arg11_1: f16[768], arg12_1: f16[768], arg13_1: f16[768], arg14_1: f16[3072, 768], arg15_1: f16[3072], arg16_1: f16[768, 3072], arg17_1: f16[768], arg18_1: f16[768], arg19_1: f16[768], arg20_1: f16[768, 768], arg21_1: f16[768], arg22_1: f16[768, 768], arg23_1: f16[768], arg24_1: f16[768, 768], arg25_1: f16[768], arg26_1: f16[768, 768], arg27_1: f16[768], arg28_1: f16[768], arg29_1: f16[768], arg30_1: f16[3072, 768], arg31_1: f16[3072], arg32_1: f16[768, 3072], arg33_1: f16[768], arg34_1: f16[768], arg35_1: f16[768], arg36_1: f16[768, 768], arg37_1: f16[768], arg38_1: f16[768, 768], arg39_1: f16[768], arg40_1: f16[768, 768], arg41_1: f16[768], arg42_1: f16[768, 768], arg43_1: f16[768], arg44_1: f16[768], arg45_1: f16[768], arg46_1: f16[3072, 768], arg47_1: f16[3072], arg48_1: f16[768, 3072], arg49_1: f16[768], arg50_1: f16[768], arg51_1: f16[768], arg52_1: f16[768, 768], arg53_1: f16[768], arg54_1: f16[768, 768], arg55_1: f16[768], arg56_1: f16[768, 768], arg57_1: f16[768], arg58_1: f16[768, 768], arg59_1: f16[768], arg60_1: f16[768], arg61_1: f16[768], arg62_1: f16[3072, 768], arg63_1: f16[3072], arg64_1: f16[768, 3072], arg65_1: f16[768], arg66_1: f16[768], arg67_1: f16[768], arg68_1: f16[768, 768], arg69_1: f16[768], arg70_1: f16[768, 768], arg71_1: f16[768], arg72_1: f16[768, 768], arg73_1: f16[768], arg74_1: f16[768, 768], arg75_1: f16[768], arg76_1: f16[768], arg77_1: f16[768], arg78_1: f16[3072, 768], arg79_1: f16[3072], arg80_1: f16[768, 3072], arg81_1: f16[768], arg82_1: f16[768], arg83_1: f16[768], arg84_1: f16[768, 768], arg85_1: f16[768], arg86_1: f16[768, 768], arg87_1: f16[768], arg88_1: f16[768, 768], arg89_1: f16[768], arg90_1: f16[768, 768], arg91_1: f16[768], arg92_1: f16[768], arg93_1: f16[768], arg94_1: f16[3072, 768], arg95_1: f16[3072], arg96_1: f16[768, 3072], arg97_1: f16[768], arg98_1: f16[768], arg99_1: f16[768], arg100_1: f16[768, 768], arg101_1: f16[768], arg102_1: f16[768, 768], arg103_1: f16[768], arg104_1: f16[768, 768], arg105_1: f16[768], arg106_1: f16[768, 768], arg107_1: f16[768], arg108_1: f16[768], arg109_1: f16[768], arg110_1: f16[3072, 768], arg111_1: f16[3072], arg112_1: f16[768, 3072], arg113_1: f16[768], arg114_1: f16[768], arg115_1: f16[768], arg116_1: f16[768, 768], arg117_1: f16[768], arg118_1: f16[768, 768], arg119_1: f16[768], arg120_1: f16[768, 768], arg121_1: f16[768], arg122_1: f16[768, 768], arg123_1: f16[768], arg124_1: f16[768], arg125_1: f16[768], arg126_1: f16[3072, 768], arg127_1: f16[3072], arg128_1: f16[768, 3072], arg129_1: f16[768], arg130_1: f16[768], arg131_1: f16[768], arg132_1: f16[768, 768], arg133_1: f16[768], arg134_1: f16[768, 768], arg135_1: f16[768], arg136_1: f16[768, 768], arg137_1: f16[768], arg138_1: f16[768, 768], arg139_1: f16[768], arg140_1: f16[768], arg141_1: f16[768], arg142_1: f16[3072, 768], arg143_1: f16[3072], arg144_1: f16[768, 3072], arg145_1: f16[768], arg146_1: f16[768], arg147_1: f16[768], arg148_1: f16[768, 768], arg149_1: f16[768], arg150_1: f16[768, 768], arg151_1: f16[768], arg152_1: f16[768, 768], arg153_1: f16[768], arg154_1: f16[768, 768], arg155_1: f16[768], arg156_1: f16[768], arg157_1: f16[768], arg158_1: f16[3072, 768], arg159_1: f16[3072], arg160_1: f16[768, 3072], arg161_1: f16[768], arg162_1: f16[768], arg163_1: f16[768], arg164_1: f16[768, 768], arg165_1: f16[768], arg166_1: f16[768, 768], arg167_1: f16[768], arg168_1: f16[768, 768], arg169_1: f16[768], arg170_1: f16[768, 768], arg171_1: f16[768], arg172_1: f16[768], arg173_1: f16[768], arg174_1: f16[3072, 768], arg175_1: f16[3072], arg176_1: f16[768, 3072], arg177_1: f16[768], arg178_1: f16[768], arg179_1: f16[768], arg180_1: f16[768, 768], arg181_1: f16[768], arg182_1: f16[768, 768], arg183_1: f16[768], arg184_1: f16[768, 768], arg185_1: f16[768], arg186_1: f16[768, 768], arg187_1: f16[768], arg188_1: f16[768], arg189_1: f16[768], arg190_1: f16[3072, 768], arg191_1: f16[3072], arg192_1: f16[768, 3072], arg193_1: f16[768], arg194_1: f16[768], arg195_1: f16[768], arg196_1: i64[1, 77], arg197_1: f16[1, 77, 768], arg198_1: i64[1, 77]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:312, code: uncond_input.input_ids.to(device),\n",
      "        _to_copy: i64[1, 77] = torch.ops.aten._to_copy.default(arg198_1, dtype = torch.int64, layout = torch.strided, device = device(type='cuda', index=0));  arg198_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:706, code: input_ids = input_ids.view(-1, input_shape[-1])\n",
      "        view: i64[1, 77] = torch.ops.aten.view.default(_to_copy, [-1, 77]);  _to_copy = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:220, code: position_ids = self.position_ids[:, :seq_length]\n",
      "        slice_1: i64[1, 77] = torch.ops.aten.slice.Tensor(arg196_1, 0, 0, 9223372036854775807);  arg196_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:223, code: inputs_embeds = self.token_embedding(input_ids)\n",
      "        embedding: f16[1, 77, 768] = torch.ops.aten.embedding.default(arg0_1, view);  arg0_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:225, code: position_embeddings = self.position_embedding(position_ids)\n",
      "        embedding_1: f16[1, 77, 768] = torch.ops.aten.embedding.default(arg1_1, slice_1);  arg1_1 = slice_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:226, code: embeddings = inputs_embeds + position_embeddings\n",
      "        add: f16[1, 77, 768] = torch.ops.aten.add.Tensor(embedding, embedding_1);  embedding = embedding_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:753, code: mask = torch.empty(bsz, seq_len, seq_len, dtype=dtype)\n",
      "        empty: f16[1, 77, 77] = torch.ops.aten.empty.memory_format([1, 77, 77], dtype = torch.float16, device = device(type='cpu'), pin_memory = False)\n",
      "        \n",
      "        # No stacktrace found for following nodes\n",
      "        _tensor_constant0 = self._tensor_constant0\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:754, code: mask.fill_(torch.tensor(torch.finfo(dtype).min))\n",
      "        lift_fresh_copy: f32[] = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None\n",
      "        fill_: f16[1, 77, 77] = torch.ops.aten.fill_.Tensor(empty, lift_fresh_copy);  empty = lift_fresh_copy = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:755, code: mask.triu_(1)  # zero out the lower diagonal\n",
      "        triu_: f16[1, 77, 77] = torch.ops.aten.triu_.default(fill_, 1);  fill_ = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:756, code: mask = mask.unsqueeze(1)  # expand mask\n",
      "        unsqueeze: f16[1, 1, 77, 77] = torch.ops.aten.unsqueeze.default(triu_, 1);  triu_ = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:713, code: causal_attention_mask = self._build_causal_attention_mask(bsz, seq_len, hidden_states.dtype).to(\n",
      "        _to_copy_1: f16[1, 1, 77, 77] = torch.ops.aten._to_copy.default(unsqueeze, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0));  unsqueeze = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm = torch.ops.aten.native_layer_norm.default(add, [768], arg2_1, arg3_1, 1e-05);  arg2_1 = arg3_1 = None\n",
      "        getitem: f16[1, 77, 768] = native_layer_norm[0]\n",
      "        getitem_1: f32[1, 77, 1] = native_layer_norm[1]\n",
      "        getitem_2: f32[1, 77, 1] = native_layer_norm[2];  native_layer_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t: f16[768, 768] = torch.ops.aten.t.default(arg4_1);  arg4_1 = None\n",
      "        view_1: f16[77, 768] = torch.ops.aten.view.default(getitem, [77, 768])\n",
      "        addmm: f16[77, 768] = torch.ops.aten.addmm.default(arg5_1, view_1, t);  arg5_1 = view_1 = t = None\n",
      "        view_2: f16[1, 77, 768] = torch.ops.aten.view.default(addmm, [1, 77, 768]);  addmm = None\n",
      "        mul: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_2, 0.125);  view_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_1: f16[768, 768] = torch.ops.aten.t.default(arg6_1);  arg6_1 = None\n",
      "        view_3: f16[77, 768] = torch.ops.aten.view.default(getitem, [77, 768])\n",
      "        addmm_1: f16[77, 768] = torch.ops.aten.addmm.default(arg7_1, view_3, t_1);  arg7_1 = view_3 = t_1 = None\n",
      "        view_4: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_1, [1, 77, 768]);  addmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_5: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_4, [1, -1, 12, 64]);  view_4 = None\n",
      "        transpose: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_5, 1, 2);  view_5 = None\n",
      "        clone: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose, memory_format = torch.contiguous_format);  transpose = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_2: f16[768, 768] = torch.ops.aten.t.default(arg8_1);  arg8_1 = None\n",
      "        view_6: f16[77, 768] = torch.ops.aten.view.default(getitem, [77, 768]);  getitem = None\n",
      "        addmm_2: f16[77, 768] = torch.ops.aten.addmm.default(arg9_1, view_6, t_2);  arg9_1 = view_6 = t_2 = None\n",
      "        view_7: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_2, [1, 77, 768]);  addmm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_8: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_7, [1, -1, 12, 64]);  view_7 = None\n",
      "        transpose_1: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_8, 1, 2);  view_8 = None\n",
      "        clone_1: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_1, memory_format = torch.contiguous_format);  transpose_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_9: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul, [1, 77, 12, 64]);  mul = None\n",
      "        transpose_2: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_9, 1, 2);  view_9 = None\n",
      "        clone_2: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_2, memory_format = torch.contiguous_format);  transpose_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_10: f16[12, 77, 64] = torch.ops.aten.view.default(clone_2, [12, -1, 64]);  clone_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_11: f16[12, 77, 64] = torch.ops.aten.view.default(clone, [12, -1, 64]);  clone = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_12: f16[12, 77, 64] = torch.ops.aten.view.default(clone_1, [12, -1, 64]);  clone_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_3: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_11, 1, 2);  view_11 = None\n",
      "        bmm: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_10, transpose_3);  view_10 = transpose_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_13: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm, [1, 12, 77, 77]);  bmm = None\n",
      "        add_1: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_13, _to_copy_1);  view_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_14: f16[12, 77, 77] = torch.ops.aten.view.default(add_1, [12, 77, 77]);  add_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_14, -1, False);  view_14 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_1: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax, view_12);  _softmax = view_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_15: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_1, [1, 12, 77, 64]);  bmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_4: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_15, 1, 2);  view_15 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_3: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_4, memory_format = torch.contiguous_format);  transpose_4 = None\n",
      "        _unsafe_view: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_3, [1, 77, 768]);  clone_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_3: f16[768, 768] = torch.ops.aten.t.default(arg10_1);  arg10_1 = None\n",
      "        view_16: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view, [77, 768]);  _unsafe_view = None\n",
      "        addmm_3: f16[77, 768] = torch.ops.aten.addmm.default(arg11_1, view_16, t_3);  arg11_1 = view_16 = t_3 = None\n",
      "        view_17: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_3, [1, 77, 768]);  addmm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_2: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add, view_17);  add = view_17 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_1 = torch.ops.aten.native_layer_norm.default(add_2, [768], arg12_1, arg13_1, 1e-05);  arg12_1 = arg13_1 = None\n",
      "        getitem_3: f16[1, 77, 768] = native_layer_norm_1[0]\n",
      "        getitem_4: f32[1, 77, 1] = native_layer_norm_1[1]\n",
      "        getitem_5: f32[1, 77, 1] = native_layer_norm_1[2];  native_layer_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_4: f16[768, 3072] = torch.ops.aten.t.default(arg14_1);  arg14_1 = None\n",
      "        view_18: f16[77, 768] = torch.ops.aten.view.default(getitem_3, [77, 768]);  getitem_3 = None\n",
      "        addmm_4: f16[77, 3072] = torch.ops.aten.addmm.default(arg15_1, view_18, t_4);  arg15_1 = view_18 = t_4 = None\n",
      "        view_19: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_4, [1, 77, 3072]);  addmm_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_1: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_19, 1.702)\n",
      "        sigmoid: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_1);  mul_1 = None\n",
      "        mul_2: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_19, sigmoid);  view_19 = sigmoid = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_5: f16[3072, 768] = torch.ops.aten.t.default(arg16_1);  arg16_1 = None\n",
      "        view_20: f16[77, 3072] = torch.ops.aten.view.default(mul_2, [77, 3072]);  mul_2 = None\n",
      "        addmm_5: f16[77, 768] = torch.ops.aten.addmm.default(arg17_1, view_20, t_5);  arg17_1 = view_20 = t_5 = None\n",
      "        view_21: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_5, [1, 77, 768]);  addmm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_3: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_2, view_21);  add_2 = view_21 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm_2 = torch.ops.aten.native_layer_norm.default(add_3, [768], arg18_1, arg19_1, 1e-05);  arg18_1 = arg19_1 = None\n",
      "        getitem_6: f16[1, 77, 768] = native_layer_norm_2[0]\n",
      "        getitem_7: f32[1, 77, 1] = native_layer_norm_2[1]\n",
      "        getitem_8: f32[1, 77, 1] = native_layer_norm_2[2];  native_layer_norm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t_6: f16[768, 768] = torch.ops.aten.t.default(arg20_1);  arg20_1 = None\n",
      "        view_22: f16[77, 768] = torch.ops.aten.view.default(getitem_6, [77, 768])\n",
      "        addmm_6: f16[77, 768] = torch.ops.aten.addmm.default(arg21_1, view_22, t_6);  arg21_1 = view_22 = t_6 = None\n",
      "        view_23: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_6, [1, 77, 768]);  addmm_6 = None\n",
      "        mul_3: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_23, 0.125);  view_23 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_7: f16[768, 768] = torch.ops.aten.t.default(arg22_1);  arg22_1 = None\n",
      "        view_24: f16[77, 768] = torch.ops.aten.view.default(getitem_6, [77, 768])\n",
      "        addmm_7: f16[77, 768] = torch.ops.aten.addmm.default(arg23_1, view_24, t_7);  arg23_1 = view_24 = t_7 = None\n",
      "        view_25: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_7, [1, 77, 768]);  addmm_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_26: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_25, [1, -1, 12, 64]);  view_25 = None\n",
      "        transpose_5: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_26, 1, 2);  view_26 = None\n",
      "        clone_4: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_5, memory_format = torch.contiguous_format);  transpose_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_8: f16[768, 768] = torch.ops.aten.t.default(arg24_1);  arg24_1 = None\n",
      "        view_27: f16[77, 768] = torch.ops.aten.view.default(getitem_6, [77, 768]);  getitem_6 = None\n",
      "        addmm_8: f16[77, 768] = torch.ops.aten.addmm.default(arg25_1, view_27, t_8);  arg25_1 = view_27 = t_8 = None\n",
      "        view_28: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_8, [1, 77, 768]);  addmm_8 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_29: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_28, [1, -1, 12, 64]);  view_28 = None\n",
      "        transpose_6: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_29, 1, 2);  view_29 = None\n",
      "        clone_5: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_6, memory_format = torch.contiguous_format);  transpose_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_30: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul_3, [1, 77, 12, 64]);  mul_3 = None\n",
      "        transpose_7: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_30, 1, 2);  view_30 = None\n",
      "        clone_6: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_7, memory_format = torch.contiguous_format);  transpose_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_31: f16[12, 77, 64] = torch.ops.aten.view.default(clone_6, [12, -1, 64]);  clone_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_32: f16[12, 77, 64] = torch.ops.aten.view.default(clone_4, [12, -1, 64]);  clone_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_33: f16[12, 77, 64] = torch.ops.aten.view.default(clone_5, [12, -1, 64]);  clone_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_8: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_32, 1, 2);  view_32 = None\n",
      "        bmm_2: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_31, transpose_8);  view_31 = transpose_8 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_34: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm_2, [1, 12, 77, 77]);  bmm_2 = None\n",
      "        add_4: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_34, _to_copy_1);  view_34 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_35: f16[12, 77, 77] = torch.ops.aten.view.default(add_4, [12, 77, 77]);  add_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax_1: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_35, -1, False);  view_35 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_3: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax_1, view_33);  _softmax_1 = view_33 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_36: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_3, [1, 12, 77, 64]);  bmm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_9: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_36, 1, 2);  view_36 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_7: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_9, memory_format = torch.contiguous_format);  transpose_9 = None\n",
      "        _unsafe_view_1: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_7, [1, 77, 768]);  clone_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_9: f16[768, 768] = torch.ops.aten.t.default(arg26_1);  arg26_1 = None\n",
      "        view_37: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view_1, [77, 768]);  _unsafe_view_1 = None\n",
      "        addmm_9: f16[77, 768] = torch.ops.aten.addmm.default(arg27_1, view_37, t_9);  arg27_1 = view_37 = t_9 = None\n",
      "        view_38: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_9, [1, 77, 768]);  addmm_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_5: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_3, view_38);  add_3 = view_38 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_3 = torch.ops.aten.native_layer_norm.default(add_5, [768], arg28_1, arg29_1, 1e-05);  arg28_1 = arg29_1 = None\n",
      "        getitem_9: f16[1, 77, 768] = native_layer_norm_3[0]\n",
      "        getitem_10: f32[1, 77, 1] = native_layer_norm_3[1]\n",
      "        getitem_11: f32[1, 77, 1] = native_layer_norm_3[2];  native_layer_norm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_10: f16[768, 3072] = torch.ops.aten.t.default(arg30_1);  arg30_1 = None\n",
      "        view_39: f16[77, 768] = torch.ops.aten.view.default(getitem_9, [77, 768]);  getitem_9 = None\n",
      "        addmm_10: f16[77, 3072] = torch.ops.aten.addmm.default(arg31_1, view_39, t_10);  arg31_1 = view_39 = t_10 = None\n",
      "        view_40: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_10, [1, 77, 3072]);  addmm_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_4: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_40, 1.702)\n",
      "        sigmoid_1: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_4);  mul_4 = None\n",
      "        mul_5: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_40, sigmoid_1);  view_40 = sigmoid_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_11: f16[3072, 768] = torch.ops.aten.t.default(arg32_1);  arg32_1 = None\n",
      "        view_41: f16[77, 3072] = torch.ops.aten.view.default(mul_5, [77, 3072]);  mul_5 = None\n",
      "        addmm_11: f16[77, 768] = torch.ops.aten.addmm.default(arg33_1, view_41, t_11);  arg33_1 = view_41 = t_11 = None\n",
      "        view_42: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_11, [1, 77, 768]);  addmm_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_6: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_5, view_42);  add_5 = view_42 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm_4 = torch.ops.aten.native_layer_norm.default(add_6, [768], arg34_1, arg35_1, 1e-05);  arg34_1 = arg35_1 = None\n",
      "        getitem_12: f16[1, 77, 768] = native_layer_norm_4[0]\n",
      "        getitem_13: f32[1, 77, 1] = native_layer_norm_4[1]\n",
      "        getitem_14: f32[1, 77, 1] = native_layer_norm_4[2];  native_layer_norm_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t_12: f16[768, 768] = torch.ops.aten.t.default(arg36_1);  arg36_1 = None\n",
      "        view_43: f16[77, 768] = torch.ops.aten.view.default(getitem_12, [77, 768])\n",
      "        addmm_12: f16[77, 768] = torch.ops.aten.addmm.default(arg37_1, view_43, t_12);  arg37_1 = view_43 = t_12 = None\n",
      "        view_44: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_12, [1, 77, 768]);  addmm_12 = None\n",
      "        mul_6: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_44, 0.125);  view_44 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_13: f16[768, 768] = torch.ops.aten.t.default(arg38_1);  arg38_1 = None\n",
      "        view_45: f16[77, 768] = torch.ops.aten.view.default(getitem_12, [77, 768])\n",
      "        addmm_13: f16[77, 768] = torch.ops.aten.addmm.default(arg39_1, view_45, t_13);  arg39_1 = view_45 = t_13 = None\n",
      "        view_46: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_13, [1, 77, 768]);  addmm_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_47: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_46, [1, -1, 12, 64]);  view_46 = None\n",
      "        transpose_10: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_47, 1, 2);  view_47 = None\n",
      "        clone_8: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_10, memory_format = torch.contiguous_format);  transpose_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_14: f16[768, 768] = torch.ops.aten.t.default(arg40_1);  arg40_1 = None\n",
      "        view_48: f16[77, 768] = torch.ops.aten.view.default(getitem_12, [77, 768]);  getitem_12 = None\n",
      "        addmm_14: f16[77, 768] = torch.ops.aten.addmm.default(arg41_1, view_48, t_14);  arg41_1 = view_48 = t_14 = None\n",
      "        view_49: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_14, [1, 77, 768]);  addmm_14 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_50: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_49, [1, -1, 12, 64]);  view_49 = None\n",
      "        transpose_11: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_50, 1, 2);  view_50 = None\n",
      "        clone_9: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_11, memory_format = torch.contiguous_format);  transpose_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_51: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul_6, [1, 77, 12, 64]);  mul_6 = None\n",
      "        transpose_12: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_51, 1, 2);  view_51 = None\n",
      "        clone_10: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_12, memory_format = torch.contiguous_format);  transpose_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_52: f16[12, 77, 64] = torch.ops.aten.view.default(clone_10, [12, -1, 64]);  clone_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_53: f16[12, 77, 64] = torch.ops.aten.view.default(clone_8, [12, -1, 64]);  clone_8 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_54: f16[12, 77, 64] = torch.ops.aten.view.default(clone_9, [12, -1, 64]);  clone_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_13: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_53, 1, 2);  view_53 = None\n",
      "        bmm_4: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_52, transpose_13);  view_52 = transpose_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_55: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm_4, [1, 12, 77, 77]);  bmm_4 = None\n",
      "        add_7: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_55, _to_copy_1);  view_55 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_56: f16[12, 77, 77] = torch.ops.aten.view.default(add_7, [12, 77, 77]);  add_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax_2: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_56, -1, False);  view_56 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_5: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax_2, view_54);  _softmax_2 = view_54 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_57: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_5, [1, 12, 77, 64]);  bmm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_14: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_57, 1, 2);  view_57 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_11: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_14, memory_format = torch.contiguous_format);  transpose_14 = None\n",
      "        _unsafe_view_2: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_11, [1, 77, 768]);  clone_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_15: f16[768, 768] = torch.ops.aten.t.default(arg42_1);  arg42_1 = None\n",
      "        view_58: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view_2, [77, 768]);  _unsafe_view_2 = None\n",
      "        addmm_15: f16[77, 768] = torch.ops.aten.addmm.default(arg43_1, view_58, t_15);  arg43_1 = view_58 = t_15 = None\n",
      "        view_59: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_15, [1, 77, 768]);  addmm_15 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_8: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_6, view_59);  add_6 = view_59 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_5 = torch.ops.aten.native_layer_norm.default(add_8, [768], arg44_1, arg45_1, 1e-05);  arg44_1 = arg45_1 = None\n",
      "        getitem_15: f16[1, 77, 768] = native_layer_norm_5[0]\n",
      "        getitem_16: f32[1, 77, 1] = native_layer_norm_5[1]\n",
      "        getitem_17: f32[1, 77, 1] = native_layer_norm_5[2];  native_layer_norm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_16: f16[768, 3072] = torch.ops.aten.t.default(arg46_1);  arg46_1 = None\n",
      "        view_60: f16[77, 768] = torch.ops.aten.view.default(getitem_15, [77, 768]);  getitem_15 = None\n",
      "        addmm_16: f16[77, 3072] = torch.ops.aten.addmm.default(arg47_1, view_60, t_16);  arg47_1 = view_60 = t_16 = None\n",
      "        view_61: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_16, [1, 77, 3072]);  addmm_16 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_7: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_61, 1.702)\n",
      "        sigmoid_2: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_7);  mul_7 = None\n",
      "        mul_8: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_61, sigmoid_2);  view_61 = sigmoid_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_17: f16[3072, 768] = torch.ops.aten.t.default(arg48_1);  arg48_1 = None\n",
      "        view_62: f16[77, 3072] = torch.ops.aten.view.default(mul_8, [77, 3072]);  mul_8 = None\n",
      "        addmm_17: f16[77, 768] = torch.ops.aten.addmm.default(arg49_1, view_62, t_17);  arg49_1 = view_62 = t_17 = None\n",
      "        view_63: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_17, [1, 77, 768]);  addmm_17 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_9: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_8, view_63);  add_8 = view_63 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm_6 = torch.ops.aten.native_layer_norm.default(add_9, [768], arg50_1, arg51_1, 1e-05);  arg50_1 = arg51_1 = None\n",
      "        getitem_18: f16[1, 77, 768] = native_layer_norm_6[0]\n",
      "        getitem_19: f32[1, 77, 1] = native_layer_norm_6[1]\n",
      "        getitem_20: f32[1, 77, 1] = native_layer_norm_6[2];  native_layer_norm_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t_18: f16[768, 768] = torch.ops.aten.t.default(arg52_1);  arg52_1 = None\n",
      "        view_64: f16[77, 768] = torch.ops.aten.view.default(getitem_18, [77, 768])\n",
      "        addmm_18: f16[77, 768] = torch.ops.aten.addmm.default(arg53_1, view_64, t_18);  arg53_1 = view_64 = t_18 = None\n",
      "        view_65: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_18, [1, 77, 768]);  addmm_18 = None\n",
      "        mul_9: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_65, 0.125);  view_65 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_19: f16[768, 768] = torch.ops.aten.t.default(arg54_1);  arg54_1 = None\n",
      "        view_66: f16[77, 768] = torch.ops.aten.view.default(getitem_18, [77, 768])\n",
      "        addmm_19: f16[77, 768] = torch.ops.aten.addmm.default(arg55_1, view_66, t_19);  arg55_1 = view_66 = t_19 = None\n",
      "        view_67: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_19, [1, 77, 768]);  addmm_19 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_68: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_67, [1, -1, 12, 64]);  view_67 = None\n",
      "        transpose_15: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_68, 1, 2);  view_68 = None\n",
      "        clone_12: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_15, memory_format = torch.contiguous_format);  transpose_15 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_20: f16[768, 768] = torch.ops.aten.t.default(arg56_1);  arg56_1 = None\n",
      "        view_69: f16[77, 768] = torch.ops.aten.view.default(getitem_18, [77, 768]);  getitem_18 = None\n",
      "        addmm_20: f16[77, 768] = torch.ops.aten.addmm.default(arg57_1, view_69, t_20);  arg57_1 = view_69 = t_20 = None\n",
      "        view_70: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_20, [1, 77, 768]);  addmm_20 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_71: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_70, [1, -1, 12, 64]);  view_70 = None\n",
      "        transpose_16: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_71, 1, 2);  view_71 = None\n",
      "        clone_13: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_16, memory_format = torch.contiguous_format);  transpose_16 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_72: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul_9, [1, 77, 12, 64]);  mul_9 = None\n",
      "        transpose_17: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_72, 1, 2);  view_72 = None\n",
      "        clone_14: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_17, memory_format = torch.contiguous_format);  transpose_17 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_73: f16[12, 77, 64] = torch.ops.aten.view.default(clone_14, [12, -1, 64]);  clone_14 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_74: f16[12, 77, 64] = torch.ops.aten.view.default(clone_12, [12, -1, 64]);  clone_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_75: f16[12, 77, 64] = torch.ops.aten.view.default(clone_13, [12, -1, 64]);  clone_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_18: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_74, 1, 2);  view_74 = None\n",
      "        bmm_6: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_73, transpose_18);  view_73 = transpose_18 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_76: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm_6, [1, 12, 77, 77]);  bmm_6 = None\n",
      "        add_10: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_76, _to_copy_1);  view_76 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_77: f16[12, 77, 77] = torch.ops.aten.view.default(add_10, [12, 77, 77]);  add_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax_3: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_77, -1, False);  view_77 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_7: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax_3, view_75);  _softmax_3 = view_75 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_78: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_7, [1, 12, 77, 64]);  bmm_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_19: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_78, 1, 2);  view_78 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_15: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_19, memory_format = torch.contiguous_format);  transpose_19 = None\n",
      "        _unsafe_view_3: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_15, [1, 77, 768]);  clone_15 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_21: f16[768, 768] = torch.ops.aten.t.default(arg58_1);  arg58_1 = None\n",
      "        view_79: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view_3, [77, 768]);  _unsafe_view_3 = None\n",
      "        addmm_21: f16[77, 768] = torch.ops.aten.addmm.default(arg59_1, view_79, t_21);  arg59_1 = view_79 = t_21 = None\n",
      "        view_80: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_21, [1, 77, 768]);  addmm_21 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_11: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_9, view_80);  add_9 = view_80 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_7 = torch.ops.aten.native_layer_norm.default(add_11, [768], arg60_1, arg61_1, 1e-05);  arg60_1 = arg61_1 = None\n",
      "        getitem_21: f16[1, 77, 768] = native_layer_norm_7[0]\n",
      "        getitem_22: f32[1, 77, 1] = native_layer_norm_7[1]\n",
      "        getitem_23: f32[1, 77, 1] = native_layer_norm_7[2];  native_layer_norm_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_22: f16[768, 3072] = torch.ops.aten.t.default(arg62_1);  arg62_1 = None\n",
      "        view_81: f16[77, 768] = torch.ops.aten.view.default(getitem_21, [77, 768]);  getitem_21 = None\n",
      "        addmm_22: f16[77, 3072] = torch.ops.aten.addmm.default(arg63_1, view_81, t_22);  arg63_1 = view_81 = t_22 = None\n",
      "        view_82: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_22, [1, 77, 3072]);  addmm_22 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_10: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_82, 1.702)\n",
      "        sigmoid_3: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_10);  mul_10 = None\n",
      "        mul_11: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_82, sigmoid_3);  view_82 = sigmoid_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_23: f16[3072, 768] = torch.ops.aten.t.default(arg64_1);  arg64_1 = None\n",
      "        view_83: f16[77, 3072] = torch.ops.aten.view.default(mul_11, [77, 3072]);  mul_11 = None\n",
      "        addmm_23: f16[77, 768] = torch.ops.aten.addmm.default(arg65_1, view_83, t_23);  arg65_1 = view_83 = t_23 = None\n",
      "        view_84: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_23, [1, 77, 768]);  addmm_23 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_12: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_11, view_84);  add_11 = view_84 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm_8 = torch.ops.aten.native_layer_norm.default(add_12, [768], arg66_1, arg67_1, 1e-05);  arg66_1 = arg67_1 = None\n",
      "        getitem_24: f16[1, 77, 768] = native_layer_norm_8[0]\n",
      "        getitem_25: f32[1, 77, 1] = native_layer_norm_8[1]\n",
      "        getitem_26: f32[1, 77, 1] = native_layer_norm_8[2];  native_layer_norm_8 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t_24: f16[768, 768] = torch.ops.aten.t.default(arg68_1);  arg68_1 = None\n",
      "        view_85: f16[77, 768] = torch.ops.aten.view.default(getitem_24, [77, 768])\n",
      "        addmm_24: f16[77, 768] = torch.ops.aten.addmm.default(arg69_1, view_85, t_24);  arg69_1 = view_85 = t_24 = None\n",
      "        view_86: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_24, [1, 77, 768]);  addmm_24 = None\n",
      "        mul_12: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_86, 0.125);  view_86 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_25: f16[768, 768] = torch.ops.aten.t.default(arg70_1);  arg70_1 = None\n",
      "        view_87: f16[77, 768] = torch.ops.aten.view.default(getitem_24, [77, 768])\n",
      "        addmm_25: f16[77, 768] = torch.ops.aten.addmm.default(arg71_1, view_87, t_25);  arg71_1 = view_87 = t_25 = None\n",
      "        view_88: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_25, [1, 77, 768]);  addmm_25 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_89: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_88, [1, -1, 12, 64]);  view_88 = None\n",
      "        transpose_20: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_89, 1, 2);  view_89 = None\n",
      "        clone_16: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_20, memory_format = torch.contiguous_format);  transpose_20 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_26: f16[768, 768] = torch.ops.aten.t.default(arg72_1);  arg72_1 = None\n",
      "        view_90: f16[77, 768] = torch.ops.aten.view.default(getitem_24, [77, 768]);  getitem_24 = None\n",
      "        addmm_26: f16[77, 768] = torch.ops.aten.addmm.default(arg73_1, view_90, t_26);  arg73_1 = view_90 = t_26 = None\n",
      "        view_91: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_26, [1, 77, 768]);  addmm_26 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_92: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_91, [1, -1, 12, 64]);  view_91 = None\n",
      "        transpose_21: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_92, 1, 2);  view_92 = None\n",
      "        clone_17: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_21, memory_format = torch.contiguous_format);  transpose_21 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_93: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul_12, [1, 77, 12, 64]);  mul_12 = None\n",
      "        transpose_22: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_93, 1, 2);  view_93 = None\n",
      "        clone_18: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_22, memory_format = torch.contiguous_format);  transpose_22 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_94: f16[12, 77, 64] = torch.ops.aten.view.default(clone_18, [12, -1, 64]);  clone_18 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_95: f16[12, 77, 64] = torch.ops.aten.view.default(clone_16, [12, -1, 64]);  clone_16 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_96: f16[12, 77, 64] = torch.ops.aten.view.default(clone_17, [12, -1, 64]);  clone_17 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_23: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_95, 1, 2);  view_95 = None\n",
      "        bmm_8: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_94, transpose_23);  view_94 = transpose_23 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_97: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm_8, [1, 12, 77, 77]);  bmm_8 = None\n",
      "        add_13: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_97, _to_copy_1);  view_97 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_98: f16[12, 77, 77] = torch.ops.aten.view.default(add_13, [12, 77, 77]);  add_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax_4: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_98, -1, False);  view_98 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_9: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax_4, view_96);  _softmax_4 = view_96 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_99: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_9, [1, 12, 77, 64]);  bmm_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_24: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_99, 1, 2);  view_99 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_19: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_24, memory_format = torch.contiguous_format);  transpose_24 = None\n",
      "        _unsafe_view_4: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_19, [1, 77, 768]);  clone_19 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_27: f16[768, 768] = torch.ops.aten.t.default(arg74_1);  arg74_1 = None\n",
      "        view_100: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view_4, [77, 768]);  _unsafe_view_4 = None\n",
      "        addmm_27: f16[77, 768] = torch.ops.aten.addmm.default(arg75_1, view_100, t_27);  arg75_1 = view_100 = t_27 = None\n",
      "        view_101: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_27, [1, 77, 768]);  addmm_27 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_14: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_12, view_101);  add_12 = view_101 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_9 = torch.ops.aten.native_layer_norm.default(add_14, [768], arg76_1, arg77_1, 1e-05);  arg76_1 = arg77_1 = None\n",
      "        getitem_27: f16[1, 77, 768] = native_layer_norm_9[0]\n",
      "        getitem_28: f32[1, 77, 1] = native_layer_norm_9[1]\n",
      "        getitem_29: f32[1, 77, 1] = native_layer_norm_9[2];  native_layer_norm_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_28: f16[768, 3072] = torch.ops.aten.t.default(arg78_1);  arg78_1 = None\n",
      "        view_102: f16[77, 768] = torch.ops.aten.view.default(getitem_27, [77, 768]);  getitem_27 = None\n",
      "        addmm_28: f16[77, 3072] = torch.ops.aten.addmm.default(arg79_1, view_102, t_28);  arg79_1 = view_102 = t_28 = None\n",
      "        view_103: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_28, [1, 77, 3072]);  addmm_28 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_13: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_103, 1.702)\n",
      "        sigmoid_4: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_13);  mul_13 = None\n",
      "        mul_14: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_103, sigmoid_4);  view_103 = sigmoid_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_29: f16[3072, 768] = torch.ops.aten.t.default(arg80_1);  arg80_1 = None\n",
      "        view_104: f16[77, 3072] = torch.ops.aten.view.default(mul_14, [77, 3072]);  mul_14 = None\n",
      "        addmm_29: f16[77, 768] = torch.ops.aten.addmm.default(arg81_1, view_104, t_29);  arg81_1 = view_104 = t_29 = None\n",
      "        view_105: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_29, [1, 77, 768]);  addmm_29 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_15: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_14, view_105);  add_14 = view_105 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm_10 = torch.ops.aten.native_layer_norm.default(add_15, [768], arg82_1, arg83_1, 1e-05);  arg82_1 = arg83_1 = None\n",
      "        getitem_30: f16[1, 77, 768] = native_layer_norm_10[0]\n",
      "        getitem_31: f32[1, 77, 1] = native_layer_norm_10[1]\n",
      "        getitem_32: f32[1, 77, 1] = native_layer_norm_10[2];  native_layer_norm_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t_30: f16[768, 768] = torch.ops.aten.t.default(arg84_1);  arg84_1 = None\n",
      "        view_106: f16[77, 768] = torch.ops.aten.view.default(getitem_30, [77, 768])\n",
      "        addmm_30: f16[77, 768] = torch.ops.aten.addmm.default(arg85_1, view_106, t_30);  arg85_1 = view_106 = t_30 = None\n",
      "        view_107: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_30, [1, 77, 768]);  addmm_30 = None\n",
      "        mul_15: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_107, 0.125);  view_107 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_31: f16[768, 768] = torch.ops.aten.t.default(arg86_1);  arg86_1 = None\n",
      "        view_108: f16[77, 768] = torch.ops.aten.view.default(getitem_30, [77, 768])\n",
      "        addmm_31: f16[77, 768] = torch.ops.aten.addmm.default(arg87_1, view_108, t_31);  arg87_1 = view_108 = t_31 = None\n",
      "        view_109: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_31, [1, 77, 768]);  addmm_31 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_110: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_109, [1, -1, 12, 64]);  view_109 = None\n",
      "        transpose_25: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_110, 1, 2);  view_110 = None\n",
      "        clone_20: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_25, memory_format = torch.contiguous_format);  transpose_25 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_32: f16[768, 768] = torch.ops.aten.t.default(arg88_1);  arg88_1 = None\n",
      "        view_111: f16[77, 768] = torch.ops.aten.view.default(getitem_30, [77, 768]);  getitem_30 = None\n",
      "        addmm_32: f16[77, 768] = torch.ops.aten.addmm.default(arg89_1, view_111, t_32);  arg89_1 = view_111 = t_32 = None\n",
      "        view_112: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_32, [1, 77, 768]);  addmm_32 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_113: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_112, [1, -1, 12, 64]);  view_112 = None\n",
      "        transpose_26: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_113, 1, 2);  view_113 = None\n",
      "        clone_21: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_26, memory_format = torch.contiguous_format);  transpose_26 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_114: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul_15, [1, 77, 12, 64]);  mul_15 = None\n",
      "        transpose_27: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_114, 1, 2);  view_114 = None\n",
      "        clone_22: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_27, memory_format = torch.contiguous_format);  transpose_27 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_115: f16[12, 77, 64] = torch.ops.aten.view.default(clone_22, [12, -1, 64]);  clone_22 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_116: f16[12, 77, 64] = torch.ops.aten.view.default(clone_20, [12, -1, 64]);  clone_20 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_117: f16[12, 77, 64] = torch.ops.aten.view.default(clone_21, [12, -1, 64]);  clone_21 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_28: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_116, 1, 2);  view_116 = None\n",
      "        bmm_10: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_115, transpose_28);  view_115 = transpose_28 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_118: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm_10, [1, 12, 77, 77]);  bmm_10 = None\n",
      "        add_16: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_118, _to_copy_1);  view_118 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_119: f16[12, 77, 77] = torch.ops.aten.view.default(add_16, [12, 77, 77]);  add_16 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax_5: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_119, -1, False);  view_119 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_11: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax_5, view_117);  _softmax_5 = view_117 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_120: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_11, [1, 12, 77, 64]);  bmm_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_29: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_120, 1, 2);  view_120 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_23: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_29, memory_format = torch.contiguous_format);  transpose_29 = None\n",
      "        _unsafe_view_5: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_23, [1, 77, 768]);  clone_23 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_33: f16[768, 768] = torch.ops.aten.t.default(arg90_1);  arg90_1 = None\n",
      "        view_121: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view_5, [77, 768]);  _unsafe_view_5 = None\n",
      "        addmm_33: f16[77, 768] = torch.ops.aten.addmm.default(arg91_1, view_121, t_33);  arg91_1 = view_121 = t_33 = None\n",
      "        view_122: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_33, [1, 77, 768]);  addmm_33 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_17: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_15, view_122);  add_15 = view_122 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_11 = torch.ops.aten.native_layer_norm.default(add_17, [768], arg92_1, arg93_1, 1e-05);  arg92_1 = arg93_1 = None\n",
      "        getitem_33: f16[1, 77, 768] = native_layer_norm_11[0]\n",
      "        getitem_34: f32[1, 77, 1] = native_layer_norm_11[1]\n",
      "        getitem_35: f32[1, 77, 1] = native_layer_norm_11[2];  native_layer_norm_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_34: f16[768, 3072] = torch.ops.aten.t.default(arg94_1);  arg94_1 = None\n",
      "        view_123: f16[77, 768] = torch.ops.aten.view.default(getitem_33, [77, 768]);  getitem_33 = None\n",
      "        addmm_34: f16[77, 3072] = torch.ops.aten.addmm.default(arg95_1, view_123, t_34);  arg95_1 = view_123 = t_34 = None\n",
      "        view_124: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_34, [1, 77, 3072]);  addmm_34 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_16: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_124, 1.702)\n",
      "        sigmoid_5: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_16);  mul_16 = None\n",
      "        mul_17: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_124, sigmoid_5);  view_124 = sigmoid_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_35: f16[3072, 768] = torch.ops.aten.t.default(arg96_1);  arg96_1 = None\n",
      "        view_125: f16[77, 3072] = torch.ops.aten.view.default(mul_17, [77, 3072]);  mul_17 = None\n",
      "        addmm_35: f16[77, 768] = torch.ops.aten.addmm.default(arg97_1, view_125, t_35);  arg97_1 = view_125 = t_35 = None\n",
      "        view_126: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_35, [1, 77, 768]);  addmm_35 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_18: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_17, view_126);  add_17 = view_126 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm_12 = torch.ops.aten.native_layer_norm.default(add_18, [768], arg98_1, arg99_1, 1e-05);  arg98_1 = arg99_1 = None\n",
      "        getitem_36: f16[1, 77, 768] = native_layer_norm_12[0]\n",
      "        getitem_37: f32[1, 77, 1] = native_layer_norm_12[1]\n",
      "        getitem_38: f32[1, 77, 1] = native_layer_norm_12[2];  native_layer_norm_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t_36: f16[768, 768] = torch.ops.aten.t.default(arg100_1);  arg100_1 = None\n",
      "        view_127: f16[77, 768] = torch.ops.aten.view.default(getitem_36, [77, 768])\n",
      "        addmm_36: f16[77, 768] = torch.ops.aten.addmm.default(arg101_1, view_127, t_36);  arg101_1 = view_127 = t_36 = None\n",
      "        view_128: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_36, [1, 77, 768]);  addmm_36 = None\n",
      "        mul_18: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_128, 0.125);  view_128 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_37: f16[768, 768] = torch.ops.aten.t.default(arg102_1);  arg102_1 = None\n",
      "        view_129: f16[77, 768] = torch.ops.aten.view.default(getitem_36, [77, 768])\n",
      "        addmm_37: f16[77, 768] = torch.ops.aten.addmm.default(arg103_1, view_129, t_37);  arg103_1 = view_129 = t_37 = None\n",
      "        view_130: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_37, [1, 77, 768]);  addmm_37 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_131: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_130, [1, -1, 12, 64]);  view_130 = None\n",
      "        transpose_30: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_131, 1, 2);  view_131 = None\n",
      "        clone_24: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_30, memory_format = torch.contiguous_format);  transpose_30 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_38: f16[768, 768] = torch.ops.aten.t.default(arg104_1);  arg104_1 = None\n",
      "        view_132: f16[77, 768] = torch.ops.aten.view.default(getitem_36, [77, 768]);  getitem_36 = None\n",
      "        addmm_38: f16[77, 768] = torch.ops.aten.addmm.default(arg105_1, view_132, t_38);  arg105_1 = view_132 = t_38 = None\n",
      "        view_133: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_38, [1, 77, 768]);  addmm_38 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_134: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_133, [1, -1, 12, 64]);  view_133 = None\n",
      "        transpose_31: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_134, 1, 2);  view_134 = None\n",
      "        clone_25: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_31, memory_format = torch.contiguous_format);  transpose_31 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_135: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul_18, [1, 77, 12, 64]);  mul_18 = None\n",
      "        transpose_32: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_135, 1, 2);  view_135 = None\n",
      "        clone_26: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_32, memory_format = torch.contiguous_format);  transpose_32 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_136: f16[12, 77, 64] = torch.ops.aten.view.default(clone_26, [12, -1, 64]);  clone_26 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_137: f16[12, 77, 64] = torch.ops.aten.view.default(clone_24, [12, -1, 64]);  clone_24 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_138: f16[12, 77, 64] = torch.ops.aten.view.default(clone_25, [12, -1, 64]);  clone_25 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_33: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_137, 1, 2);  view_137 = None\n",
      "        bmm_12: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_136, transpose_33);  view_136 = transpose_33 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_139: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm_12, [1, 12, 77, 77]);  bmm_12 = None\n",
      "        add_19: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_139, _to_copy_1);  view_139 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_140: f16[12, 77, 77] = torch.ops.aten.view.default(add_19, [12, 77, 77]);  add_19 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax_6: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_140, -1, False);  view_140 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_13: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax_6, view_138);  _softmax_6 = view_138 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_141: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_13, [1, 12, 77, 64]);  bmm_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_34: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_141, 1, 2);  view_141 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_27: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_34, memory_format = torch.contiguous_format);  transpose_34 = None\n",
      "        _unsafe_view_6: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_27, [1, 77, 768]);  clone_27 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_39: f16[768, 768] = torch.ops.aten.t.default(arg106_1);  arg106_1 = None\n",
      "        view_142: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view_6, [77, 768]);  _unsafe_view_6 = None\n",
      "        addmm_39: f16[77, 768] = torch.ops.aten.addmm.default(arg107_1, view_142, t_39);  arg107_1 = view_142 = t_39 = None\n",
      "        view_143: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_39, [1, 77, 768]);  addmm_39 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_20: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_18, view_143);  add_18 = view_143 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_13 = torch.ops.aten.native_layer_norm.default(add_20, [768], arg108_1, arg109_1, 1e-05);  arg108_1 = arg109_1 = None\n",
      "        getitem_39: f16[1, 77, 768] = native_layer_norm_13[0]\n",
      "        getitem_40: f32[1, 77, 1] = native_layer_norm_13[1]\n",
      "        getitem_41: f32[1, 77, 1] = native_layer_norm_13[2];  native_layer_norm_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_40: f16[768, 3072] = torch.ops.aten.t.default(arg110_1);  arg110_1 = None\n",
      "        view_144: f16[77, 768] = torch.ops.aten.view.default(getitem_39, [77, 768]);  getitem_39 = None\n",
      "        addmm_40: f16[77, 3072] = torch.ops.aten.addmm.default(arg111_1, view_144, t_40);  arg111_1 = view_144 = t_40 = None\n",
      "        view_145: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_40, [1, 77, 3072]);  addmm_40 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_19: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_145, 1.702)\n",
      "        sigmoid_6: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_19);  mul_19 = None\n",
      "        mul_20: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_145, sigmoid_6);  view_145 = sigmoid_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_41: f16[3072, 768] = torch.ops.aten.t.default(arg112_1);  arg112_1 = None\n",
      "        view_146: f16[77, 3072] = torch.ops.aten.view.default(mul_20, [77, 3072]);  mul_20 = None\n",
      "        addmm_41: f16[77, 768] = torch.ops.aten.addmm.default(arg113_1, view_146, t_41);  arg113_1 = view_146 = t_41 = None\n",
      "        view_147: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_41, [1, 77, 768]);  addmm_41 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_21: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_20, view_147);  add_20 = view_147 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm_14 = torch.ops.aten.native_layer_norm.default(add_21, [768], arg114_1, arg115_1, 1e-05);  arg114_1 = arg115_1 = None\n",
      "        getitem_42: f16[1, 77, 768] = native_layer_norm_14[0]\n",
      "        getitem_43: f32[1, 77, 1] = native_layer_norm_14[1]\n",
      "        getitem_44: f32[1, 77, 1] = native_layer_norm_14[2];  native_layer_norm_14 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t_42: f16[768, 768] = torch.ops.aten.t.default(arg116_1);  arg116_1 = None\n",
      "        view_148: f16[77, 768] = torch.ops.aten.view.default(getitem_42, [77, 768])\n",
      "        addmm_42: f16[77, 768] = torch.ops.aten.addmm.default(arg117_1, view_148, t_42);  arg117_1 = view_148 = t_42 = None\n",
      "        view_149: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_42, [1, 77, 768]);  addmm_42 = None\n",
      "        mul_21: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_149, 0.125);  view_149 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_43: f16[768, 768] = torch.ops.aten.t.default(arg118_1);  arg118_1 = None\n",
      "        view_150: f16[77, 768] = torch.ops.aten.view.default(getitem_42, [77, 768])\n",
      "        addmm_43: f16[77, 768] = torch.ops.aten.addmm.default(arg119_1, view_150, t_43);  arg119_1 = view_150 = t_43 = None\n",
      "        view_151: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_43, [1, 77, 768]);  addmm_43 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_152: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_151, [1, -1, 12, 64]);  view_151 = None\n",
      "        transpose_35: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_152, 1, 2);  view_152 = None\n",
      "        clone_28: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_35, memory_format = torch.contiguous_format);  transpose_35 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_44: f16[768, 768] = torch.ops.aten.t.default(arg120_1);  arg120_1 = None\n",
      "        view_153: f16[77, 768] = torch.ops.aten.view.default(getitem_42, [77, 768]);  getitem_42 = None\n",
      "        addmm_44: f16[77, 768] = torch.ops.aten.addmm.default(arg121_1, view_153, t_44);  arg121_1 = view_153 = t_44 = None\n",
      "        view_154: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_44, [1, 77, 768]);  addmm_44 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_155: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_154, [1, -1, 12, 64]);  view_154 = None\n",
      "        transpose_36: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_155, 1, 2);  view_155 = None\n",
      "        clone_29: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_36, memory_format = torch.contiguous_format);  transpose_36 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_156: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul_21, [1, 77, 12, 64]);  mul_21 = None\n",
      "        transpose_37: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_156, 1, 2);  view_156 = None\n",
      "        clone_30: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_37, memory_format = torch.contiguous_format);  transpose_37 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_157: f16[12, 77, 64] = torch.ops.aten.view.default(clone_30, [12, -1, 64]);  clone_30 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_158: f16[12, 77, 64] = torch.ops.aten.view.default(clone_28, [12, -1, 64]);  clone_28 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_159: f16[12, 77, 64] = torch.ops.aten.view.default(clone_29, [12, -1, 64]);  clone_29 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_38: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_158, 1, 2);  view_158 = None\n",
      "        bmm_14: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_157, transpose_38);  view_157 = transpose_38 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_160: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm_14, [1, 12, 77, 77]);  bmm_14 = None\n",
      "        add_22: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_160, _to_copy_1);  view_160 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_161: f16[12, 77, 77] = torch.ops.aten.view.default(add_22, [12, 77, 77]);  add_22 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax_7: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_161, -1, False);  view_161 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_15: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax_7, view_159);  _softmax_7 = view_159 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_162: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_15, [1, 12, 77, 64]);  bmm_15 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_39: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_162, 1, 2);  view_162 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_31: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_39, memory_format = torch.contiguous_format);  transpose_39 = None\n",
      "        _unsafe_view_7: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_31, [1, 77, 768]);  clone_31 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_45: f16[768, 768] = torch.ops.aten.t.default(arg122_1);  arg122_1 = None\n",
      "        view_163: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view_7, [77, 768]);  _unsafe_view_7 = None\n",
      "        addmm_45: f16[77, 768] = torch.ops.aten.addmm.default(arg123_1, view_163, t_45);  arg123_1 = view_163 = t_45 = None\n",
      "        view_164: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_45, [1, 77, 768]);  addmm_45 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_23: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_21, view_164);  add_21 = view_164 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_15 = torch.ops.aten.native_layer_norm.default(add_23, [768], arg124_1, arg125_1, 1e-05);  arg124_1 = arg125_1 = None\n",
      "        getitem_45: f16[1, 77, 768] = native_layer_norm_15[0]\n",
      "        getitem_46: f32[1, 77, 1] = native_layer_norm_15[1]\n",
      "        getitem_47: f32[1, 77, 1] = native_layer_norm_15[2];  native_layer_norm_15 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_46: f16[768, 3072] = torch.ops.aten.t.default(arg126_1);  arg126_1 = None\n",
      "        view_165: f16[77, 768] = torch.ops.aten.view.default(getitem_45, [77, 768]);  getitem_45 = None\n",
      "        addmm_46: f16[77, 3072] = torch.ops.aten.addmm.default(arg127_1, view_165, t_46);  arg127_1 = view_165 = t_46 = None\n",
      "        view_166: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_46, [1, 77, 3072]);  addmm_46 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_22: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_166, 1.702)\n",
      "        sigmoid_7: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_22);  mul_22 = None\n",
      "        mul_23: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_166, sigmoid_7);  view_166 = sigmoid_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_47: f16[3072, 768] = torch.ops.aten.t.default(arg128_1);  arg128_1 = None\n",
      "        view_167: f16[77, 3072] = torch.ops.aten.view.default(mul_23, [77, 3072]);  mul_23 = None\n",
      "        addmm_47: f16[77, 768] = torch.ops.aten.addmm.default(arg129_1, view_167, t_47);  arg129_1 = view_167 = t_47 = None\n",
      "        view_168: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_47, [1, 77, 768]);  addmm_47 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_24: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_23, view_168);  add_23 = view_168 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm_16 = torch.ops.aten.native_layer_norm.default(add_24, [768], arg130_1, arg131_1, 1e-05);  arg130_1 = arg131_1 = None\n",
      "        getitem_48: f16[1, 77, 768] = native_layer_norm_16[0]\n",
      "        getitem_49: f32[1, 77, 1] = native_layer_norm_16[1]\n",
      "        getitem_50: f32[1, 77, 1] = native_layer_norm_16[2];  native_layer_norm_16 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t_48: f16[768, 768] = torch.ops.aten.t.default(arg132_1);  arg132_1 = None\n",
      "        view_169: f16[77, 768] = torch.ops.aten.view.default(getitem_48, [77, 768])\n",
      "        addmm_48: f16[77, 768] = torch.ops.aten.addmm.default(arg133_1, view_169, t_48);  arg133_1 = view_169 = t_48 = None\n",
      "        view_170: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_48, [1, 77, 768]);  addmm_48 = None\n",
      "        mul_24: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_170, 0.125);  view_170 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_49: f16[768, 768] = torch.ops.aten.t.default(arg134_1);  arg134_1 = None\n",
      "        view_171: f16[77, 768] = torch.ops.aten.view.default(getitem_48, [77, 768])\n",
      "        addmm_49: f16[77, 768] = torch.ops.aten.addmm.default(arg135_1, view_171, t_49);  arg135_1 = view_171 = t_49 = None\n",
      "        view_172: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_49, [1, 77, 768]);  addmm_49 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_173: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_172, [1, -1, 12, 64]);  view_172 = None\n",
      "        transpose_40: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_173, 1, 2);  view_173 = None\n",
      "        clone_32: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_40, memory_format = torch.contiguous_format);  transpose_40 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_50: f16[768, 768] = torch.ops.aten.t.default(arg136_1);  arg136_1 = None\n",
      "        view_174: f16[77, 768] = torch.ops.aten.view.default(getitem_48, [77, 768]);  getitem_48 = None\n",
      "        addmm_50: f16[77, 768] = torch.ops.aten.addmm.default(arg137_1, view_174, t_50);  arg137_1 = view_174 = t_50 = None\n",
      "        view_175: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_50, [1, 77, 768]);  addmm_50 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_176: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_175, [1, -1, 12, 64]);  view_175 = None\n",
      "        transpose_41: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_176, 1, 2);  view_176 = None\n",
      "        clone_33: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_41, memory_format = torch.contiguous_format);  transpose_41 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_177: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul_24, [1, 77, 12, 64]);  mul_24 = None\n",
      "        transpose_42: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_177, 1, 2);  view_177 = None\n",
      "        clone_34: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_42, memory_format = torch.contiguous_format);  transpose_42 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_178: f16[12, 77, 64] = torch.ops.aten.view.default(clone_34, [12, -1, 64]);  clone_34 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_179: f16[12, 77, 64] = torch.ops.aten.view.default(clone_32, [12, -1, 64]);  clone_32 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_180: f16[12, 77, 64] = torch.ops.aten.view.default(clone_33, [12, -1, 64]);  clone_33 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_43: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_179, 1, 2);  view_179 = None\n",
      "        bmm_16: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_178, transpose_43);  view_178 = transpose_43 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_181: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm_16, [1, 12, 77, 77]);  bmm_16 = None\n",
      "        add_25: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_181, _to_copy_1);  view_181 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_182: f16[12, 77, 77] = torch.ops.aten.view.default(add_25, [12, 77, 77]);  add_25 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax_8: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_182, -1, False);  view_182 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_17: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax_8, view_180);  _softmax_8 = view_180 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_183: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_17, [1, 12, 77, 64]);  bmm_17 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_44: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_183, 1, 2);  view_183 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_35: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_44, memory_format = torch.contiguous_format);  transpose_44 = None\n",
      "        _unsafe_view_8: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_35, [1, 77, 768]);  clone_35 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_51: f16[768, 768] = torch.ops.aten.t.default(arg138_1);  arg138_1 = None\n",
      "        view_184: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view_8, [77, 768]);  _unsafe_view_8 = None\n",
      "        addmm_51: f16[77, 768] = torch.ops.aten.addmm.default(arg139_1, view_184, t_51);  arg139_1 = view_184 = t_51 = None\n",
      "        view_185: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_51, [1, 77, 768]);  addmm_51 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_26: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_24, view_185);  add_24 = view_185 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_17 = torch.ops.aten.native_layer_norm.default(add_26, [768], arg140_1, arg141_1, 1e-05);  arg140_1 = arg141_1 = None\n",
      "        getitem_51: f16[1, 77, 768] = native_layer_norm_17[0]\n",
      "        getitem_52: f32[1, 77, 1] = native_layer_norm_17[1]\n",
      "        getitem_53: f32[1, 77, 1] = native_layer_norm_17[2];  native_layer_norm_17 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_52: f16[768, 3072] = torch.ops.aten.t.default(arg142_1);  arg142_1 = None\n",
      "        view_186: f16[77, 768] = torch.ops.aten.view.default(getitem_51, [77, 768]);  getitem_51 = None\n",
      "        addmm_52: f16[77, 3072] = torch.ops.aten.addmm.default(arg143_1, view_186, t_52);  arg143_1 = view_186 = t_52 = None\n",
      "        view_187: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_52, [1, 77, 3072]);  addmm_52 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_25: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_187, 1.702)\n",
      "        sigmoid_8: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_25);  mul_25 = None\n",
      "        mul_26: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_187, sigmoid_8);  view_187 = sigmoid_8 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_53: f16[3072, 768] = torch.ops.aten.t.default(arg144_1);  arg144_1 = None\n",
      "        view_188: f16[77, 3072] = torch.ops.aten.view.default(mul_26, [77, 3072]);  mul_26 = None\n",
      "        addmm_53: f16[77, 768] = torch.ops.aten.addmm.default(arg145_1, view_188, t_53);  arg145_1 = view_188 = t_53 = None\n",
      "        view_189: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_53, [1, 77, 768]);  addmm_53 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_27: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_26, view_189);  add_26 = view_189 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm_18 = torch.ops.aten.native_layer_norm.default(add_27, [768], arg146_1, arg147_1, 1e-05);  arg146_1 = arg147_1 = None\n",
      "        getitem_54: f16[1, 77, 768] = native_layer_norm_18[0]\n",
      "        getitem_55: f32[1, 77, 1] = native_layer_norm_18[1]\n",
      "        getitem_56: f32[1, 77, 1] = native_layer_norm_18[2];  native_layer_norm_18 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t_54: f16[768, 768] = torch.ops.aten.t.default(arg148_1);  arg148_1 = None\n",
      "        view_190: f16[77, 768] = torch.ops.aten.view.default(getitem_54, [77, 768])\n",
      "        addmm_54: f16[77, 768] = torch.ops.aten.addmm.default(arg149_1, view_190, t_54);  arg149_1 = view_190 = t_54 = None\n",
      "        view_191: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_54, [1, 77, 768]);  addmm_54 = None\n",
      "        mul_27: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_191, 0.125);  view_191 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_55: f16[768, 768] = torch.ops.aten.t.default(arg150_1);  arg150_1 = None\n",
      "        view_192: f16[77, 768] = torch.ops.aten.view.default(getitem_54, [77, 768])\n",
      "        addmm_55: f16[77, 768] = torch.ops.aten.addmm.default(arg151_1, view_192, t_55);  arg151_1 = view_192 = t_55 = None\n",
      "        view_193: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_55, [1, 77, 768]);  addmm_55 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_194: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_193, [1, -1, 12, 64]);  view_193 = None\n",
      "        transpose_45: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_194, 1, 2);  view_194 = None\n",
      "        clone_36: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_45, memory_format = torch.contiguous_format);  transpose_45 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_56: f16[768, 768] = torch.ops.aten.t.default(arg152_1);  arg152_1 = None\n",
      "        view_195: f16[77, 768] = torch.ops.aten.view.default(getitem_54, [77, 768]);  getitem_54 = None\n",
      "        addmm_56: f16[77, 768] = torch.ops.aten.addmm.default(arg153_1, view_195, t_56);  arg153_1 = view_195 = t_56 = None\n",
      "        view_196: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_56, [1, 77, 768]);  addmm_56 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_197: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_196, [1, -1, 12, 64]);  view_196 = None\n",
      "        transpose_46: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_197, 1, 2);  view_197 = None\n",
      "        clone_37: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_46, memory_format = torch.contiguous_format);  transpose_46 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_198: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul_27, [1, 77, 12, 64]);  mul_27 = None\n",
      "        transpose_47: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_198, 1, 2);  view_198 = None\n",
      "        clone_38: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_47, memory_format = torch.contiguous_format);  transpose_47 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_199: f16[12, 77, 64] = torch.ops.aten.view.default(clone_38, [12, -1, 64]);  clone_38 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_200: f16[12, 77, 64] = torch.ops.aten.view.default(clone_36, [12, -1, 64]);  clone_36 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_201: f16[12, 77, 64] = torch.ops.aten.view.default(clone_37, [12, -1, 64]);  clone_37 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_48: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_200, 1, 2);  view_200 = None\n",
      "        bmm_18: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_199, transpose_48);  view_199 = transpose_48 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_202: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm_18, [1, 12, 77, 77]);  bmm_18 = None\n",
      "        add_28: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_202, _to_copy_1);  view_202 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_203: f16[12, 77, 77] = torch.ops.aten.view.default(add_28, [12, 77, 77]);  add_28 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax_9: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_203, -1, False);  view_203 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_19: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax_9, view_201);  _softmax_9 = view_201 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_204: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_19, [1, 12, 77, 64]);  bmm_19 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_49: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_204, 1, 2);  view_204 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_39: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_49, memory_format = torch.contiguous_format);  transpose_49 = None\n",
      "        _unsafe_view_9: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_39, [1, 77, 768]);  clone_39 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_57: f16[768, 768] = torch.ops.aten.t.default(arg154_1);  arg154_1 = None\n",
      "        view_205: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view_9, [77, 768]);  _unsafe_view_9 = None\n",
      "        addmm_57: f16[77, 768] = torch.ops.aten.addmm.default(arg155_1, view_205, t_57);  arg155_1 = view_205 = t_57 = None\n",
      "        view_206: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_57, [1, 77, 768]);  addmm_57 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_29: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_27, view_206);  add_27 = view_206 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_19 = torch.ops.aten.native_layer_norm.default(add_29, [768], arg156_1, arg157_1, 1e-05);  arg156_1 = arg157_1 = None\n",
      "        getitem_57: f16[1, 77, 768] = native_layer_norm_19[0]\n",
      "        getitem_58: f32[1, 77, 1] = native_layer_norm_19[1]\n",
      "        getitem_59: f32[1, 77, 1] = native_layer_norm_19[2];  native_layer_norm_19 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_58: f16[768, 3072] = torch.ops.aten.t.default(arg158_1);  arg158_1 = None\n",
      "        view_207: f16[77, 768] = torch.ops.aten.view.default(getitem_57, [77, 768]);  getitem_57 = None\n",
      "        addmm_58: f16[77, 3072] = torch.ops.aten.addmm.default(arg159_1, view_207, t_58);  arg159_1 = view_207 = t_58 = None\n",
      "        view_208: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_58, [1, 77, 3072]);  addmm_58 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_28: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_208, 1.702)\n",
      "        sigmoid_9: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_28);  mul_28 = None\n",
      "        mul_29: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_208, sigmoid_9);  view_208 = sigmoid_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_59: f16[3072, 768] = torch.ops.aten.t.default(arg160_1);  arg160_1 = None\n",
      "        view_209: f16[77, 3072] = torch.ops.aten.view.default(mul_29, [77, 3072]);  mul_29 = None\n",
      "        addmm_59: f16[77, 768] = torch.ops.aten.addmm.default(arg161_1, view_209, t_59);  arg161_1 = view_209 = t_59 = None\n",
      "        view_210: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_59, [1, 77, 768]);  addmm_59 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_30: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_29, view_210);  add_29 = view_210 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm_20 = torch.ops.aten.native_layer_norm.default(add_30, [768], arg162_1, arg163_1, 1e-05);  arg162_1 = arg163_1 = None\n",
      "        getitem_60: f16[1, 77, 768] = native_layer_norm_20[0]\n",
      "        getitem_61: f32[1, 77, 1] = native_layer_norm_20[1]\n",
      "        getitem_62: f32[1, 77, 1] = native_layer_norm_20[2];  native_layer_norm_20 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t_60: f16[768, 768] = torch.ops.aten.t.default(arg164_1);  arg164_1 = None\n",
      "        view_211: f16[77, 768] = torch.ops.aten.view.default(getitem_60, [77, 768])\n",
      "        addmm_60: f16[77, 768] = torch.ops.aten.addmm.default(arg165_1, view_211, t_60);  arg165_1 = view_211 = t_60 = None\n",
      "        view_212: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_60, [1, 77, 768]);  addmm_60 = None\n",
      "        mul_30: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_212, 0.125);  view_212 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_61: f16[768, 768] = torch.ops.aten.t.default(arg166_1);  arg166_1 = None\n",
      "        view_213: f16[77, 768] = torch.ops.aten.view.default(getitem_60, [77, 768])\n",
      "        addmm_61: f16[77, 768] = torch.ops.aten.addmm.default(arg167_1, view_213, t_61);  arg167_1 = view_213 = t_61 = None\n",
      "        view_214: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_61, [1, 77, 768]);  addmm_61 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_215: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_214, [1, -1, 12, 64]);  view_214 = None\n",
      "        transpose_50: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_215, 1, 2);  view_215 = None\n",
      "        clone_40: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_50, memory_format = torch.contiguous_format);  transpose_50 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_62: f16[768, 768] = torch.ops.aten.t.default(arg168_1);  arg168_1 = None\n",
      "        view_216: f16[77, 768] = torch.ops.aten.view.default(getitem_60, [77, 768]);  getitem_60 = None\n",
      "        addmm_62: f16[77, 768] = torch.ops.aten.addmm.default(arg169_1, view_216, t_62);  arg169_1 = view_216 = t_62 = None\n",
      "        view_217: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_62, [1, 77, 768]);  addmm_62 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_218: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_217, [1, -1, 12, 64]);  view_217 = None\n",
      "        transpose_51: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_218, 1, 2);  view_218 = None\n",
      "        clone_41: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_51, memory_format = torch.contiguous_format);  transpose_51 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_219: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul_30, [1, 77, 12, 64]);  mul_30 = None\n",
      "        transpose_52: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_219, 1, 2);  view_219 = None\n",
      "        clone_42: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_52, memory_format = torch.contiguous_format);  transpose_52 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_220: f16[12, 77, 64] = torch.ops.aten.view.default(clone_42, [12, -1, 64]);  clone_42 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_221: f16[12, 77, 64] = torch.ops.aten.view.default(clone_40, [12, -1, 64]);  clone_40 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_222: f16[12, 77, 64] = torch.ops.aten.view.default(clone_41, [12, -1, 64]);  clone_41 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_53: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_221, 1, 2);  view_221 = None\n",
      "        bmm_20: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_220, transpose_53);  view_220 = transpose_53 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_223: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm_20, [1, 12, 77, 77]);  bmm_20 = None\n",
      "        add_31: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_223, _to_copy_1);  view_223 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_224: f16[12, 77, 77] = torch.ops.aten.view.default(add_31, [12, 77, 77]);  add_31 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax_10: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_224, -1, False);  view_224 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_21: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax_10, view_222);  _softmax_10 = view_222 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_225: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_21, [1, 12, 77, 64]);  bmm_21 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_54: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_225, 1, 2);  view_225 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_43: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_54, memory_format = torch.contiguous_format);  transpose_54 = None\n",
      "        _unsafe_view_10: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_43, [1, 77, 768]);  clone_43 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_63: f16[768, 768] = torch.ops.aten.t.default(arg170_1);  arg170_1 = None\n",
      "        view_226: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view_10, [77, 768]);  _unsafe_view_10 = None\n",
      "        addmm_63: f16[77, 768] = torch.ops.aten.addmm.default(arg171_1, view_226, t_63);  arg171_1 = view_226 = t_63 = None\n",
      "        view_227: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_63, [1, 77, 768]);  addmm_63 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_32: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_30, view_227);  add_30 = view_227 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_21 = torch.ops.aten.native_layer_norm.default(add_32, [768], arg172_1, arg173_1, 1e-05);  arg172_1 = arg173_1 = None\n",
      "        getitem_63: f16[1, 77, 768] = native_layer_norm_21[0]\n",
      "        getitem_64: f32[1, 77, 1] = native_layer_norm_21[1]\n",
      "        getitem_65: f32[1, 77, 1] = native_layer_norm_21[2];  native_layer_norm_21 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_64: f16[768, 3072] = torch.ops.aten.t.default(arg174_1);  arg174_1 = None\n",
      "        view_228: f16[77, 768] = torch.ops.aten.view.default(getitem_63, [77, 768]);  getitem_63 = None\n",
      "        addmm_64: f16[77, 3072] = torch.ops.aten.addmm.default(arg175_1, view_228, t_64);  arg175_1 = view_228 = t_64 = None\n",
      "        view_229: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_64, [1, 77, 3072]);  addmm_64 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_31: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_229, 1.702)\n",
      "        sigmoid_10: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_31);  mul_31 = None\n",
      "        mul_32: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_229, sigmoid_10);  view_229 = sigmoid_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_65: f16[3072, 768] = torch.ops.aten.t.default(arg176_1);  arg176_1 = None\n",
      "        view_230: f16[77, 3072] = torch.ops.aten.view.default(mul_32, [77, 3072]);  mul_32 = None\n",
      "        addmm_65: f16[77, 768] = torch.ops.aten.addmm.default(arg177_1, view_230, t_65);  arg177_1 = view_230 = t_65 = None\n",
      "        view_231: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_65, [1, 77, 768]);  addmm_65 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_33: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_32, view_231);  add_32 = view_231 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:378, code: hidden_states = self.layer_norm1(hidden_states)\n",
      "        native_layer_norm_22 = torch.ops.aten.native_layer_norm.default(add_33, [768], arg178_1, arg179_1, 1e-05);  arg178_1 = arg179_1 = None\n",
      "        getitem_66: f16[1, 77, 768] = native_layer_norm_22[0]\n",
      "        getitem_67: f32[1, 77, 1] = native_layer_norm_22[1]\n",
      "        getitem_68: f32[1, 77, 1] = native_layer_norm_22[2];  native_layer_norm_22 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:268, code: query_states = self.q_proj(hidden_states) * self.scale\n",
      "        t_66: f16[768, 768] = torch.ops.aten.t.default(arg180_1);  arg180_1 = None\n",
      "        view_232: f16[77, 768] = torch.ops.aten.view.default(getitem_66, [77, 768])\n",
      "        addmm_66: f16[77, 768] = torch.ops.aten.addmm.default(arg181_1, view_232, t_66);  arg181_1 = view_232 = t_66 = None\n",
      "        view_233: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_66, [1, 77, 768]);  addmm_66 = None\n",
      "        mul_33: f16[1, 77, 768] = torch.ops.aten.mul.Tensor(view_233, 0.125);  view_233 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:269, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "        t_67: f16[768, 768] = torch.ops.aten.t.default(arg182_1);  arg182_1 = None\n",
      "        view_234: f16[77, 768] = torch.ops.aten.view.default(getitem_66, [77, 768])\n",
      "        addmm_67: f16[77, 768] = torch.ops.aten.addmm.default(arg183_1, view_234, t_67);  arg183_1 = view_234 = t_67 = None\n",
      "        view_235: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_67, [1, 77, 768]);  addmm_67 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_236: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_235, [1, -1, 12, 64]);  view_235 = None\n",
      "        transpose_55: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_236, 1, 2);  view_236 = None\n",
      "        clone_44: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_55, memory_format = torch.contiguous_format);  transpose_55 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:270, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
      "        t_68: f16[768, 768] = torch.ops.aten.t.default(arg184_1);  arg184_1 = None\n",
      "        view_237: f16[77, 768] = torch.ops.aten.view.default(getitem_66, [77, 768]);  getitem_66 = None\n",
      "        addmm_68: f16[77, 768] = torch.ops.aten.addmm.default(arg185_1, view_237, t_68);  arg185_1 = view_237 = t_68 = None\n",
      "        view_238: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_68, [1, 77, 768]);  addmm_68 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_239: f16[1, 77, 12, 64] = torch.ops.aten.view.default(view_238, [1, -1, 12, 64]);  view_238 = None\n",
      "        transpose_56: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_239, 1, 2);  view_239 = None\n",
      "        clone_45: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_56, memory_format = torch.contiguous_format);  transpose_56 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:254, code: return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "        view_240: f16[1, 77, 12, 64] = torch.ops.aten.view.default(mul_33, [1, 77, 12, 64]);  mul_33 = None\n",
      "        transpose_57: f16[1, 12, 77, 64] = torch.ops.aten.transpose.int(view_240, 1, 2);  view_240 = None\n",
      "        clone_46: f16[1, 12, 77, 64] = torch.ops.aten.clone.default(transpose_57, memory_format = torch.contiguous_format);  transpose_57 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
      "        view_241: f16[12, 77, 64] = torch.ops.aten.view.default(clone_46, [12, -1, 64]);  clone_46 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:274, code: key_states = key_states.view(*proj_shape)\n",
      "        view_242: f16[12, 77, 64] = torch.ops.aten.view.default(clone_44, [12, -1, 64]);  clone_44 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:275, code: value_states = value_states.view(*proj_shape)\n",
      "        view_243: f16[12, 77, 64] = torch.ops.aten.view.default(clone_45, [12, -1, 64]);  clone_45 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:278, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
      "        transpose_58: f16[12, 64, 77] = torch.ops.aten.transpose.int(view_242, 1, 2);  view_242 = None\n",
      "        bmm_22: f16[12, 77, 77] = torch.ops.aten.bmm.default(view_241, transpose_58);  view_241 = transpose_58 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:293, code: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
      "        view_244: f16[1, 12, 77, 77] = torch.ops.aten.view.default(bmm_22, [1, 12, 77, 77]);  bmm_22 = None\n",
      "        add_34: f16[1, 12, 77, 77] = torch.ops.aten.add.Tensor(view_244, _to_copy_1);  view_244 = _to_copy_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:294, code: attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
      "        view_245: f16[12, 77, 77] = torch.ops.aten.view.default(add_34, [12, 77, 77]);  add_34 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:304, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "        _softmax_11: f16[12, 77, 77] = torch.ops.aten._softmax.default(view_245, -1, False);  view_245 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:318, code: attn_output = torch.bmm(attn_probs, value_states)\n",
      "        bmm_23: f16[12, 77, 64] = torch.ops.aten.bmm.default(_softmax_11, view_243);  _softmax_11 = view_243 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:326, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
      "        view_246: f16[1, 12, 77, 64] = torch.ops.aten.view.default(bmm_23, [1, 12, 77, 64]);  bmm_23 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:327, code: attn_output = attn_output.transpose(1, 2)\n",
      "        transpose_59: f16[1, 77, 12, 64] = torch.ops.aten.transpose.int(view_246, 1, 2);  view_246 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:328, code: attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
      "        clone_47: f16[1, 77, 12, 64] = torch.ops.aten.clone.default(transpose_59, memory_format = torch.contiguous_format);  transpose_59 = None\n",
      "        _unsafe_view_11: f16[1, 77, 768] = torch.ops.aten._unsafe_view.default(clone_47, [1, 77, 768]);  clone_47 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330, code: attn_output = self.out_proj(attn_output)\n",
      "        t_69: f16[768, 768] = torch.ops.aten.t.default(arg186_1);  arg186_1 = None\n",
      "        view_247: f16[77, 768] = torch.ops.aten.view.default(_unsafe_view_11, [77, 768]);  _unsafe_view_11 = None\n",
      "        addmm_69: f16[77, 768] = torch.ops.aten.addmm.default(arg187_1, view_247, t_69);  arg187_1 = view_247 = t_69 = None\n",
      "        view_248: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_69, [1, 77, 768]);  addmm_69 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385, code: hidden_states = residual + hidden_states\n",
      "        add_35: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_33, view_248);  add_33 = view_248 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:388, code: hidden_states = self.layer_norm2(hidden_states)\n",
      "        native_layer_norm_23 = torch.ops.aten.native_layer_norm.default(add_35, [768], arg188_1, arg189_1, 1e-05);  arg188_1 = arg189_1 = None\n",
      "        getitem_69: f16[1, 77, 768] = native_layer_norm_23[0]\n",
      "        getitem_70: f32[1, 77, 1] = native_layer_norm_23[1]\n",
      "        getitem_71: f32[1, 77, 1] = native_layer_norm_23[2];  native_layer_norm_23 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:344, code: hidden_states = self.fc1(hidden_states)\n",
      "        t_70: f16[768, 3072] = torch.ops.aten.t.default(arg190_1);  arg190_1 = None\n",
      "        view_249: f16[77, 768] = torch.ops.aten.view.default(getitem_69, [77, 768]);  getitem_69 = None\n",
      "        addmm_70: f16[77, 3072] = torch.ops.aten.addmm.default(arg191_1, view_249, t_70);  arg191_1 = view_249 = t_70 = None\n",
      "        view_250: f16[1, 77, 3072] = torch.ops.aten.view.default(addmm_70, [1, 77, 3072]);  addmm_70 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/activations.py:75, code: return input * torch.sigmoid(1.702 * input)\n",
      "        mul_34: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_250, 1.702)\n",
      "        sigmoid_11: f16[1, 77, 3072] = torch.ops.aten.sigmoid.default(mul_34);  mul_34 = None\n",
      "        mul_35: f16[1, 77, 3072] = torch.ops.aten.mul.Tensor(view_250, sigmoid_11);  view_250 = sigmoid_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:346, code: hidden_states = self.fc2(hidden_states)\n",
      "        t_71: f16[3072, 768] = torch.ops.aten.t.default(arg192_1);  arg192_1 = None\n",
      "        view_251: f16[77, 3072] = torch.ops.aten.view.default(mul_35, [77, 3072]);  mul_35 = None\n",
      "        addmm_71: f16[77, 768] = torch.ops.aten.addmm.default(arg193_1, view_251, t_71);  arg193_1 = view_251 = t_71 = None\n",
      "        view_252: f16[1, 77, 768] = torch.ops.aten.view.default(addmm_71, [1, 77, 768]);  addmm_71 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:390, code: hidden_states = residual + hidden_states\n",
      "        add_36: f16[1, 77, 768] = torch.ops.aten.add.Tensor(add_35, view_252);  add_35 = view_252 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:731, code: last_hidden_state = self.final_layer_norm(last_hidden_state)\n",
      "        native_layer_norm_24 = torch.ops.aten.native_layer_norm.default(add_36, [768], arg194_1, arg195_1, 1e-05);  add_36 = arg194_1 = arg195_1 = None\n",
      "        getitem_72: f16[1, 77, 768] = native_layer_norm_24[0]\n",
      "        getitem_73: f32[1, 77, 1] = native_layer_norm_24[1]\n",
      "        getitem_74: f32[1, 77, 1] = native_layer_norm_24[2];  native_layer_norm_24 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:737, code: torch.arange(last_hidden_state.shape[0], device=input_ids.device), input_ids.to(torch.int).argmax(dim=-1)\n",
      "        arange: i64[1] = torch.ops.aten.arange.default(1, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        _to_copy_2: i32[1, 77] = torch.ops.aten._to_copy.default(view, dtype = torch.int32);  view = None\n",
      "        argmax: i64[1] = torch.ops.aten.argmax.default(_to_copy_2, -1);  _to_copy_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:319, code: uncond_embeddings = uncond_embeddings.repeat(1, num_images_per_prompt, 1)\n",
      "        repeat: f16[1, 77, 768] = torch.ops.aten.repeat.default(getitem_72, [1, 1, 1]);  getitem_72 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:320, code: uncond_embeddings = uncond_embeddings.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
      "        view_253: f16[1, 77, 768] = torch.ops.aten.view.default(repeat, [1, 77, -1]);  repeat = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:325, code: text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
      "        cat: f16[2, 77, 768] = torch.ops.aten.cat.default([view_253, arg197_1]);  view_253 = arg197_1 = None\n",
      "        return (cat,)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:398, code: latents = torch.randn(shape, generator=generator, device=rand_device, dtype=dtype).to(device)\n",
      "        randn: f16[1, 4, 64, 64] = torch.ops.aten.randn.generator([1, 4, 64, 64], generator = None, dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:405, code: latents = latents * self.scheduler.init_noise_sigma\n",
      "        mul: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(randn, 1.0);  randn = None\n",
      "        return (mul,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: i64[2]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/embeddings.py:40, code: exponent = -math.log(max_period) * torch.arange(\n",
      "        arange: f32[160] = torch.ops.aten.arange.start(0, 160, dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        mul: f32[160] = torch.ops.aten.mul.Tensor(arange, -9.210340371976184);  arange = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/embeddings.py:43, code: exponent = exponent / (half_dim - downscale_freq_shift)\n",
      "        div: f32[160] = torch.ops.aten.div.Tensor(mul, 160);  mul = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/embeddings.py:45, code: emb = torch.exp(exponent)\n",
      "        exp: f32[160] = torch.ops.aten.exp.default(div);  div = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/embeddings.py:46, code: emb = timesteps[:, None].float() * emb[None, :]\n",
      "        slice_1: i64[2] = torch.ops.aten.slice.Tensor(arg0_1, 0, 0, 9223372036854775807);  arg0_1 = None\n",
      "        unsqueeze: i64[2, 1] = torch.ops.aten.unsqueeze.default(slice_1, 1);  slice_1 = None\n",
      "        _to_copy: f32[2, 1] = torch.ops.aten._to_copy.default(unsqueeze, dtype = torch.float32);  unsqueeze = None\n",
      "        unsqueeze_1: f32[1, 160] = torch.ops.aten.unsqueeze.default(exp, 0);  exp = None\n",
      "        slice_2: f32[1, 160] = torch.ops.aten.slice.Tensor(unsqueeze_1, 1, 0, 9223372036854775807);  unsqueeze_1 = None\n",
      "        mul_1: f32[2, 160] = torch.ops.aten.mul.Tensor(_to_copy, slice_2);  _to_copy = slice_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/embeddings.py:49, code: emb = scale * emb\n",
      "        mul_2: f32[2, 160] = torch.ops.aten.mul.Tensor(mul_1, 1);  mul_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/embeddings.py:52, code: emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
      "        sin: f32[2, 160] = torch.ops.aten.sin.default(mul_2)\n",
      "        cos: f32[2, 160] = torch.ops.aten.cos.default(mul_2);  mul_2 = None\n",
      "        cat: f32[2, 320] = torch.ops.aten.cat.default([sin, cos], -1);  sin = cos = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/embeddings.py:56, code: emb = torch.cat([emb[:, half_dim:], emb[:, :half_dim]], dim=-1)\n",
      "        slice_3: f32[2, 320] = torch.ops.aten.slice.Tensor(cat, 0, 0, 9223372036854775807)\n",
      "        slice_4: f32[2, 160] = torch.ops.aten.slice.Tensor(slice_3, 1, 160, 9223372036854775807);  slice_3 = None\n",
      "        slice_5: f32[2, 320] = torch.ops.aten.slice.Tensor(cat, 0, 0, 9223372036854775807);  cat = None\n",
      "        slice_6: f32[2, 160] = torch.ops.aten.slice.Tensor(slice_5, 1, 0, 160);  slice_5 = None\n",
      "        cat_1: f32[2, 320] = torch.ops.aten.cat.default([slice_4, slice_6], -1);  slice_4 = slice_6 = None\n",
      "        return (cat_1,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1280, 320], arg1_1: f16[1280], arg2_1: f16[1280, 1280], arg3_1: f16[1280], arg4_1: f16[2, 320]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/embeddings.py:82, code: sample = self.linear_1(sample)\n",
      "        t: f16[320, 1280] = torch.ops.aten.t.default(arg0_1);  arg0_1 = None\n",
      "        addmm: f16[2, 1280] = torch.ops.aten.addmm.default(arg1_1, arg4_1, t);  arg1_1 = arg4_1 = t = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/embeddings.py:85, code: sample = self.act(sample)\n",
      "        silu: f16[2, 1280] = torch.ops.aten.silu.default(addmm);  addmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/embeddings.py:87, code: sample = self.linear_2(sample)\n",
      "        t_1: f16[1280, 1280] = torch.ops.aten.t.default(arg2_1);  arg2_1 = None\n",
      "        addmm_1: f16[2, 1280] = torch.ops.aten.addmm.default(arg3_1, silu, t_1);  arg3_1 = silu = t_1 = None\n",
      "        return (addmm_1,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[320], arg1_1: f16[320], arg2_1: f16[320, 320, 3, 3], arg3_1: f16[320], arg4_1: f16[320, 1280], arg5_1: f16[320], arg6_1: f16[320], arg7_1: f16[320], arg8_1: f16[320, 320, 3, 3], arg9_1: f16[320], arg10_1: f16[2, 320, 64, 64], arg11_1: f16[2, 1280]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg10_1, arg0_1, arg1_1, 2, 320, 4096, 32, 1e-05);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 320, 64, 64] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu: f16[2, 320, 64, 64] = torch.ops.aten.silu.default(getitem);  getitem = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution: f16[2, 320, 64, 64] = torch.ops.aten.convolution.default(silu, arg2_1, arg3_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:474, code: temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
      "        silu_1: f16[2, 1280] = torch.ops.aten.silu.default(arg11_1);  arg11_1 = None\n",
      "        t: f16[1280, 320] = torch.ops.aten.t.default(arg4_1);  arg4_1 = None\n",
      "        addmm: f16[2, 320] = torch.ops.aten.addmm.default(arg5_1, silu_1, t);  arg5_1 = silu_1 = t = None\n",
      "        slice_1: f16[2, 320] = torch.ops.aten.slice.Tensor(addmm, 0, 0, 9223372036854775807);  addmm = None\n",
      "        slice_2: f16[2, 320] = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807);  slice_1 = None\n",
      "        unsqueeze: f16[2, 320, 1] = torch.ops.aten.unsqueeze.default(slice_2, 2);  slice_2 = None\n",
      "        unsqueeze_1: f16[2, 320, 1, 1] = torch.ops.aten.unsqueeze.default(unsqueeze, 3);  unsqueeze = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:477, code: hidden_states = hidden_states + temb\n",
      "        add: f16[2, 320, 64, 64] = torch.ops.aten.add.Tensor(convolution, unsqueeze_1);  convolution = unsqueeze_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_1 = torch.ops.aten.native_group_norm.default(add, arg6_1, arg7_1, 2, 320, 4096, 32, 1e-05);  add = arg6_1 = arg7_1 = None\n",
      "        getitem_3: f16[2, 320, 64, 64] = native_group_norm_1[0]\n",
      "        getitem_4: f16[2, 32] = native_group_norm_1[1]\n",
      "        getitem_5: f16[2, 32] = native_group_norm_1[2];  native_group_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_2: f16[2, 320, 64, 64] = torch.ops.aten.silu.default(getitem_3);  getitem_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_1: f16[2, 320, 64, 64] = torch.ops.aten.convolution.default(silu_2, arg8_1, arg9_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_2 = arg8_1 = arg9_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_1: f16[2, 320, 64, 64] = torch.ops.aten.add.Tensor(arg10_1, convolution_1);  arg10_1 = convolution_1 = None\n",
      "        div: f16[2, 320, 64, 64] = torch.ops.aten.div.Tensor(add_1, 1.0);  add_1 = None\n",
      "        return (div,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[320], arg1_1: f16[320], arg2_1: f16[320, 320, 1, 1], arg3_1: f16[320], arg4_1: f16[320], arg5_1: f16[320], arg6_1: f16[320, 320], arg7_1: f16[320, 320], arg8_1: f16[320, 320], arg9_1: f16[320, 320], arg10_1: f16[320], arg11_1: f16[320], arg12_1: f16[320], arg13_1: f16[320, 320], arg14_1: f16[320, 768], arg15_1: f16[320, 768], arg16_1: f16[320, 320], arg17_1: f16[320], arg18_1: f16[320], arg19_1: f16[320], arg20_1: f16[2560, 320], arg21_1: f16[2560], arg22_1: f16[320, 1280], arg23_1: f16[320], arg24_1: f16[320, 320, 1, 1], arg25_1: f16[320], arg26_1: f16[2, 320, 64, 64], arg27_1: f16[2, 77, 768]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:202, code: hidden_states = self.norm(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg26_1, arg0_1, arg1_1, 2, 320, 4096, 32, 1e-06);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 320, 64, 64] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:204, code: hidden_states = self.proj_in(hidden_states)\n",
      "        convolution: f16[2, 320, 64, 64] = torch.ops.aten.convolution.default(getitem, arg2_1, arg3_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  getitem = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:206, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * weight, inner_dim)\n",
      "        permute: f16[2, 64, 64, 320] = torch.ops.aten.permute.default(convolution, [0, 2, 3, 1]);  convolution = None\n",
      "        view: f16[2, 4096, 320] = torch.ops.aten.view.default(permute, [2, 4096, 320]);  permute = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:482, code: self.norm1(hidden_states, timestep) if self.use_ada_layer_norm else self.norm1(hidden_states)\n",
      "        native_layer_norm = torch.ops.aten.native_layer_norm.default(view, [320], arg4_1, arg5_1, 1e-05);  arg4_1 = arg5_1 = None\n",
      "        getitem_3: f16[2, 4096, 320] = native_layer_norm[0]\n",
      "        getitem_4: f32[2, 4096, 1] = native_layer_norm[1]\n",
      "        getitem_5: f32[2, 4096, 1] = native_layer_norm[2];  native_layer_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t: f16[320, 320] = torch.ops.aten.t.default(arg6_1);  arg6_1 = None\n",
      "        view_1: f16[8192, 320] = torch.ops.aten.view.default(getitem_3, [8192, 320])\n",
      "        mm: f16[8192, 320] = torch.ops.aten.mm.default(view_1, t);  view_1 = t = None\n",
      "        _unsafe_view: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(mm, [2, 4096, 320]);  mm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_2: f16[2, 4096, 8, 40] = torch.ops.aten.view.default(_unsafe_view, [2, 4096, 8, 40]);  _unsafe_view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_1: f16[2, 8, 4096, 40] = torch.ops.aten.permute.default(view_2, [0, 2, 1, 3]);  view_2 = None\n",
      "        clone: f16[2, 8, 4096, 40] = torch.ops.aten.clone.default(permute_1, memory_format = torch.contiguous_format);  permute_1 = None\n",
      "        _unsafe_view_1: f16[16, 4096, 40] = torch.ops.aten._unsafe_view.default(clone, [16, 4096, 40]);  clone = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_1: f16[320, 320] = torch.ops.aten.t.default(arg7_1);  arg7_1 = None\n",
      "        view_3: f16[8192, 320] = torch.ops.aten.view.default(getitem_3, [8192, 320])\n",
      "        mm_1: f16[8192, 320] = torch.ops.aten.mm.default(view_3, t_1);  view_3 = t_1 = None\n",
      "        _unsafe_view_2: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(mm_1, [2, 4096, 320]);  mm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_2: f16[320, 320] = torch.ops.aten.t.default(arg8_1);  arg8_1 = None\n",
      "        view_4: f16[8192, 320] = torch.ops.aten.view.default(getitem_3, [8192, 320]);  getitem_3 = None\n",
      "        mm_2: f16[8192, 320] = torch.ops.aten.mm.default(view_4, t_2);  view_4 = t_2 = None\n",
      "        _unsafe_view_3: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(mm_2, [2, 4096, 320]);  mm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_5: f16[2, 4096, 8, 40] = torch.ops.aten.view.default(_unsafe_view_2, [2, 4096, 8, 40]);  _unsafe_view_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_2: f16[2, 8, 4096, 40] = torch.ops.aten.permute.default(view_5, [0, 2, 1, 3]);  view_5 = None\n",
      "        clone_1: f16[2, 8, 4096, 40] = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None\n",
      "        _unsafe_view_4: f16[16, 4096, 40] = torch.ops.aten._unsafe_view.default(clone_1, [16, 4096, 40]);  clone_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_6: f16[2, 4096, 8, 40] = torch.ops.aten.view.default(_unsafe_view_3, [2, 4096, 8, 40]);  _unsafe_view_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_3: f16[2, 8, 4096, 40] = torch.ops.aten.permute.default(view_6, [0, 2, 1, 3]);  view_6 = None\n",
      "        clone_2: f16[2, 8, 4096, 40] = torch.ops.aten.clone.default(permute_3, memory_format = torch.contiguous_format);  permute_3 = None\n",
      "        _unsafe_view_5: f16[16, 4096, 40] = torch.ops.aten._unsafe_view.default(clone_2, [16, 4096, 40]);  clone_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty: f16[16, 4096, 4096] = torch.ops.aten.empty.memory_format([16, 4096, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose: f16[16, 40, 4096] = torch.ops.aten.transpose.int(_unsafe_view_4, -1, -2);  _unsafe_view_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm: f16[16, 4096, 4096] = torch.ops.aten.baddbmm.default(empty, _unsafe_view_1, transpose, beta = 0, alpha = 0.15811388300841897);  empty = _unsafe_view_1 = transpose = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax: f16[16, 4096, 4096] = torch.ops.aten._softmax.default(baddbmm, -1, False);  baddbmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm: f16[16, 4096, 40] = torch.ops.aten.bmm.default(_softmax, _unsafe_view_5);  _softmax = _unsafe_view_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_7: f16[2, 8, 4096, 40] = torch.ops.aten.view.default(bmm, [2, 8, 4096, 40]);  bmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_4: f16[2, 4096, 8, 40] = torch.ops.aten.permute.default(view_7, [0, 2, 1, 3]);  view_7 = None\n",
      "        clone_3: f16[2, 4096, 8, 40] = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None\n",
      "        _unsafe_view_6: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(clone_3, [2, 4096, 320]);  clone_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_3: f16[320, 320] = torch.ops.aten.t.default(arg9_1);  arg9_1 = None\n",
      "        view_8: f16[8192, 320] = torch.ops.aten.view.default(_unsafe_view_6, [8192, 320]);  _unsafe_view_6 = None\n",
      "        addmm: f16[8192, 320] = torch.ops.aten.addmm.default(arg10_1, view_8, t_3);  arg10_1 = view_8 = t_3 = None\n",
      "        view_9: f16[2, 4096, 320] = torch.ops.aten.view.default(addmm, [2, 4096, 320]);  addmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:490, code: hidden_states = self.attn1(norm_hidden_states, attention_mask=attention_mask) + hidden_states\n",
      "        add: f16[2, 4096, 320] = torch.ops.aten.add.Tensor(view_9, view);  view_9 = view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:495, code: self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)\n",
      "        native_layer_norm_1 = torch.ops.aten.native_layer_norm.default(add, [320], arg11_1, arg12_1, 1e-05);  arg11_1 = arg12_1 = None\n",
      "        getitem_6: f16[2, 4096, 320] = native_layer_norm_1[0]\n",
      "        getitem_7: f32[2, 4096, 1] = native_layer_norm_1[1]\n",
      "        getitem_8: f32[2, 4096, 1] = native_layer_norm_1[2];  native_layer_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t_4: f16[320, 320] = torch.ops.aten.t.default(arg13_1);  arg13_1 = None\n",
      "        view_10: f16[8192, 320] = torch.ops.aten.view.default(getitem_6, [8192, 320]);  getitem_6 = None\n",
      "        mm_3: f16[8192, 320] = torch.ops.aten.mm.default(view_10, t_4);  view_10 = t_4 = None\n",
      "        _unsafe_view_7: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(mm_3, [2, 4096, 320]);  mm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_11: f16[2, 4096, 8, 40] = torch.ops.aten.view.default(_unsafe_view_7, [2, 4096, 8, 40]);  _unsafe_view_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_5: f16[2, 8, 4096, 40] = torch.ops.aten.permute.default(view_11, [0, 2, 1, 3]);  view_11 = None\n",
      "        clone_4: f16[2, 8, 4096, 40] = torch.ops.aten.clone.default(permute_5, memory_format = torch.contiguous_format);  permute_5 = None\n",
      "        _unsafe_view_8: f16[16, 4096, 40] = torch.ops.aten._unsafe_view.default(clone_4, [16, 4096, 40]);  clone_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_5: f16[768, 320] = torch.ops.aten.t.default(arg14_1);  arg14_1 = None\n",
      "        view_12: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768])\n",
      "        mm_4: f16[154, 320] = torch.ops.aten.mm.default(view_12, t_5);  view_12 = t_5 = None\n",
      "        _unsafe_view_9: f16[2, 77, 320] = torch.ops.aten._unsafe_view.default(mm_4, [2, 77, 320]);  mm_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_6: f16[768, 320] = torch.ops.aten.t.default(arg15_1);  arg15_1 = None\n",
      "        view_13: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768]);  arg27_1 = None\n",
      "        mm_5: f16[154, 320] = torch.ops.aten.mm.default(view_13, t_6);  view_13 = t_6 = None\n",
      "        _unsafe_view_10: f16[2, 77, 320] = torch.ops.aten._unsafe_view.default(mm_5, [2, 77, 320]);  mm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_14: f16[2, 77, 8, 40] = torch.ops.aten.view.default(_unsafe_view_9, [2, 77, 8, 40]);  _unsafe_view_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_6: f16[2, 8, 77, 40] = torch.ops.aten.permute.default(view_14, [0, 2, 1, 3]);  view_14 = None\n",
      "        clone_5: f16[2, 8, 77, 40] = torch.ops.aten.clone.default(permute_6, memory_format = torch.contiguous_format);  permute_6 = None\n",
      "        _unsafe_view_11: f16[16, 77, 40] = torch.ops.aten._unsafe_view.default(clone_5, [16, 77, 40]);  clone_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_15: f16[2, 77, 8, 40] = torch.ops.aten.view.default(_unsafe_view_10, [2, 77, 8, 40]);  _unsafe_view_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_7: f16[2, 8, 77, 40] = torch.ops.aten.permute.default(view_15, [0, 2, 1, 3]);  view_15 = None\n",
      "        clone_6: f16[2, 8, 77, 40] = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None\n",
      "        _unsafe_view_12: f16[16, 77, 40] = torch.ops.aten._unsafe_view.default(clone_6, [16, 77, 40]);  clone_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty_1: f16[16, 4096, 77] = torch.ops.aten.empty.memory_format([16, 4096, 77], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose_1: f16[16, 40, 77] = torch.ops.aten.transpose.int(_unsafe_view_11, -1, -2);  _unsafe_view_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm_1: f16[16, 4096, 77] = torch.ops.aten.baddbmm.default(empty_1, _unsafe_view_8, transpose_1, beta = 0, alpha = 0.15811388300841897);  empty_1 = _unsafe_view_8 = transpose_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax_1: f16[16, 4096, 77] = torch.ops.aten._softmax.default(baddbmm_1, -1, False);  baddbmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm_1: f16[16, 4096, 40] = torch.ops.aten.bmm.default(_softmax_1, _unsafe_view_12);  _softmax_1 = _unsafe_view_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_16: f16[2, 8, 4096, 40] = torch.ops.aten.view.default(bmm_1, [2, 8, 4096, 40]);  bmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_8: f16[2, 4096, 8, 40] = torch.ops.aten.permute.default(view_16, [0, 2, 1, 3]);  view_16 = None\n",
      "        clone_7: f16[2, 4096, 8, 40] = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None\n",
      "        _unsafe_view_13: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(clone_7, [2, 4096, 320]);  clone_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_7: f16[320, 320] = torch.ops.aten.t.default(arg16_1);  arg16_1 = None\n",
      "        view_17: f16[8192, 320] = torch.ops.aten.view.default(_unsafe_view_13, [8192, 320]);  _unsafe_view_13 = None\n",
      "        addmm_1: f16[8192, 320] = torch.ops.aten.addmm.default(arg17_1, view_17, t_7);  arg17_1 = view_17 = t_7 = None\n",
      "        view_18: f16[2, 4096, 320] = torch.ops.aten.view.default(addmm_1, [2, 4096, 320]);  addmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:498, code: self.attn2(\n",
      "        add_1: f16[2, 4096, 320] = torch.ops.aten.add.Tensor(view_18, add);  view_18 = add = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        native_layer_norm_2 = torch.ops.aten.native_layer_norm.default(add_1, [320], arg18_1, arg19_1, 1e-05);  arg18_1 = arg19_1 = None\n",
      "        getitem_9: f16[2, 4096, 320] = native_layer_norm_2[0]\n",
      "        getitem_10: f32[2, 4096, 1] = native_layer_norm_2[1]\n",
      "        getitem_11: f32[2, 4096, 1] = native_layer_norm_2[2];  native_layer_norm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:820, code: hidden_states, gate = self.proj(hidden_states).chunk(2, dim=-1)\n",
      "        t_8: f16[320, 2560] = torch.ops.aten.t.default(arg20_1);  arg20_1 = None\n",
      "        view_19: f16[8192, 320] = torch.ops.aten.view.default(getitem_9, [8192, 320]);  getitem_9 = None\n",
      "        addmm_2: f16[8192, 2560] = torch.ops.aten.addmm.default(arg21_1, view_19, t_8);  arg21_1 = view_19 = t_8 = None\n",
      "        view_20: f16[2, 4096, 2560] = torch.ops.aten.view.default(addmm_2, [2, 4096, 2560]);  addmm_2 = None\n",
      "        split = torch.ops.aten.split.Tensor(view_20, 1280, -1);  view_20 = None\n",
      "        getitem_12: f16[2, 4096, 1280] = split[0]\n",
      "        getitem_13: f16[2, 4096, 1280] = split[1];  split = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:815, code: return F.gelu(gate)\n",
      "        gelu: f16[2, 4096, 1280] = torch.ops.aten.gelu.default(getitem_13);  getitem_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:821, code: return hidden_states * self.gelu(gate)\n",
      "        mul: f16[2, 4096, 1280] = torch.ops.aten.mul.Tensor(getitem_12, gelu);  getitem_12 = gelu = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:774, code: hidden_states = module(hidden_states)\n",
      "        t_9: f16[1280, 320] = torch.ops.aten.t.default(arg22_1);  arg22_1 = None\n",
      "        view_21: f16[8192, 1280] = torch.ops.aten.view.default(mul, [8192, 1280]);  mul = None\n",
      "        addmm_3: f16[8192, 320] = torch.ops.aten.addmm.default(arg23_1, view_21, t_9);  arg23_1 = view_21 = t_9 = None\n",
      "        view_22: f16[2, 4096, 320] = torch.ops.aten.view.default(addmm_3, [2, 4096, 320]);  addmm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        add_2: f16[2, 4096, 320] = torch.ops.aten.add.Tensor(view_22, add_1);  view_22 = add_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:222, code: hidden_states.reshape(batch, height, weight, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
      "        view_23: f16[2, 64, 64, 320] = torch.ops.aten.view.default(add_2, [2, 64, 64, 320]);  add_2 = None\n",
      "        permute_9: f16[2, 320, 64, 64] = torch.ops.aten.permute.default(view_23, [0, 3, 1, 2]);  view_23 = None\n",
      "        clone_8: f16[2, 320, 64, 64] = torch.ops.aten.clone.default(permute_9, memory_format = torch.contiguous_format);  permute_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:224, code: hidden_states = self.proj_out(hidden_states)\n",
      "        convolution_1: f16[2, 320, 64, 64] = torch.ops.aten.convolution.default(clone_8, arg24_1, arg25_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  clone_8 = arg24_1 = arg25_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:231, code: output = hidden_states + residual\n",
      "        add_3: f16[2, 320, 64, 64] = torch.ops.aten.add.Tensor(convolution_1, arg26_1);  convolution_1 = arg26_1 = None\n",
      "        return (add_3,)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[320], arg1_1: f16[320], arg2_1: f16[320, 320, 3, 3], arg3_1: f16[320], arg4_1: f16[320, 1280], arg5_1: f16[320], arg6_1: f16[320], arg7_1: f16[320], arg8_1: f16[320, 320, 3, 3], arg9_1: f16[320], arg10_1: f16[2, 320, 64, 64], arg11_1: f16[2, 1280]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg10_1, arg0_1, arg1_1, 2, 320, 4096, 32, 1e-05);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 320, 64, 64] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu: f16[2, 320, 64, 64] = torch.ops.aten.silu.default(getitem);  getitem = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution: f16[2, 320, 64, 64] = torch.ops.aten.convolution.default(silu, arg2_1, arg3_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:474, code: temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
      "        silu_1: f16[2, 1280] = torch.ops.aten.silu.default(arg11_1);  arg11_1 = None\n",
      "        t: f16[1280, 320] = torch.ops.aten.t.default(arg4_1);  arg4_1 = None\n",
      "        addmm: f16[2, 320] = torch.ops.aten.addmm.default(arg5_1, silu_1, t);  arg5_1 = silu_1 = t = None\n",
      "        slice_1: f16[2, 320] = torch.ops.aten.slice.Tensor(addmm, 0, 0, 9223372036854775807);  addmm = None\n",
      "        slice_2: f16[2, 320] = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807);  slice_1 = None\n",
      "        unsqueeze: f16[2, 320, 1] = torch.ops.aten.unsqueeze.default(slice_2, 2);  slice_2 = None\n",
      "        unsqueeze_1: f16[2, 320, 1, 1] = torch.ops.aten.unsqueeze.default(unsqueeze, 3);  unsqueeze = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:477, code: hidden_states = hidden_states + temb\n",
      "        add: f16[2, 320, 64, 64] = torch.ops.aten.add.Tensor(convolution, unsqueeze_1);  convolution = unsqueeze_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_1 = torch.ops.aten.native_group_norm.default(add, arg6_1, arg7_1, 2, 320, 4096, 32, 1e-05);  add = arg6_1 = arg7_1 = None\n",
      "        getitem_3: f16[2, 320, 64, 64] = native_group_norm_1[0]\n",
      "        getitem_4: f16[2, 32] = native_group_norm_1[1]\n",
      "        getitem_5: f16[2, 32] = native_group_norm_1[2];  native_group_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_2: f16[2, 320, 64, 64] = torch.ops.aten.silu.default(getitem_3);  getitem_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_1: f16[2, 320, 64, 64] = torch.ops.aten.convolution.default(silu_2, arg8_1, arg9_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_2 = arg8_1 = arg9_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_1: f16[2, 320, 64, 64] = torch.ops.aten.add.Tensor(arg10_1, convolution_1);  arg10_1 = convolution_1 = None\n",
      "        div: f16[2, 320, 64, 64] = torch.ops.aten.div.Tensor(add_1, 1.0);  add_1 = None\n",
      "        return (div,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[320], arg1_1: f16[320], arg2_1: f16[320, 320, 1, 1], arg3_1: f16[320], arg4_1: f16[320], arg5_1: f16[320], arg6_1: f16[320, 320], arg7_1: f16[320, 320], arg8_1: f16[320, 320], arg9_1: f16[320, 320], arg10_1: f16[320], arg11_1: f16[320], arg12_1: f16[320], arg13_1: f16[320, 320], arg14_1: f16[320, 768], arg15_1: f16[320, 768], arg16_1: f16[320, 320], arg17_1: f16[320], arg18_1: f16[320], arg19_1: f16[320], arg20_1: f16[2560, 320], arg21_1: f16[2560], arg22_1: f16[320, 1280], arg23_1: f16[320], arg24_1: f16[320, 320, 1, 1], arg25_1: f16[320], arg26_1: f16[2, 320, 64, 64], arg27_1: f16[2, 77, 768]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:202, code: hidden_states = self.norm(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg26_1, arg0_1, arg1_1, 2, 320, 4096, 32, 1e-06);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 320, 64, 64] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:204, code: hidden_states = self.proj_in(hidden_states)\n",
      "        convolution: f16[2, 320, 64, 64] = torch.ops.aten.convolution.default(getitem, arg2_1, arg3_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  getitem = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:206, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * weight, inner_dim)\n",
      "        permute: f16[2, 64, 64, 320] = torch.ops.aten.permute.default(convolution, [0, 2, 3, 1]);  convolution = None\n",
      "        view: f16[2, 4096, 320] = torch.ops.aten.view.default(permute, [2, 4096, 320]);  permute = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:482, code: self.norm1(hidden_states, timestep) if self.use_ada_layer_norm else self.norm1(hidden_states)\n",
      "        native_layer_norm = torch.ops.aten.native_layer_norm.default(view, [320], arg4_1, arg5_1, 1e-05);  arg4_1 = arg5_1 = None\n",
      "        getitem_3: f16[2, 4096, 320] = native_layer_norm[0]\n",
      "        getitem_4: f32[2, 4096, 1] = native_layer_norm[1]\n",
      "        getitem_5: f32[2, 4096, 1] = native_layer_norm[2];  native_layer_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t: f16[320, 320] = torch.ops.aten.t.default(arg6_1);  arg6_1 = None\n",
      "        view_1: f16[8192, 320] = torch.ops.aten.view.default(getitem_3, [8192, 320])\n",
      "        mm: f16[8192, 320] = torch.ops.aten.mm.default(view_1, t);  view_1 = t = None\n",
      "        _unsafe_view: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(mm, [2, 4096, 320]);  mm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_2: f16[2, 4096, 8, 40] = torch.ops.aten.view.default(_unsafe_view, [2, 4096, 8, 40]);  _unsafe_view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_1: f16[2, 8, 4096, 40] = torch.ops.aten.permute.default(view_2, [0, 2, 1, 3]);  view_2 = None\n",
      "        clone: f16[2, 8, 4096, 40] = torch.ops.aten.clone.default(permute_1, memory_format = torch.contiguous_format);  permute_1 = None\n",
      "        _unsafe_view_1: f16[16, 4096, 40] = torch.ops.aten._unsafe_view.default(clone, [16, 4096, 40]);  clone = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_1: f16[320, 320] = torch.ops.aten.t.default(arg7_1);  arg7_1 = None\n",
      "        view_3: f16[8192, 320] = torch.ops.aten.view.default(getitem_3, [8192, 320])\n",
      "        mm_1: f16[8192, 320] = torch.ops.aten.mm.default(view_3, t_1);  view_3 = t_1 = None\n",
      "        _unsafe_view_2: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(mm_1, [2, 4096, 320]);  mm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_2: f16[320, 320] = torch.ops.aten.t.default(arg8_1);  arg8_1 = None\n",
      "        view_4: f16[8192, 320] = torch.ops.aten.view.default(getitem_3, [8192, 320]);  getitem_3 = None\n",
      "        mm_2: f16[8192, 320] = torch.ops.aten.mm.default(view_4, t_2);  view_4 = t_2 = None\n",
      "        _unsafe_view_3: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(mm_2, [2, 4096, 320]);  mm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_5: f16[2, 4096, 8, 40] = torch.ops.aten.view.default(_unsafe_view_2, [2, 4096, 8, 40]);  _unsafe_view_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_2: f16[2, 8, 4096, 40] = torch.ops.aten.permute.default(view_5, [0, 2, 1, 3]);  view_5 = None\n",
      "        clone_1: f16[2, 8, 4096, 40] = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None\n",
      "        _unsafe_view_4: f16[16, 4096, 40] = torch.ops.aten._unsafe_view.default(clone_1, [16, 4096, 40]);  clone_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_6: f16[2, 4096, 8, 40] = torch.ops.aten.view.default(_unsafe_view_3, [2, 4096, 8, 40]);  _unsafe_view_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_3: f16[2, 8, 4096, 40] = torch.ops.aten.permute.default(view_6, [0, 2, 1, 3]);  view_6 = None\n",
      "        clone_2: f16[2, 8, 4096, 40] = torch.ops.aten.clone.default(permute_3, memory_format = torch.contiguous_format);  permute_3 = None\n",
      "        _unsafe_view_5: f16[16, 4096, 40] = torch.ops.aten._unsafe_view.default(clone_2, [16, 4096, 40]);  clone_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty: f16[16, 4096, 4096] = torch.ops.aten.empty.memory_format([16, 4096, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose: f16[16, 40, 4096] = torch.ops.aten.transpose.int(_unsafe_view_4, -1, -2);  _unsafe_view_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm: f16[16, 4096, 4096] = torch.ops.aten.baddbmm.default(empty, _unsafe_view_1, transpose, beta = 0, alpha = 0.15811388300841897);  empty = _unsafe_view_1 = transpose = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax: f16[16, 4096, 4096] = torch.ops.aten._softmax.default(baddbmm, -1, False);  baddbmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm: f16[16, 4096, 40] = torch.ops.aten.bmm.default(_softmax, _unsafe_view_5);  _softmax = _unsafe_view_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_7: f16[2, 8, 4096, 40] = torch.ops.aten.view.default(bmm, [2, 8, 4096, 40]);  bmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_4: f16[2, 4096, 8, 40] = torch.ops.aten.permute.default(view_7, [0, 2, 1, 3]);  view_7 = None\n",
      "        clone_3: f16[2, 4096, 8, 40] = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None\n",
      "        _unsafe_view_6: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(clone_3, [2, 4096, 320]);  clone_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_3: f16[320, 320] = torch.ops.aten.t.default(arg9_1);  arg9_1 = None\n",
      "        view_8: f16[8192, 320] = torch.ops.aten.view.default(_unsafe_view_6, [8192, 320]);  _unsafe_view_6 = None\n",
      "        addmm: f16[8192, 320] = torch.ops.aten.addmm.default(arg10_1, view_8, t_3);  arg10_1 = view_8 = t_3 = None\n",
      "        view_9: f16[2, 4096, 320] = torch.ops.aten.view.default(addmm, [2, 4096, 320]);  addmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:490, code: hidden_states = self.attn1(norm_hidden_states, attention_mask=attention_mask) + hidden_states\n",
      "        add: f16[2, 4096, 320] = torch.ops.aten.add.Tensor(view_9, view);  view_9 = view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:495, code: self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)\n",
      "        native_layer_norm_1 = torch.ops.aten.native_layer_norm.default(add, [320], arg11_1, arg12_1, 1e-05);  arg11_1 = arg12_1 = None\n",
      "        getitem_6: f16[2, 4096, 320] = native_layer_norm_1[0]\n",
      "        getitem_7: f32[2, 4096, 1] = native_layer_norm_1[1]\n",
      "        getitem_8: f32[2, 4096, 1] = native_layer_norm_1[2];  native_layer_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t_4: f16[320, 320] = torch.ops.aten.t.default(arg13_1);  arg13_1 = None\n",
      "        view_10: f16[8192, 320] = torch.ops.aten.view.default(getitem_6, [8192, 320]);  getitem_6 = None\n",
      "        mm_3: f16[8192, 320] = torch.ops.aten.mm.default(view_10, t_4);  view_10 = t_4 = None\n",
      "        _unsafe_view_7: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(mm_3, [2, 4096, 320]);  mm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_11: f16[2, 4096, 8, 40] = torch.ops.aten.view.default(_unsafe_view_7, [2, 4096, 8, 40]);  _unsafe_view_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_5: f16[2, 8, 4096, 40] = torch.ops.aten.permute.default(view_11, [0, 2, 1, 3]);  view_11 = None\n",
      "        clone_4: f16[2, 8, 4096, 40] = torch.ops.aten.clone.default(permute_5, memory_format = torch.contiguous_format);  permute_5 = None\n",
      "        _unsafe_view_8: f16[16, 4096, 40] = torch.ops.aten._unsafe_view.default(clone_4, [16, 4096, 40]);  clone_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_5: f16[768, 320] = torch.ops.aten.t.default(arg14_1);  arg14_1 = None\n",
      "        view_12: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768])\n",
      "        mm_4: f16[154, 320] = torch.ops.aten.mm.default(view_12, t_5);  view_12 = t_5 = None\n",
      "        _unsafe_view_9: f16[2, 77, 320] = torch.ops.aten._unsafe_view.default(mm_4, [2, 77, 320]);  mm_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_6: f16[768, 320] = torch.ops.aten.t.default(arg15_1);  arg15_1 = None\n",
      "        view_13: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768]);  arg27_1 = None\n",
      "        mm_5: f16[154, 320] = torch.ops.aten.mm.default(view_13, t_6);  view_13 = t_6 = None\n",
      "        _unsafe_view_10: f16[2, 77, 320] = torch.ops.aten._unsafe_view.default(mm_5, [2, 77, 320]);  mm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_14: f16[2, 77, 8, 40] = torch.ops.aten.view.default(_unsafe_view_9, [2, 77, 8, 40]);  _unsafe_view_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_6: f16[2, 8, 77, 40] = torch.ops.aten.permute.default(view_14, [0, 2, 1, 3]);  view_14 = None\n",
      "        clone_5: f16[2, 8, 77, 40] = torch.ops.aten.clone.default(permute_6, memory_format = torch.contiguous_format);  permute_6 = None\n",
      "        _unsafe_view_11: f16[16, 77, 40] = torch.ops.aten._unsafe_view.default(clone_5, [16, 77, 40]);  clone_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_15: f16[2, 77, 8, 40] = torch.ops.aten.view.default(_unsafe_view_10, [2, 77, 8, 40]);  _unsafe_view_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_7: f16[2, 8, 77, 40] = torch.ops.aten.permute.default(view_15, [0, 2, 1, 3]);  view_15 = None\n",
      "        clone_6: f16[2, 8, 77, 40] = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None\n",
      "        _unsafe_view_12: f16[16, 77, 40] = torch.ops.aten._unsafe_view.default(clone_6, [16, 77, 40]);  clone_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty_1: f16[16, 4096, 77] = torch.ops.aten.empty.memory_format([16, 4096, 77], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose_1: f16[16, 40, 77] = torch.ops.aten.transpose.int(_unsafe_view_11, -1, -2);  _unsafe_view_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm_1: f16[16, 4096, 77] = torch.ops.aten.baddbmm.default(empty_1, _unsafe_view_8, transpose_1, beta = 0, alpha = 0.15811388300841897);  empty_1 = _unsafe_view_8 = transpose_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax_1: f16[16, 4096, 77] = torch.ops.aten._softmax.default(baddbmm_1, -1, False);  baddbmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm_1: f16[16, 4096, 40] = torch.ops.aten.bmm.default(_softmax_1, _unsafe_view_12);  _softmax_1 = _unsafe_view_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_16: f16[2, 8, 4096, 40] = torch.ops.aten.view.default(bmm_1, [2, 8, 4096, 40]);  bmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_8: f16[2, 4096, 8, 40] = torch.ops.aten.permute.default(view_16, [0, 2, 1, 3]);  view_16 = None\n",
      "        clone_7: f16[2, 4096, 8, 40] = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None\n",
      "        _unsafe_view_13: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(clone_7, [2, 4096, 320]);  clone_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_7: f16[320, 320] = torch.ops.aten.t.default(arg16_1);  arg16_1 = None\n",
      "        view_17: f16[8192, 320] = torch.ops.aten.view.default(_unsafe_view_13, [8192, 320]);  _unsafe_view_13 = None\n",
      "        addmm_1: f16[8192, 320] = torch.ops.aten.addmm.default(arg17_1, view_17, t_7);  arg17_1 = view_17 = t_7 = None\n",
      "        view_18: f16[2, 4096, 320] = torch.ops.aten.view.default(addmm_1, [2, 4096, 320]);  addmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:498, code: self.attn2(\n",
      "        add_1: f16[2, 4096, 320] = torch.ops.aten.add.Tensor(view_18, add);  view_18 = add = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        native_layer_norm_2 = torch.ops.aten.native_layer_norm.default(add_1, [320], arg18_1, arg19_1, 1e-05);  arg18_1 = arg19_1 = None\n",
      "        getitem_9: f16[2, 4096, 320] = native_layer_norm_2[0]\n",
      "        getitem_10: f32[2, 4096, 1] = native_layer_norm_2[1]\n",
      "        getitem_11: f32[2, 4096, 1] = native_layer_norm_2[2];  native_layer_norm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:820, code: hidden_states, gate = self.proj(hidden_states).chunk(2, dim=-1)\n",
      "        t_8: f16[320, 2560] = torch.ops.aten.t.default(arg20_1);  arg20_1 = None\n",
      "        view_19: f16[8192, 320] = torch.ops.aten.view.default(getitem_9, [8192, 320]);  getitem_9 = None\n",
      "        addmm_2: f16[8192, 2560] = torch.ops.aten.addmm.default(arg21_1, view_19, t_8);  arg21_1 = view_19 = t_8 = None\n",
      "        view_20: f16[2, 4096, 2560] = torch.ops.aten.view.default(addmm_2, [2, 4096, 2560]);  addmm_2 = None\n",
      "        split = torch.ops.aten.split.Tensor(view_20, 1280, -1);  view_20 = None\n",
      "        getitem_12: f16[2, 4096, 1280] = split[0]\n",
      "        getitem_13: f16[2, 4096, 1280] = split[1];  split = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:815, code: return F.gelu(gate)\n",
      "        gelu: f16[2, 4096, 1280] = torch.ops.aten.gelu.default(getitem_13);  getitem_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:821, code: return hidden_states * self.gelu(gate)\n",
      "        mul: f16[2, 4096, 1280] = torch.ops.aten.mul.Tensor(getitem_12, gelu);  getitem_12 = gelu = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:774, code: hidden_states = module(hidden_states)\n",
      "        t_9: f16[1280, 320] = torch.ops.aten.t.default(arg22_1);  arg22_1 = None\n",
      "        view_21: f16[8192, 1280] = torch.ops.aten.view.default(mul, [8192, 1280]);  mul = None\n",
      "        addmm_3: f16[8192, 320] = torch.ops.aten.addmm.default(arg23_1, view_21, t_9);  arg23_1 = view_21 = t_9 = None\n",
      "        view_22: f16[2, 4096, 320] = torch.ops.aten.view.default(addmm_3, [2, 4096, 320]);  addmm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        add_2: f16[2, 4096, 320] = torch.ops.aten.add.Tensor(view_22, add_1);  view_22 = add_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:222, code: hidden_states.reshape(batch, height, weight, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
      "        view_23: f16[2, 64, 64, 320] = torch.ops.aten.view.default(add_2, [2, 64, 64, 320]);  add_2 = None\n",
      "        permute_9: f16[2, 320, 64, 64] = torch.ops.aten.permute.default(view_23, [0, 3, 1, 2]);  view_23 = None\n",
      "        clone_8: f16[2, 320, 64, 64] = torch.ops.aten.clone.default(permute_9, memory_format = torch.contiguous_format);  permute_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:224, code: hidden_states = self.proj_out(hidden_states)\n",
      "        convolution_1: f16[2, 320, 64, 64] = torch.ops.aten.convolution.default(clone_8, arg24_1, arg25_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  clone_8 = arg24_1 = arg25_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:231, code: output = hidden_states + residual\n",
      "        add_3: f16[2, 320, 64, 64] = torch.ops.aten.add.Tensor(convolution_1, arg26_1);  convolution_1 = arg26_1 = None\n",
      "        return (add_3,)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[320, 320, 3, 3], arg1_1: f16[320], arg2_1: f16[2, 320, 64, 64]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:188, code: hidden_states = self.conv(hidden_states)\n",
      "        convolution: f16[2, 320, 32, 32] = torch.ops.aten.convolution.default(arg2_1, arg0_1, arg1_1, [2, 2], [1, 1], [1, 1], False, [0, 0], 1);  arg2_1 = arg0_1 = arg1_1 = None\n",
      "        return (convolution,)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[320], arg1_1: f16[320], arg2_1: f16[640, 320, 3, 3], arg3_1: f16[640], arg4_1: f16[640, 1280], arg5_1: f16[640], arg6_1: f16[640], arg7_1: f16[640], arg8_1: f16[640, 640, 3, 3], arg9_1: f16[640], arg10_1: f16[640, 320, 1, 1], arg11_1: f16[640], arg12_1: f16[2, 320, 32, 32], arg13_1: f16[2, 1280]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg12_1, arg0_1, arg1_1, 2, 320, 1024, 32, 1e-05);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 320, 32, 32] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu: f16[2, 320, 32, 32] = torch.ops.aten.silu.default(getitem);  getitem = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(silu, arg2_1, arg3_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:474, code: temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
      "        silu_1: f16[2, 1280] = torch.ops.aten.silu.default(arg13_1);  arg13_1 = None\n",
      "        t: f16[1280, 640] = torch.ops.aten.t.default(arg4_1);  arg4_1 = None\n",
      "        addmm: f16[2, 640] = torch.ops.aten.addmm.default(arg5_1, silu_1, t);  arg5_1 = silu_1 = t = None\n",
      "        slice_1: f16[2, 640] = torch.ops.aten.slice.Tensor(addmm, 0, 0, 9223372036854775807);  addmm = None\n",
      "        slice_2: f16[2, 640] = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807);  slice_1 = None\n",
      "        unsqueeze: f16[2, 640, 1] = torch.ops.aten.unsqueeze.default(slice_2, 2);  slice_2 = None\n",
      "        unsqueeze_1: f16[2, 640, 1, 1] = torch.ops.aten.unsqueeze.default(unsqueeze, 3);  unsqueeze = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:477, code: hidden_states = hidden_states + temb\n",
      "        add: f16[2, 640, 32, 32] = torch.ops.aten.add.Tensor(convolution, unsqueeze_1);  convolution = unsqueeze_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_1 = torch.ops.aten.native_group_norm.default(add, arg6_1, arg7_1, 2, 640, 1024, 32, 1e-05);  add = arg6_1 = arg7_1 = None\n",
      "        getitem_3: f16[2, 640, 32, 32] = native_group_norm_1[0]\n",
      "        getitem_4: f16[2, 32] = native_group_norm_1[1]\n",
      "        getitem_5: f16[2, 32] = native_group_norm_1[2];  native_group_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_2: f16[2, 640, 32, 32] = torch.ops.aten.silu.default(getitem_3);  getitem_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_1: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(silu_2, arg8_1, arg9_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_2 = arg8_1 = arg9_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:491, code: input_tensor = self.conv_shortcut(input_tensor)\n",
      "        convolution_2: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(arg12_1, arg10_1, arg11_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  arg12_1 = arg10_1 = arg11_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_1: f16[2, 640, 32, 32] = torch.ops.aten.add.Tensor(convolution_2, convolution_1);  convolution_2 = convolution_1 = None\n",
      "        div: f16[2, 640, 32, 32] = torch.ops.aten.div.Tensor(add_1, 1.0);  add_1 = None\n",
      "        return (div,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[640], arg1_1: f16[640], arg2_1: f16[640, 640, 1, 1], arg3_1: f16[640], arg4_1: f16[640], arg5_1: f16[640], arg6_1: f16[640, 640], arg7_1: f16[640, 640], arg8_1: f16[640, 640], arg9_1: f16[640, 640], arg10_1: f16[640], arg11_1: f16[640], arg12_1: f16[640], arg13_1: f16[640, 640], arg14_1: f16[640, 768], arg15_1: f16[640, 768], arg16_1: f16[640, 640], arg17_1: f16[640], arg18_1: f16[640], arg19_1: f16[640], arg20_1: f16[5120, 640], arg21_1: f16[5120], arg22_1: f16[640, 2560], arg23_1: f16[640], arg24_1: f16[640, 640, 1, 1], arg25_1: f16[640], arg26_1: f16[2, 640, 32, 32], arg27_1: f16[2, 77, 768]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:202, code: hidden_states = self.norm(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg26_1, arg0_1, arg1_1, 2, 640, 1024, 32, 1e-06);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 640, 32, 32] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:204, code: hidden_states = self.proj_in(hidden_states)\n",
      "        convolution: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(getitem, arg2_1, arg3_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  getitem = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:206, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * weight, inner_dim)\n",
      "        permute: f16[2, 32, 32, 640] = torch.ops.aten.permute.default(convolution, [0, 2, 3, 1]);  convolution = None\n",
      "        view: f16[2, 1024, 640] = torch.ops.aten.view.default(permute, [2, 1024, 640]);  permute = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:482, code: self.norm1(hidden_states, timestep) if self.use_ada_layer_norm else self.norm1(hidden_states)\n",
      "        native_layer_norm = torch.ops.aten.native_layer_norm.default(view, [640], arg4_1, arg5_1, 1e-05);  arg4_1 = arg5_1 = None\n",
      "        getitem_3: f16[2, 1024, 640] = native_layer_norm[0]\n",
      "        getitem_4: f32[2, 1024, 1] = native_layer_norm[1]\n",
      "        getitem_5: f32[2, 1024, 1] = native_layer_norm[2];  native_layer_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t: f16[640, 640] = torch.ops.aten.t.default(arg6_1);  arg6_1 = None\n",
      "        view_1: f16[2048, 640] = torch.ops.aten.view.default(getitem_3, [2048, 640])\n",
      "        mm: f16[2048, 640] = torch.ops.aten.mm.default(view_1, t);  view_1 = t = None\n",
      "        _unsafe_view: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(mm, [2, 1024, 640]);  mm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_2: f16[2, 1024, 8, 80] = torch.ops.aten.view.default(_unsafe_view, [2, 1024, 8, 80]);  _unsafe_view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_1: f16[2, 8, 1024, 80] = torch.ops.aten.permute.default(view_2, [0, 2, 1, 3]);  view_2 = None\n",
      "        clone: f16[2, 8, 1024, 80] = torch.ops.aten.clone.default(permute_1, memory_format = torch.contiguous_format);  permute_1 = None\n",
      "        _unsafe_view_1: f16[16, 1024, 80] = torch.ops.aten._unsafe_view.default(clone, [16, 1024, 80]);  clone = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_1: f16[640, 640] = torch.ops.aten.t.default(arg7_1);  arg7_1 = None\n",
      "        view_3: f16[2048, 640] = torch.ops.aten.view.default(getitem_3, [2048, 640])\n",
      "        mm_1: f16[2048, 640] = torch.ops.aten.mm.default(view_3, t_1);  view_3 = t_1 = None\n",
      "        _unsafe_view_2: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(mm_1, [2, 1024, 640]);  mm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_2: f16[640, 640] = torch.ops.aten.t.default(arg8_1);  arg8_1 = None\n",
      "        view_4: f16[2048, 640] = torch.ops.aten.view.default(getitem_3, [2048, 640]);  getitem_3 = None\n",
      "        mm_2: f16[2048, 640] = torch.ops.aten.mm.default(view_4, t_2);  view_4 = t_2 = None\n",
      "        _unsafe_view_3: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(mm_2, [2, 1024, 640]);  mm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_5: f16[2, 1024, 8, 80] = torch.ops.aten.view.default(_unsafe_view_2, [2, 1024, 8, 80]);  _unsafe_view_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_2: f16[2, 8, 1024, 80] = torch.ops.aten.permute.default(view_5, [0, 2, 1, 3]);  view_5 = None\n",
      "        clone_1: f16[2, 8, 1024, 80] = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None\n",
      "        _unsafe_view_4: f16[16, 1024, 80] = torch.ops.aten._unsafe_view.default(clone_1, [16, 1024, 80]);  clone_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_6: f16[2, 1024, 8, 80] = torch.ops.aten.view.default(_unsafe_view_3, [2, 1024, 8, 80]);  _unsafe_view_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_3: f16[2, 8, 1024, 80] = torch.ops.aten.permute.default(view_6, [0, 2, 1, 3]);  view_6 = None\n",
      "        clone_2: f16[2, 8, 1024, 80] = torch.ops.aten.clone.default(permute_3, memory_format = torch.contiguous_format);  permute_3 = None\n",
      "        _unsafe_view_5: f16[16, 1024, 80] = torch.ops.aten._unsafe_view.default(clone_2, [16, 1024, 80]);  clone_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty: f16[16, 1024, 1024] = torch.ops.aten.empty.memory_format([16, 1024, 1024], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose: f16[16, 80, 1024] = torch.ops.aten.transpose.int(_unsafe_view_4, -1, -2);  _unsafe_view_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm: f16[16, 1024, 1024] = torch.ops.aten.baddbmm.default(empty, _unsafe_view_1, transpose, beta = 0, alpha = 0.11180339887498948);  empty = _unsafe_view_1 = transpose = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax: f16[16, 1024, 1024] = torch.ops.aten._softmax.default(baddbmm, -1, False);  baddbmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm: f16[16, 1024, 80] = torch.ops.aten.bmm.default(_softmax, _unsafe_view_5);  _softmax = _unsafe_view_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_7: f16[2, 8, 1024, 80] = torch.ops.aten.view.default(bmm, [2, 8, 1024, 80]);  bmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_4: f16[2, 1024, 8, 80] = torch.ops.aten.permute.default(view_7, [0, 2, 1, 3]);  view_7 = None\n",
      "        clone_3: f16[2, 1024, 8, 80] = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None\n",
      "        _unsafe_view_6: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(clone_3, [2, 1024, 640]);  clone_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_3: f16[640, 640] = torch.ops.aten.t.default(arg9_1);  arg9_1 = None\n",
      "        view_8: f16[2048, 640] = torch.ops.aten.view.default(_unsafe_view_6, [2048, 640]);  _unsafe_view_6 = None\n",
      "        addmm: f16[2048, 640] = torch.ops.aten.addmm.default(arg10_1, view_8, t_3);  arg10_1 = view_8 = t_3 = None\n",
      "        view_9: f16[2, 1024, 640] = torch.ops.aten.view.default(addmm, [2, 1024, 640]);  addmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:490, code: hidden_states = self.attn1(norm_hidden_states, attention_mask=attention_mask) + hidden_states\n",
      "        add: f16[2, 1024, 640] = torch.ops.aten.add.Tensor(view_9, view);  view_9 = view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:495, code: self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)\n",
      "        native_layer_norm_1 = torch.ops.aten.native_layer_norm.default(add, [640], arg11_1, arg12_1, 1e-05);  arg11_1 = arg12_1 = None\n",
      "        getitem_6: f16[2, 1024, 640] = native_layer_norm_1[0]\n",
      "        getitem_7: f32[2, 1024, 1] = native_layer_norm_1[1]\n",
      "        getitem_8: f32[2, 1024, 1] = native_layer_norm_1[2];  native_layer_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t_4: f16[640, 640] = torch.ops.aten.t.default(arg13_1);  arg13_1 = None\n",
      "        view_10: f16[2048, 640] = torch.ops.aten.view.default(getitem_6, [2048, 640]);  getitem_6 = None\n",
      "        mm_3: f16[2048, 640] = torch.ops.aten.mm.default(view_10, t_4);  view_10 = t_4 = None\n",
      "        _unsafe_view_7: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(mm_3, [2, 1024, 640]);  mm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_11: f16[2, 1024, 8, 80] = torch.ops.aten.view.default(_unsafe_view_7, [2, 1024, 8, 80]);  _unsafe_view_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_5: f16[2, 8, 1024, 80] = torch.ops.aten.permute.default(view_11, [0, 2, 1, 3]);  view_11 = None\n",
      "        clone_4: f16[2, 8, 1024, 80] = torch.ops.aten.clone.default(permute_5, memory_format = torch.contiguous_format);  permute_5 = None\n",
      "        _unsafe_view_8: f16[16, 1024, 80] = torch.ops.aten._unsafe_view.default(clone_4, [16, 1024, 80]);  clone_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_5: f16[768, 640] = torch.ops.aten.t.default(arg14_1);  arg14_1 = None\n",
      "        view_12: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768])\n",
      "        mm_4: f16[154, 640] = torch.ops.aten.mm.default(view_12, t_5);  view_12 = t_5 = None\n",
      "        _unsafe_view_9: f16[2, 77, 640] = torch.ops.aten._unsafe_view.default(mm_4, [2, 77, 640]);  mm_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_6: f16[768, 640] = torch.ops.aten.t.default(arg15_1);  arg15_1 = None\n",
      "        view_13: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768]);  arg27_1 = None\n",
      "        mm_5: f16[154, 640] = torch.ops.aten.mm.default(view_13, t_6);  view_13 = t_6 = None\n",
      "        _unsafe_view_10: f16[2, 77, 640] = torch.ops.aten._unsafe_view.default(mm_5, [2, 77, 640]);  mm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_14: f16[2, 77, 8, 80] = torch.ops.aten.view.default(_unsafe_view_9, [2, 77, 8, 80]);  _unsafe_view_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_6: f16[2, 8, 77, 80] = torch.ops.aten.permute.default(view_14, [0, 2, 1, 3]);  view_14 = None\n",
      "        clone_5: f16[2, 8, 77, 80] = torch.ops.aten.clone.default(permute_6, memory_format = torch.contiguous_format);  permute_6 = None\n",
      "        _unsafe_view_11: f16[16, 77, 80] = torch.ops.aten._unsafe_view.default(clone_5, [16, 77, 80]);  clone_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_15: f16[2, 77, 8, 80] = torch.ops.aten.view.default(_unsafe_view_10, [2, 77, 8, 80]);  _unsafe_view_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_7: f16[2, 8, 77, 80] = torch.ops.aten.permute.default(view_15, [0, 2, 1, 3]);  view_15 = None\n",
      "        clone_6: f16[2, 8, 77, 80] = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None\n",
      "        _unsafe_view_12: f16[16, 77, 80] = torch.ops.aten._unsafe_view.default(clone_6, [16, 77, 80]);  clone_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty_1: f16[16, 1024, 77] = torch.ops.aten.empty.memory_format([16, 1024, 77], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose_1: f16[16, 80, 77] = torch.ops.aten.transpose.int(_unsafe_view_11, -1, -2);  _unsafe_view_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm_1: f16[16, 1024, 77] = torch.ops.aten.baddbmm.default(empty_1, _unsafe_view_8, transpose_1, beta = 0, alpha = 0.11180339887498948);  empty_1 = _unsafe_view_8 = transpose_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax_1: f16[16, 1024, 77] = torch.ops.aten._softmax.default(baddbmm_1, -1, False);  baddbmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm_1: f16[16, 1024, 80] = torch.ops.aten.bmm.default(_softmax_1, _unsafe_view_12);  _softmax_1 = _unsafe_view_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_16: f16[2, 8, 1024, 80] = torch.ops.aten.view.default(bmm_1, [2, 8, 1024, 80]);  bmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_8: f16[2, 1024, 8, 80] = torch.ops.aten.permute.default(view_16, [0, 2, 1, 3]);  view_16 = None\n",
      "        clone_7: f16[2, 1024, 8, 80] = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None\n",
      "        _unsafe_view_13: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(clone_7, [2, 1024, 640]);  clone_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_7: f16[640, 640] = torch.ops.aten.t.default(arg16_1);  arg16_1 = None\n",
      "        view_17: f16[2048, 640] = torch.ops.aten.view.default(_unsafe_view_13, [2048, 640]);  _unsafe_view_13 = None\n",
      "        addmm_1: f16[2048, 640] = torch.ops.aten.addmm.default(arg17_1, view_17, t_7);  arg17_1 = view_17 = t_7 = None\n",
      "        view_18: f16[2, 1024, 640] = torch.ops.aten.view.default(addmm_1, [2, 1024, 640]);  addmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:498, code: self.attn2(\n",
      "        add_1: f16[2, 1024, 640] = torch.ops.aten.add.Tensor(view_18, add);  view_18 = add = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        native_layer_norm_2 = torch.ops.aten.native_layer_norm.default(add_1, [640], arg18_1, arg19_1, 1e-05);  arg18_1 = arg19_1 = None\n",
      "        getitem_9: f16[2, 1024, 640] = native_layer_norm_2[0]\n",
      "        getitem_10: f32[2, 1024, 1] = native_layer_norm_2[1]\n",
      "        getitem_11: f32[2, 1024, 1] = native_layer_norm_2[2];  native_layer_norm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:820, code: hidden_states, gate = self.proj(hidden_states).chunk(2, dim=-1)\n",
      "        t_8: f16[640, 5120] = torch.ops.aten.t.default(arg20_1);  arg20_1 = None\n",
      "        view_19: f16[2048, 640] = torch.ops.aten.view.default(getitem_9, [2048, 640]);  getitem_9 = None\n",
      "        addmm_2: f16[2048, 5120] = torch.ops.aten.addmm.default(arg21_1, view_19, t_8);  arg21_1 = view_19 = t_8 = None\n",
      "        view_20: f16[2, 1024, 5120] = torch.ops.aten.view.default(addmm_2, [2, 1024, 5120]);  addmm_2 = None\n",
      "        split = torch.ops.aten.split.Tensor(view_20, 2560, -1);  view_20 = None\n",
      "        getitem_12: f16[2, 1024, 2560] = split[0]\n",
      "        getitem_13: f16[2, 1024, 2560] = split[1];  split = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:815, code: return F.gelu(gate)\n",
      "        gelu: f16[2, 1024, 2560] = torch.ops.aten.gelu.default(getitem_13);  getitem_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:821, code: return hidden_states * self.gelu(gate)\n",
      "        mul: f16[2, 1024, 2560] = torch.ops.aten.mul.Tensor(getitem_12, gelu);  getitem_12 = gelu = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:774, code: hidden_states = module(hidden_states)\n",
      "        t_9: f16[2560, 640] = torch.ops.aten.t.default(arg22_1);  arg22_1 = None\n",
      "        view_21: f16[2048, 2560] = torch.ops.aten.view.default(mul, [2048, 2560]);  mul = None\n",
      "        addmm_3: f16[2048, 640] = torch.ops.aten.addmm.default(arg23_1, view_21, t_9);  arg23_1 = view_21 = t_9 = None\n",
      "        view_22: f16[2, 1024, 640] = torch.ops.aten.view.default(addmm_3, [2, 1024, 640]);  addmm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        add_2: f16[2, 1024, 640] = torch.ops.aten.add.Tensor(view_22, add_1);  view_22 = add_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:222, code: hidden_states.reshape(batch, height, weight, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
      "        view_23: f16[2, 32, 32, 640] = torch.ops.aten.view.default(add_2, [2, 32, 32, 640]);  add_2 = None\n",
      "        permute_9: f16[2, 640, 32, 32] = torch.ops.aten.permute.default(view_23, [0, 3, 1, 2]);  view_23 = None\n",
      "        clone_8: f16[2, 640, 32, 32] = torch.ops.aten.clone.default(permute_9, memory_format = torch.contiguous_format);  permute_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:224, code: hidden_states = self.proj_out(hidden_states)\n",
      "        convolution_1: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(clone_8, arg24_1, arg25_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  clone_8 = arg24_1 = arg25_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:231, code: output = hidden_states + residual\n",
      "        add_3: f16[2, 640, 32, 32] = torch.ops.aten.add.Tensor(convolution_1, arg26_1);  convolution_1 = arg26_1 = None\n",
      "        return (add_3,)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[640], arg1_1: f16[640], arg2_1: f16[640, 640, 3, 3], arg3_1: f16[640], arg4_1: f16[640, 1280], arg5_1: f16[640], arg6_1: f16[640], arg7_1: f16[640], arg8_1: f16[640, 640, 3, 3], arg9_1: f16[640], arg10_1: f16[2, 640, 32, 32], arg11_1: f16[2, 1280]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg10_1, arg0_1, arg1_1, 2, 640, 1024, 32, 1e-05);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 640, 32, 32] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu: f16[2, 640, 32, 32] = torch.ops.aten.silu.default(getitem);  getitem = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(silu, arg2_1, arg3_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:474, code: temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
      "        silu_1: f16[2, 1280] = torch.ops.aten.silu.default(arg11_1);  arg11_1 = None\n",
      "        t: f16[1280, 640] = torch.ops.aten.t.default(arg4_1);  arg4_1 = None\n",
      "        addmm: f16[2, 640] = torch.ops.aten.addmm.default(arg5_1, silu_1, t);  arg5_1 = silu_1 = t = None\n",
      "        slice_1: f16[2, 640] = torch.ops.aten.slice.Tensor(addmm, 0, 0, 9223372036854775807);  addmm = None\n",
      "        slice_2: f16[2, 640] = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807);  slice_1 = None\n",
      "        unsqueeze: f16[2, 640, 1] = torch.ops.aten.unsqueeze.default(slice_2, 2);  slice_2 = None\n",
      "        unsqueeze_1: f16[2, 640, 1, 1] = torch.ops.aten.unsqueeze.default(unsqueeze, 3);  unsqueeze = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:477, code: hidden_states = hidden_states + temb\n",
      "        add: f16[2, 640, 32, 32] = torch.ops.aten.add.Tensor(convolution, unsqueeze_1);  convolution = unsqueeze_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_1 = torch.ops.aten.native_group_norm.default(add, arg6_1, arg7_1, 2, 640, 1024, 32, 1e-05);  add = arg6_1 = arg7_1 = None\n",
      "        getitem_3: f16[2, 640, 32, 32] = native_group_norm_1[0]\n",
      "        getitem_4: f16[2, 32] = native_group_norm_1[1]\n",
      "        getitem_5: f16[2, 32] = native_group_norm_1[2];  native_group_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_2: f16[2, 640, 32, 32] = torch.ops.aten.silu.default(getitem_3);  getitem_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_1: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(silu_2, arg8_1, arg9_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_2 = arg8_1 = arg9_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_1: f16[2, 640, 32, 32] = torch.ops.aten.add.Tensor(arg10_1, convolution_1);  arg10_1 = convolution_1 = None\n",
      "        div: f16[2, 640, 32, 32] = torch.ops.aten.div.Tensor(add_1, 1.0);  add_1 = None\n",
      "        return (div,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[640], arg1_1: f16[640], arg2_1: f16[640, 640, 1, 1], arg3_1: f16[640], arg4_1: f16[640], arg5_1: f16[640], arg6_1: f16[640, 640], arg7_1: f16[640, 640], arg8_1: f16[640, 640], arg9_1: f16[640, 640], arg10_1: f16[640], arg11_1: f16[640], arg12_1: f16[640], arg13_1: f16[640, 640], arg14_1: f16[640, 768], arg15_1: f16[640, 768], arg16_1: f16[640, 640], arg17_1: f16[640], arg18_1: f16[640], arg19_1: f16[640], arg20_1: f16[5120, 640], arg21_1: f16[5120], arg22_1: f16[640, 2560], arg23_1: f16[640], arg24_1: f16[640, 640, 1, 1], arg25_1: f16[640], arg26_1: f16[2, 640, 32, 32], arg27_1: f16[2, 77, 768]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:202, code: hidden_states = self.norm(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg26_1, arg0_1, arg1_1, 2, 640, 1024, 32, 1e-06);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 640, 32, 32] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:204, code: hidden_states = self.proj_in(hidden_states)\n",
      "        convolution: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(getitem, arg2_1, arg3_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  getitem = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:206, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * weight, inner_dim)\n",
      "        permute: f16[2, 32, 32, 640] = torch.ops.aten.permute.default(convolution, [0, 2, 3, 1]);  convolution = None\n",
      "        view: f16[2, 1024, 640] = torch.ops.aten.view.default(permute, [2, 1024, 640]);  permute = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:482, code: self.norm1(hidden_states, timestep) if self.use_ada_layer_norm else self.norm1(hidden_states)\n",
      "        native_layer_norm = torch.ops.aten.native_layer_norm.default(view, [640], arg4_1, arg5_1, 1e-05);  arg4_1 = arg5_1 = None\n",
      "        getitem_3: f16[2, 1024, 640] = native_layer_norm[0]\n",
      "        getitem_4: f32[2, 1024, 1] = native_layer_norm[1]\n",
      "        getitem_5: f32[2, 1024, 1] = native_layer_norm[2];  native_layer_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t: f16[640, 640] = torch.ops.aten.t.default(arg6_1);  arg6_1 = None\n",
      "        view_1: f16[2048, 640] = torch.ops.aten.view.default(getitem_3, [2048, 640])\n",
      "        mm: f16[2048, 640] = torch.ops.aten.mm.default(view_1, t);  view_1 = t = None\n",
      "        _unsafe_view: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(mm, [2, 1024, 640]);  mm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_2: f16[2, 1024, 8, 80] = torch.ops.aten.view.default(_unsafe_view, [2, 1024, 8, 80]);  _unsafe_view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_1: f16[2, 8, 1024, 80] = torch.ops.aten.permute.default(view_2, [0, 2, 1, 3]);  view_2 = None\n",
      "        clone: f16[2, 8, 1024, 80] = torch.ops.aten.clone.default(permute_1, memory_format = torch.contiguous_format);  permute_1 = None\n",
      "        _unsafe_view_1: f16[16, 1024, 80] = torch.ops.aten._unsafe_view.default(clone, [16, 1024, 80]);  clone = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_1: f16[640, 640] = torch.ops.aten.t.default(arg7_1);  arg7_1 = None\n",
      "        view_3: f16[2048, 640] = torch.ops.aten.view.default(getitem_3, [2048, 640])\n",
      "        mm_1: f16[2048, 640] = torch.ops.aten.mm.default(view_3, t_1);  view_3 = t_1 = None\n",
      "        _unsafe_view_2: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(mm_1, [2, 1024, 640]);  mm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_2: f16[640, 640] = torch.ops.aten.t.default(arg8_1);  arg8_1 = None\n",
      "        view_4: f16[2048, 640] = torch.ops.aten.view.default(getitem_3, [2048, 640]);  getitem_3 = None\n",
      "        mm_2: f16[2048, 640] = torch.ops.aten.mm.default(view_4, t_2);  view_4 = t_2 = None\n",
      "        _unsafe_view_3: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(mm_2, [2, 1024, 640]);  mm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_5: f16[2, 1024, 8, 80] = torch.ops.aten.view.default(_unsafe_view_2, [2, 1024, 8, 80]);  _unsafe_view_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_2: f16[2, 8, 1024, 80] = torch.ops.aten.permute.default(view_5, [0, 2, 1, 3]);  view_5 = None\n",
      "        clone_1: f16[2, 8, 1024, 80] = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None\n",
      "        _unsafe_view_4: f16[16, 1024, 80] = torch.ops.aten._unsafe_view.default(clone_1, [16, 1024, 80]);  clone_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_6: f16[2, 1024, 8, 80] = torch.ops.aten.view.default(_unsafe_view_3, [2, 1024, 8, 80]);  _unsafe_view_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_3: f16[2, 8, 1024, 80] = torch.ops.aten.permute.default(view_6, [0, 2, 1, 3]);  view_6 = None\n",
      "        clone_2: f16[2, 8, 1024, 80] = torch.ops.aten.clone.default(permute_3, memory_format = torch.contiguous_format);  permute_3 = None\n",
      "        _unsafe_view_5: f16[16, 1024, 80] = torch.ops.aten._unsafe_view.default(clone_2, [16, 1024, 80]);  clone_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty: f16[16, 1024, 1024] = torch.ops.aten.empty.memory_format([16, 1024, 1024], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose: f16[16, 80, 1024] = torch.ops.aten.transpose.int(_unsafe_view_4, -1, -2);  _unsafe_view_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm: f16[16, 1024, 1024] = torch.ops.aten.baddbmm.default(empty, _unsafe_view_1, transpose, beta = 0, alpha = 0.11180339887498948);  empty = _unsafe_view_1 = transpose = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax: f16[16, 1024, 1024] = torch.ops.aten._softmax.default(baddbmm, -1, False);  baddbmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm: f16[16, 1024, 80] = torch.ops.aten.bmm.default(_softmax, _unsafe_view_5);  _softmax = _unsafe_view_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_7: f16[2, 8, 1024, 80] = torch.ops.aten.view.default(bmm, [2, 8, 1024, 80]);  bmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_4: f16[2, 1024, 8, 80] = torch.ops.aten.permute.default(view_7, [0, 2, 1, 3]);  view_7 = None\n",
      "        clone_3: f16[2, 1024, 8, 80] = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None\n",
      "        _unsafe_view_6: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(clone_3, [2, 1024, 640]);  clone_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_3: f16[640, 640] = torch.ops.aten.t.default(arg9_1);  arg9_1 = None\n",
      "        view_8: f16[2048, 640] = torch.ops.aten.view.default(_unsafe_view_6, [2048, 640]);  _unsafe_view_6 = None\n",
      "        addmm: f16[2048, 640] = torch.ops.aten.addmm.default(arg10_1, view_8, t_3);  arg10_1 = view_8 = t_3 = None\n",
      "        view_9: f16[2, 1024, 640] = torch.ops.aten.view.default(addmm, [2, 1024, 640]);  addmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:490, code: hidden_states = self.attn1(norm_hidden_states, attention_mask=attention_mask) + hidden_states\n",
      "        add: f16[2, 1024, 640] = torch.ops.aten.add.Tensor(view_9, view);  view_9 = view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:495, code: self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)\n",
      "        native_layer_norm_1 = torch.ops.aten.native_layer_norm.default(add, [640], arg11_1, arg12_1, 1e-05);  arg11_1 = arg12_1 = None\n",
      "        getitem_6: f16[2, 1024, 640] = native_layer_norm_1[0]\n",
      "        getitem_7: f32[2, 1024, 1] = native_layer_norm_1[1]\n",
      "        getitem_8: f32[2, 1024, 1] = native_layer_norm_1[2];  native_layer_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t_4: f16[640, 640] = torch.ops.aten.t.default(arg13_1);  arg13_1 = None\n",
      "        view_10: f16[2048, 640] = torch.ops.aten.view.default(getitem_6, [2048, 640]);  getitem_6 = None\n",
      "        mm_3: f16[2048, 640] = torch.ops.aten.mm.default(view_10, t_4);  view_10 = t_4 = None\n",
      "        _unsafe_view_7: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(mm_3, [2, 1024, 640]);  mm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_11: f16[2, 1024, 8, 80] = torch.ops.aten.view.default(_unsafe_view_7, [2, 1024, 8, 80]);  _unsafe_view_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_5: f16[2, 8, 1024, 80] = torch.ops.aten.permute.default(view_11, [0, 2, 1, 3]);  view_11 = None\n",
      "        clone_4: f16[2, 8, 1024, 80] = torch.ops.aten.clone.default(permute_5, memory_format = torch.contiguous_format);  permute_5 = None\n",
      "        _unsafe_view_8: f16[16, 1024, 80] = torch.ops.aten._unsafe_view.default(clone_4, [16, 1024, 80]);  clone_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_5: f16[768, 640] = torch.ops.aten.t.default(arg14_1);  arg14_1 = None\n",
      "        view_12: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768])\n",
      "        mm_4: f16[154, 640] = torch.ops.aten.mm.default(view_12, t_5);  view_12 = t_5 = None\n",
      "        _unsafe_view_9: f16[2, 77, 640] = torch.ops.aten._unsafe_view.default(mm_4, [2, 77, 640]);  mm_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_6: f16[768, 640] = torch.ops.aten.t.default(arg15_1);  arg15_1 = None\n",
      "        view_13: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768]);  arg27_1 = None\n",
      "        mm_5: f16[154, 640] = torch.ops.aten.mm.default(view_13, t_6);  view_13 = t_6 = None\n",
      "        _unsafe_view_10: f16[2, 77, 640] = torch.ops.aten._unsafe_view.default(mm_5, [2, 77, 640]);  mm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_14: f16[2, 77, 8, 80] = torch.ops.aten.view.default(_unsafe_view_9, [2, 77, 8, 80]);  _unsafe_view_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_6: f16[2, 8, 77, 80] = torch.ops.aten.permute.default(view_14, [0, 2, 1, 3]);  view_14 = None\n",
      "        clone_5: f16[2, 8, 77, 80] = torch.ops.aten.clone.default(permute_6, memory_format = torch.contiguous_format);  permute_6 = None\n",
      "        _unsafe_view_11: f16[16, 77, 80] = torch.ops.aten._unsafe_view.default(clone_5, [16, 77, 80]);  clone_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_15: f16[2, 77, 8, 80] = torch.ops.aten.view.default(_unsafe_view_10, [2, 77, 8, 80]);  _unsafe_view_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_7: f16[2, 8, 77, 80] = torch.ops.aten.permute.default(view_15, [0, 2, 1, 3]);  view_15 = None\n",
      "        clone_6: f16[2, 8, 77, 80] = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None\n",
      "        _unsafe_view_12: f16[16, 77, 80] = torch.ops.aten._unsafe_view.default(clone_6, [16, 77, 80]);  clone_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty_1: f16[16, 1024, 77] = torch.ops.aten.empty.memory_format([16, 1024, 77], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose_1: f16[16, 80, 77] = torch.ops.aten.transpose.int(_unsafe_view_11, -1, -2);  _unsafe_view_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm_1: f16[16, 1024, 77] = torch.ops.aten.baddbmm.default(empty_1, _unsafe_view_8, transpose_1, beta = 0, alpha = 0.11180339887498948);  empty_1 = _unsafe_view_8 = transpose_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax_1: f16[16, 1024, 77] = torch.ops.aten._softmax.default(baddbmm_1, -1, False);  baddbmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm_1: f16[16, 1024, 80] = torch.ops.aten.bmm.default(_softmax_1, _unsafe_view_12);  _softmax_1 = _unsafe_view_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_16: f16[2, 8, 1024, 80] = torch.ops.aten.view.default(bmm_1, [2, 8, 1024, 80]);  bmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_8: f16[2, 1024, 8, 80] = torch.ops.aten.permute.default(view_16, [0, 2, 1, 3]);  view_16 = None\n",
      "        clone_7: f16[2, 1024, 8, 80] = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None\n",
      "        _unsafe_view_13: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(clone_7, [2, 1024, 640]);  clone_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_7: f16[640, 640] = torch.ops.aten.t.default(arg16_1);  arg16_1 = None\n",
      "        view_17: f16[2048, 640] = torch.ops.aten.view.default(_unsafe_view_13, [2048, 640]);  _unsafe_view_13 = None\n",
      "        addmm_1: f16[2048, 640] = torch.ops.aten.addmm.default(arg17_1, view_17, t_7);  arg17_1 = view_17 = t_7 = None\n",
      "        view_18: f16[2, 1024, 640] = torch.ops.aten.view.default(addmm_1, [2, 1024, 640]);  addmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:498, code: self.attn2(\n",
      "        add_1: f16[2, 1024, 640] = torch.ops.aten.add.Tensor(view_18, add);  view_18 = add = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        native_layer_norm_2 = torch.ops.aten.native_layer_norm.default(add_1, [640], arg18_1, arg19_1, 1e-05);  arg18_1 = arg19_1 = None\n",
      "        getitem_9: f16[2, 1024, 640] = native_layer_norm_2[0]\n",
      "        getitem_10: f32[2, 1024, 1] = native_layer_norm_2[1]\n",
      "        getitem_11: f32[2, 1024, 1] = native_layer_norm_2[2];  native_layer_norm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:820, code: hidden_states, gate = self.proj(hidden_states).chunk(2, dim=-1)\n",
      "        t_8: f16[640, 5120] = torch.ops.aten.t.default(arg20_1);  arg20_1 = None\n",
      "        view_19: f16[2048, 640] = torch.ops.aten.view.default(getitem_9, [2048, 640]);  getitem_9 = None\n",
      "        addmm_2: f16[2048, 5120] = torch.ops.aten.addmm.default(arg21_1, view_19, t_8);  arg21_1 = view_19 = t_8 = None\n",
      "        view_20: f16[2, 1024, 5120] = torch.ops.aten.view.default(addmm_2, [2, 1024, 5120]);  addmm_2 = None\n",
      "        split = torch.ops.aten.split.Tensor(view_20, 2560, -1);  view_20 = None\n",
      "        getitem_12: f16[2, 1024, 2560] = split[0]\n",
      "        getitem_13: f16[2, 1024, 2560] = split[1];  split = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:815, code: return F.gelu(gate)\n",
      "        gelu: f16[2, 1024, 2560] = torch.ops.aten.gelu.default(getitem_13);  getitem_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:821, code: return hidden_states * self.gelu(gate)\n",
      "        mul: f16[2, 1024, 2560] = torch.ops.aten.mul.Tensor(getitem_12, gelu);  getitem_12 = gelu = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:774, code: hidden_states = module(hidden_states)\n",
      "        t_9: f16[2560, 640] = torch.ops.aten.t.default(arg22_1);  arg22_1 = None\n",
      "        view_21: f16[2048, 2560] = torch.ops.aten.view.default(mul, [2048, 2560]);  mul = None\n",
      "        addmm_3: f16[2048, 640] = torch.ops.aten.addmm.default(arg23_1, view_21, t_9);  arg23_1 = view_21 = t_9 = None\n",
      "        view_22: f16[2, 1024, 640] = torch.ops.aten.view.default(addmm_3, [2, 1024, 640]);  addmm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        add_2: f16[2, 1024, 640] = torch.ops.aten.add.Tensor(view_22, add_1);  view_22 = add_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:222, code: hidden_states.reshape(batch, height, weight, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
      "        view_23: f16[2, 32, 32, 640] = torch.ops.aten.view.default(add_2, [2, 32, 32, 640]);  add_2 = None\n",
      "        permute_9: f16[2, 640, 32, 32] = torch.ops.aten.permute.default(view_23, [0, 3, 1, 2]);  view_23 = None\n",
      "        clone_8: f16[2, 640, 32, 32] = torch.ops.aten.clone.default(permute_9, memory_format = torch.contiguous_format);  permute_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:224, code: hidden_states = self.proj_out(hidden_states)\n",
      "        convolution_1: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(clone_8, arg24_1, arg25_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  clone_8 = arg24_1 = arg25_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:231, code: output = hidden_states + residual\n",
      "        add_3: f16[2, 640, 32, 32] = torch.ops.aten.add.Tensor(convolution_1, arg26_1);  convolution_1 = arg26_1 = None\n",
      "        return (add_3,)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[640, 640, 3, 3], arg1_1: f16[640], arg2_1: f16[2, 640, 32, 32]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:188, code: hidden_states = self.conv(hidden_states)\n",
      "        convolution: f16[2, 640, 16, 16] = torch.ops.aten.convolution.default(arg2_1, arg0_1, arg1_1, [2, 2], [1, 1], [1, 1], False, [0, 0], 1);  arg2_1 = arg0_1 = arg1_1 = None\n",
      "        return (convolution,)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[640], arg1_1: f16[640], arg2_1: f16[1280, 640, 3, 3], arg3_1: f16[1280], arg4_1: f16[1280, 1280], arg5_1: f16[1280], arg6_1: f16[1280], arg7_1: f16[1280], arg8_1: f16[1280, 1280, 3, 3], arg9_1: f16[1280], arg10_1: f16[1280, 640, 1, 1], arg11_1: f16[1280], arg12_1: f16[2, 640, 16, 16], arg13_1: f16[2, 1280]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg12_1, arg0_1, arg1_1, 2, 640, 256, 32, 1e-05);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 640, 16, 16] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu: f16[2, 640, 16, 16] = torch.ops.aten.silu.default(getitem);  getitem = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(silu, arg2_1, arg3_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:474, code: temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
      "        silu_1: f16[2, 1280] = torch.ops.aten.silu.default(arg13_1);  arg13_1 = None\n",
      "        t: f16[1280, 1280] = torch.ops.aten.t.default(arg4_1);  arg4_1 = None\n",
      "        addmm: f16[2, 1280] = torch.ops.aten.addmm.default(arg5_1, silu_1, t);  arg5_1 = silu_1 = t = None\n",
      "        slice_1: f16[2, 1280] = torch.ops.aten.slice.Tensor(addmm, 0, 0, 9223372036854775807);  addmm = None\n",
      "        slice_2: f16[2, 1280] = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807);  slice_1 = None\n",
      "        unsqueeze: f16[2, 1280, 1] = torch.ops.aten.unsqueeze.default(slice_2, 2);  slice_2 = None\n",
      "        unsqueeze_1: f16[2, 1280, 1, 1] = torch.ops.aten.unsqueeze.default(unsqueeze, 3);  unsqueeze = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:477, code: hidden_states = hidden_states + temb\n",
      "        add: f16[2, 1280, 16, 16] = torch.ops.aten.add.Tensor(convolution, unsqueeze_1);  convolution = unsqueeze_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_1 = torch.ops.aten.native_group_norm.default(add, arg6_1, arg7_1, 2, 1280, 256, 32, 1e-05);  add = arg6_1 = arg7_1 = None\n",
      "        getitem_3: f16[2, 1280, 16, 16] = native_group_norm_1[0]\n",
      "        getitem_4: f16[2, 32] = native_group_norm_1[1]\n",
      "        getitem_5: f16[2, 32] = native_group_norm_1[2];  native_group_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_2: f16[2, 1280, 16, 16] = torch.ops.aten.silu.default(getitem_3);  getitem_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_1: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(silu_2, arg8_1, arg9_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_2 = arg8_1 = arg9_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:491, code: input_tensor = self.conv_shortcut(input_tensor)\n",
      "        convolution_2: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(arg12_1, arg10_1, arg11_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  arg12_1 = arg10_1 = arg11_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_1: f16[2, 1280, 16, 16] = torch.ops.aten.add.Tensor(convolution_2, convolution_1);  convolution_2 = convolution_1 = None\n",
      "        div: f16[2, 1280, 16, 16] = torch.ops.aten.div.Tensor(add_1, 1.0);  add_1 = None\n",
      "        return (div,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1280], arg1_1: f16[1280], arg2_1: f16[1280, 1280, 1, 1], arg3_1: f16[1280], arg4_1: f16[1280], arg5_1: f16[1280], arg6_1: f16[1280, 1280], arg7_1: f16[1280, 1280], arg8_1: f16[1280, 1280], arg9_1: f16[1280, 1280], arg10_1: f16[1280], arg11_1: f16[1280], arg12_1: f16[1280], arg13_1: f16[1280, 1280], arg14_1: f16[1280, 768], arg15_1: f16[1280, 768], arg16_1: f16[1280, 1280], arg17_1: f16[1280], arg18_1: f16[1280], arg19_1: f16[1280], arg20_1: f16[10240, 1280], arg21_1: f16[10240], arg22_1: f16[1280, 5120], arg23_1: f16[1280], arg24_1: f16[1280, 1280, 1, 1], arg25_1: f16[1280], arg26_1: f16[2, 1280, 16, 16], arg27_1: f16[2, 77, 768]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:202, code: hidden_states = self.norm(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg26_1, arg0_1, arg1_1, 2, 1280, 256, 32, 1e-06);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 1280, 16, 16] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:204, code: hidden_states = self.proj_in(hidden_states)\n",
      "        convolution: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(getitem, arg2_1, arg3_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  getitem = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:206, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * weight, inner_dim)\n",
      "        permute: f16[2, 16, 16, 1280] = torch.ops.aten.permute.default(convolution, [0, 2, 3, 1]);  convolution = None\n",
      "        view: f16[2, 256, 1280] = torch.ops.aten.view.default(permute, [2, 256, 1280]);  permute = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:482, code: self.norm1(hidden_states, timestep) if self.use_ada_layer_norm else self.norm1(hidden_states)\n",
      "        native_layer_norm = torch.ops.aten.native_layer_norm.default(view, [1280], arg4_1, arg5_1, 1e-05);  arg4_1 = arg5_1 = None\n",
      "        getitem_3: f16[2, 256, 1280] = native_layer_norm[0]\n",
      "        getitem_4: f32[2, 256, 1] = native_layer_norm[1]\n",
      "        getitem_5: f32[2, 256, 1] = native_layer_norm[2];  native_layer_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t: f16[1280, 1280] = torch.ops.aten.t.default(arg6_1);  arg6_1 = None\n",
      "        view_1: f16[512, 1280] = torch.ops.aten.view.default(getitem_3, [512, 1280])\n",
      "        mm: f16[512, 1280] = torch.ops.aten.mm.default(view_1, t);  view_1 = t = None\n",
      "        _unsafe_view: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(mm, [2, 256, 1280]);  mm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_2: f16[2, 256, 8, 160] = torch.ops.aten.view.default(_unsafe_view, [2, 256, 8, 160]);  _unsafe_view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_1: f16[2, 8, 256, 160] = torch.ops.aten.permute.default(view_2, [0, 2, 1, 3]);  view_2 = None\n",
      "        clone: f16[2, 8, 256, 160] = torch.ops.aten.clone.default(permute_1, memory_format = torch.contiguous_format);  permute_1 = None\n",
      "        _unsafe_view_1: f16[16, 256, 160] = torch.ops.aten._unsafe_view.default(clone, [16, 256, 160]);  clone = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_1: f16[1280, 1280] = torch.ops.aten.t.default(arg7_1);  arg7_1 = None\n",
      "        view_3: f16[512, 1280] = torch.ops.aten.view.default(getitem_3, [512, 1280])\n",
      "        mm_1: f16[512, 1280] = torch.ops.aten.mm.default(view_3, t_1);  view_3 = t_1 = None\n",
      "        _unsafe_view_2: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(mm_1, [2, 256, 1280]);  mm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_2: f16[1280, 1280] = torch.ops.aten.t.default(arg8_1);  arg8_1 = None\n",
      "        view_4: f16[512, 1280] = torch.ops.aten.view.default(getitem_3, [512, 1280]);  getitem_3 = None\n",
      "        mm_2: f16[512, 1280] = torch.ops.aten.mm.default(view_4, t_2);  view_4 = t_2 = None\n",
      "        _unsafe_view_3: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(mm_2, [2, 256, 1280]);  mm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_5: f16[2, 256, 8, 160] = torch.ops.aten.view.default(_unsafe_view_2, [2, 256, 8, 160]);  _unsafe_view_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_2: f16[2, 8, 256, 160] = torch.ops.aten.permute.default(view_5, [0, 2, 1, 3]);  view_5 = None\n",
      "        clone_1: f16[2, 8, 256, 160] = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None\n",
      "        _unsafe_view_4: f16[16, 256, 160] = torch.ops.aten._unsafe_view.default(clone_1, [16, 256, 160]);  clone_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_6: f16[2, 256, 8, 160] = torch.ops.aten.view.default(_unsafe_view_3, [2, 256, 8, 160]);  _unsafe_view_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_3: f16[2, 8, 256, 160] = torch.ops.aten.permute.default(view_6, [0, 2, 1, 3]);  view_6 = None\n",
      "        clone_2: f16[2, 8, 256, 160] = torch.ops.aten.clone.default(permute_3, memory_format = torch.contiguous_format);  permute_3 = None\n",
      "        _unsafe_view_5: f16[16, 256, 160] = torch.ops.aten._unsafe_view.default(clone_2, [16, 256, 160]);  clone_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty: f16[16, 256, 256] = torch.ops.aten.empty.memory_format([16, 256, 256], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose: f16[16, 160, 256] = torch.ops.aten.transpose.int(_unsafe_view_4, -1, -2);  _unsafe_view_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm: f16[16, 256, 256] = torch.ops.aten.baddbmm.default(empty, _unsafe_view_1, transpose, beta = 0, alpha = 0.07905694150420949);  empty = _unsafe_view_1 = transpose = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax: f16[16, 256, 256] = torch.ops.aten._softmax.default(baddbmm, -1, False);  baddbmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm: f16[16, 256, 160] = torch.ops.aten.bmm.default(_softmax, _unsafe_view_5);  _softmax = _unsafe_view_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_7: f16[2, 8, 256, 160] = torch.ops.aten.view.default(bmm, [2, 8, 256, 160]);  bmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_4: f16[2, 256, 8, 160] = torch.ops.aten.permute.default(view_7, [0, 2, 1, 3]);  view_7 = None\n",
      "        clone_3: f16[2, 256, 8, 160] = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None\n",
      "        _unsafe_view_6: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(clone_3, [2, 256, 1280]);  clone_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_3: f16[1280, 1280] = torch.ops.aten.t.default(arg9_1);  arg9_1 = None\n",
      "        view_8: f16[512, 1280] = torch.ops.aten.view.default(_unsafe_view_6, [512, 1280]);  _unsafe_view_6 = None\n",
      "        addmm: f16[512, 1280] = torch.ops.aten.addmm.default(arg10_1, view_8, t_3);  arg10_1 = view_8 = t_3 = None\n",
      "        view_9: f16[2, 256, 1280] = torch.ops.aten.view.default(addmm, [2, 256, 1280]);  addmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:490, code: hidden_states = self.attn1(norm_hidden_states, attention_mask=attention_mask) + hidden_states\n",
      "        add: f16[2, 256, 1280] = torch.ops.aten.add.Tensor(view_9, view);  view_9 = view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:495, code: self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)\n",
      "        native_layer_norm_1 = torch.ops.aten.native_layer_norm.default(add, [1280], arg11_1, arg12_1, 1e-05);  arg11_1 = arg12_1 = None\n",
      "        getitem_6: f16[2, 256, 1280] = native_layer_norm_1[0]\n",
      "        getitem_7: f32[2, 256, 1] = native_layer_norm_1[1]\n",
      "        getitem_8: f32[2, 256, 1] = native_layer_norm_1[2];  native_layer_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t_4: f16[1280, 1280] = torch.ops.aten.t.default(arg13_1);  arg13_1 = None\n",
      "        view_10: f16[512, 1280] = torch.ops.aten.view.default(getitem_6, [512, 1280]);  getitem_6 = None\n",
      "        mm_3: f16[512, 1280] = torch.ops.aten.mm.default(view_10, t_4);  view_10 = t_4 = None\n",
      "        _unsafe_view_7: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(mm_3, [2, 256, 1280]);  mm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_11: f16[2, 256, 8, 160] = torch.ops.aten.view.default(_unsafe_view_7, [2, 256, 8, 160]);  _unsafe_view_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_5: f16[2, 8, 256, 160] = torch.ops.aten.permute.default(view_11, [0, 2, 1, 3]);  view_11 = None\n",
      "        clone_4: f16[2, 8, 256, 160] = torch.ops.aten.clone.default(permute_5, memory_format = torch.contiguous_format);  permute_5 = None\n",
      "        _unsafe_view_8: f16[16, 256, 160] = torch.ops.aten._unsafe_view.default(clone_4, [16, 256, 160]);  clone_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_5: f16[768, 1280] = torch.ops.aten.t.default(arg14_1);  arg14_1 = None\n",
      "        view_12: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768])\n",
      "        mm_4: f16[154, 1280] = torch.ops.aten.mm.default(view_12, t_5);  view_12 = t_5 = None\n",
      "        _unsafe_view_9: f16[2, 77, 1280] = torch.ops.aten._unsafe_view.default(mm_4, [2, 77, 1280]);  mm_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_6: f16[768, 1280] = torch.ops.aten.t.default(arg15_1);  arg15_1 = None\n",
      "        view_13: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768]);  arg27_1 = None\n",
      "        mm_5: f16[154, 1280] = torch.ops.aten.mm.default(view_13, t_6);  view_13 = t_6 = None\n",
      "        _unsafe_view_10: f16[2, 77, 1280] = torch.ops.aten._unsafe_view.default(mm_5, [2, 77, 1280]);  mm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_14: f16[2, 77, 8, 160] = torch.ops.aten.view.default(_unsafe_view_9, [2, 77, 8, 160]);  _unsafe_view_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_6: f16[2, 8, 77, 160] = torch.ops.aten.permute.default(view_14, [0, 2, 1, 3]);  view_14 = None\n",
      "        clone_5: f16[2, 8, 77, 160] = torch.ops.aten.clone.default(permute_6, memory_format = torch.contiguous_format);  permute_6 = None\n",
      "        _unsafe_view_11: f16[16, 77, 160] = torch.ops.aten._unsafe_view.default(clone_5, [16, 77, 160]);  clone_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_15: f16[2, 77, 8, 160] = torch.ops.aten.view.default(_unsafe_view_10, [2, 77, 8, 160]);  _unsafe_view_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_7: f16[2, 8, 77, 160] = torch.ops.aten.permute.default(view_15, [0, 2, 1, 3]);  view_15 = None\n",
      "        clone_6: f16[2, 8, 77, 160] = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None\n",
      "        _unsafe_view_12: f16[16, 77, 160] = torch.ops.aten._unsafe_view.default(clone_6, [16, 77, 160]);  clone_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty_1: f16[16, 256, 77] = torch.ops.aten.empty.memory_format([16, 256, 77], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose_1: f16[16, 160, 77] = torch.ops.aten.transpose.int(_unsafe_view_11, -1, -2);  _unsafe_view_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm_1: f16[16, 256, 77] = torch.ops.aten.baddbmm.default(empty_1, _unsafe_view_8, transpose_1, beta = 0, alpha = 0.07905694150420949);  empty_1 = _unsafe_view_8 = transpose_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax_1: f16[16, 256, 77] = torch.ops.aten._softmax.default(baddbmm_1, -1, False);  baddbmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm_1: f16[16, 256, 160] = torch.ops.aten.bmm.default(_softmax_1, _unsafe_view_12);  _softmax_1 = _unsafe_view_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_16: f16[2, 8, 256, 160] = torch.ops.aten.view.default(bmm_1, [2, 8, 256, 160]);  bmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_8: f16[2, 256, 8, 160] = torch.ops.aten.permute.default(view_16, [0, 2, 1, 3]);  view_16 = None\n",
      "        clone_7: f16[2, 256, 8, 160] = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None\n",
      "        _unsafe_view_13: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(clone_7, [2, 256, 1280]);  clone_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_7: f16[1280, 1280] = torch.ops.aten.t.default(arg16_1);  arg16_1 = None\n",
      "        view_17: f16[512, 1280] = torch.ops.aten.view.default(_unsafe_view_13, [512, 1280]);  _unsafe_view_13 = None\n",
      "        addmm_1: f16[512, 1280] = torch.ops.aten.addmm.default(arg17_1, view_17, t_7);  arg17_1 = view_17 = t_7 = None\n",
      "        view_18: f16[2, 256, 1280] = torch.ops.aten.view.default(addmm_1, [2, 256, 1280]);  addmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:498, code: self.attn2(\n",
      "        add_1: f16[2, 256, 1280] = torch.ops.aten.add.Tensor(view_18, add);  view_18 = add = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        native_layer_norm_2 = torch.ops.aten.native_layer_norm.default(add_1, [1280], arg18_1, arg19_1, 1e-05);  arg18_1 = arg19_1 = None\n",
      "        getitem_9: f16[2, 256, 1280] = native_layer_norm_2[0]\n",
      "        getitem_10: f32[2, 256, 1] = native_layer_norm_2[1]\n",
      "        getitem_11: f32[2, 256, 1] = native_layer_norm_2[2];  native_layer_norm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:820, code: hidden_states, gate = self.proj(hidden_states).chunk(2, dim=-1)\n",
      "        t_8: f16[1280, 10240] = torch.ops.aten.t.default(arg20_1);  arg20_1 = None\n",
      "        view_19: f16[512, 1280] = torch.ops.aten.view.default(getitem_9, [512, 1280]);  getitem_9 = None\n",
      "        addmm_2: f16[512, 10240] = torch.ops.aten.addmm.default(arg21_1, view_19, t_8);  arg21_1 = view_19 = t_8 = None\n",
      "        view_20: f16[2, 256, 10240] = torch.ops.aten.view.default(addmm_2, [2, 256, 10240]);  addmm_2 = None\n",
      "        split = torch.ops.aten.split.Tensor(view_20, 5120, -1);  view_20 = None\n",
      "        getitem_12: f16[2, 256, 5120] = split[0]\n",
      "        getitem_13: f16[2, 256, 5120] = split[1];  split = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:815, code: return F.gelu(gate)\n",
      "        gelu: f16[2, 256, 5120] = torch.ops.aten.gelu.default(getitem_13);  getitem_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:821, code: return hidden_states * self.gelu(gate)\n",
      "        mul: f16[2, 256, 5120] = torch.ops.aten.mul.Tensor(getitem_12, gelu);  getitem_12 = gelu = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:774, code: hidden_states = module(hidden_states)\n",
      "        t_9: f16[5120, 1280] = torch.ops.aten.t.default(arg22_1);  arg22_1 = None\n",
      "        view_21: f16[512, 5120] = torch.ops.aten.view.default(mul, [512, 5120]);  mul = None\n",
      "        addmm_3: f16[512, 1280] = torch.ops.aten.addmm.default(arg23_1, view_21, t_9);  arg23_1 = view_21 = t_9 = None\n",
      "        view_22: f16[2, 256, 1280] = torch.ops.aten.view.default(addmm_3, [2, 256, 1280]);  addmm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        add_2: f16[2, 256, 1280] = torch.ops.aten.add.Tensor(view_22, add_1);  view_22 = add_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:222, code: hidden_states.reshape(batch, height, weight, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
      "        view_23: f16[2, 16, 16, 1280] = torch.ops.aten.view.default(add_2, [2, 16, 16, 1280]);  add_2 = None\n",
      "        permute_9: f16[2, 1280, 16, 16] = torch.ops.aten.permute.default(view_23, [0, 3, 1, 2]);  view_23 = None\n",
      "        clone_8: f16[2, 1280, 16, 16] = torch.ops.aten.clone.default(permute_9, memory_format = torch.contiguous_format);  permute_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:224, code: hidden_states = self.proj_out(hidden_states)\n",
      "        convolution_1: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(clone_8, arg24_1, arg25_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  clone_8 = arg24_1 = arg25_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:231, code: output = hidden_states + residual\n",
      "        add_3: f16[2, 1280, 16, 16] = torch.ops.aten.add.Tensor(convolution_1, arg26_1);  convolution_1 = arg26_1 = None\n",
      "        return (add_3,)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1280], arg1_1: f16[1280], arg2_1: f16[1280, 1280, 3, 3], arg3_1: f16[1280], arg4_1: f16[1280, 1280], arg5_1: f16[1280], arg6_1: f16[1280], arg7_1: f16[1280], arg8_1: f16[1280, 1280, 3, 3], arg9_1: f16[1280], arg10_1: f16[2, 1280, 16, 16], arg11_1: f16[2, 1280]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg10_1, arg0_1, arg1_1, 2, 1280, 256, 32, 1e-05);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 1280, 16, 16] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu: f16[2, 1280, 16, 16] = torch.ops.aten.silu.default(getitem);  getitem = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(silu, arg2_1, arg3_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:474, code: temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
      "        silu_1: f16[2, 1280] = torch.ops.aten.silu.default(arg11_1);  arg11_1 = None\n",
      "        t: f16[1280, 1280] = torch.ops.aten.t.default(arg4_1);  arg4_1 = None\n",
      "        addmm: f16[2, 1280] = torch.ops.aten.addmm.default(arg5_1, silu_1, t);  arg5_1 = silu_1 = t = None\n",
      "        slice_1: f16[2, 1280] = torch.ops.aten.slice.Tensor(addmm, 0, 0, 9223372036854775807);  addmm = None\n",
      "        slice_2: f16[2, 1280] = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807);  slice_1 = None\n",
      "        unsqueeze: f16[2, 1280, 1] = torch.ops.aten.unsqueeze.default(slice_2, 2);  slice_2 = None\n",
      "        unsqueeze_1: f16[2, 1280, 1, 1] = torch.ops.aten.unsqueeze.default(unsqueeze, 3);  unsqueeze = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:477, code: hidden_states = hidden_states + temb\n",
      "        add: f16[2, 1280, 16, 16] = torch.ops.aten.add.Tensor(convolution, unsqueeze_1);  convolution = unsqueeze_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_1 = torch.ops.aten.native_group_norm.default(add, arg6_1, arg7_1, 2, 1280, 256, 32, 1e-05);  add = arg6_1 = arg7_1 = None\n",
      "        getitem_3: f16[2, 1280, 16, 16] = native_group_norm_1[0]\n",
      "        getitem_4: f16[2, 32] = native_group_norm_1[1]\n",
      "        getitem_5: f16[2, 32] = native_group_norm_1[2];  native_group_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_2: f16[2, 1280, 16, 16] = torch.ops.aten.silu.default(getitem_3);  getitem_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_1: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(silu_2, arg8_1, arg9_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_2 = arg8_1 = arg9_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_1: f16[2, 1280, 16, 16] = torch.ops.aten.add.Tensor(arg10_1, convolution_1);  arg10_1 = convolution_1 = None\n",
      "        div: f16[2, 1280, 16, 16] = torch.ops.aten.div.Tensor(add_1, 1.0);  add_1 = None\n",
      "        return (div,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1280], arg1_1: f16[1280], arg2_1: f16[1280, 1280, 1, 1], arg3_1: f16[1280], arg4_1: f16[1280], arg5_1: f16[1280], arg6_1: f16[1280, 1280], arg7_1: f16[1280, 1280], arg8_1: f16[1280, 1280], arg9_1: f16[1280, 1280], arg10_1: f16[1280], arg11_1: f16[1280], arg12_1: f16[1280], arg13_1: f16[1280, 1280], arg14_1: f16[1280, 768], arg15_1: f16[1280, 768], arg16_1: f16[1280, 1280], arg17_1: f16[1280], arg18_1: f16[1280], arg19_1: f16[1280], arg20_1: f16[10240, 1280], arg21_1: f16[10240], arg22_1: f16[1280, 5120], arg23_1: f16[1280], arg24_1: f16[1280, 1280, 1, 1], arg25_1: f16[1280], arg26_1: f16[2, 1280, 16, 16], arg27_1: f16[2, 77, 768]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:202, code: hidden_states = self.norm(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg26_1, arg0_1, arg1_1, 2, 1280, 256, 32, 1e-06);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 1280, 16, 16] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:204, code: hidden_states = self.proj_in(hidden_states)\n",
      "        convolution: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(getitem, arg2_1, arg3_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  getitem = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:206, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * weight, inner_dim)\n",
      "        permute: f16[2, 16, 16, 1280] = torch.ops.aten.permute.default(convolution, [0, 2, 3, 1]);  convolution = None\n",
      "        view: f16[2, 256, 1280] = torch.ops.aten.view.default(permute, [2, 256, 1280]);  permute = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:482, code: self.norm1(hidden_states, timestep) if self.use_ada_layer_norm else self.norm1(hidden_states)\n",
      "        native_layer_norm = torch.ops.aten.native_layer_norm.default(view, [1280], arg4_1, arg5_1, 1e-05);  arg4_1 = arg5_1 = None\n",
      "        getitem_3: f16[2, 256, 1280] = native_layer_norm[0]\n",
      "        getitem_4: f32[2, 256, 1] = native_layer_norm[1]\n",
      "        getitem_5: f32[2, 256, 1] = native_layer_norm[2];  native_layer_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t: f16[1280, 1280] = torch.ops.aten.t.default(arg6_1);  arg6_1 = None\n",
      "        view_1: f16[512, 1280] = torch.ops.aten.view.default(getitem_3, [512, 1280])\n",
      "        mm: f16[512, 1280] = torch.ops.aten.mm.default(view_1, t);  view_1 = t = None\n",
      "        _unsafe_view: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(mm, [2, 256, 1280]);  mm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_2: f16[2, 256, 8, 160] = torch.ops.aten.view.default(_unsafe_view, [2, 256, 8, 160]);  _unsafe_view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_1: f16[2, 8, 256, 160] = torch.ops.aten.permute.default(view_2, [0, 2, 1, 3]);  view_2 = None\n",
      "        clone: f16[2, 8, 256, 160] = torch.ops.aten.clone.default(permute_1, memory_format = torch.contiguous_format);  permute_1 = None\n",
      "        _unsafe_view_1: f16[16, 256, 160] = torch.ops.aten._unsafe_view.default(clone, [16, 256, 160]);  clone = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_1: f16[1280, 1280] = torch.ops.aten.t.default(arg7_1);  arg7_1 = None\n",
      "        view_3: f16[512, 1280] = torch.ops.aten.view.default(getitem_3, [512, 1280])\n",
      "        mm_1: f16[512, 1280] = torch.ops.aten.mm.default(view_3, t_1);  view_3 = t_1 = None\n",
      "        _unsafe_view_2: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(mm_1, [2, 256, 1280]);  mm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_2: f16[1280, 1280] = torch.ops.aten.t.default(arg8_1);  arg8_1 = None\n",
      "        view_4: f16[512, 1280] = torch.ops.aten.view.default(getitem_3, [512, 1280]);  getitem_3 = None\n",
      "        mm_2: f16[512, 1280] = torch.ops.aten.mm.default(view_4, t_2);  view_4 = t_2 = None\n",
      "        _unsafe_view_3: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(mm_2, [2, 256, 1280]);  mm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_5: f16[2, 256, 8, 160] = torch.ops.aten.view.default(_unsafe_view_2, [2, 256, 8, 160]);  _unsafe_view_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_2: f16[2, 8, 256, 160] = torch.ops.aten.permute.default(view_5, [0, 2, 1, 3]);  view_5 = None\n",
      "        clone_1: f16[2, 8, 256, 160] = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None\n",
      "        _unsafe_view_4: f16[16, 256, 160] = torch.ops.aten._unsafe_view.default(clone_1, [16, 256, 160]);  clone_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_6: f16[2, 256, 8, 160] = torch.ops.aten.view.default(_unsafe_view_3, [2, 256, 8, 160]);  _unsafe_view_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_3: f16[2, 8, 256, 160] = torch.ops.aten.permute.default(view_6, [0, 2, 1, 3]);  view_6 = None\n",
      "        clone_2: f16[2, 8, 256, 160] = torch.ops.aten.clone.default(permute_3, memory_format = torch.contiguous_format);  permute_3 = None\n",
      "        _unsafe_view_5: f16[16, 256, 160] = torch.ops.aten._unsafe_view.default(clone_2, [16, 256, 160]);  clone_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty: f16[16, 256, 256] = torch.ops.aten.empty.memory_format([16, 256, 256], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose: f16[16, 160, 256] = torch.ops.aten.transpose.int(_unsafe_view_4, -1, -2);  _unsafe_view_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm: f16[16, 256, 256] = torch.ops.aten.baddbmm.default(empty, _unsafe_view_1, transpose, beta = 0, alpha = 0.07905694150420949);  empty = _unsafe_view_1 = transpose = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax: f16[16, 256, 256] = torch.ops.aten._softmax.default(baddbmm, -1, False);  baddbmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm: f16[16, 256, 160] = torch.ops.aten.bmm.default(_softmax, _unsafe_view_5);  _softmax = _unsafe_view_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_7: f16[2, 8, 256, 160] = torch.ops.aten.view.default(bmm, [2, 8, 256, 160]);  bmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_4: f16[2, 256, 8, 160] = torch.ops.aten.permute.default(view_7, [0, 2, 1, 3]);  view_7 = None\n",
      "        clone_3: f16[2, 256, 8, 160] = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None\n",
      "        _unsafe_view_6: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(clone_3, [2, 256, 1280]);  clone_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_3: f16[1280, 1280] = torch.ops.aten.t.default(arg9_1);  arg9_1 = None\n",
      "        view_8: f16[512, 1280] = torch.ops.aten.view.default(_unsafe_view_6, [512, 1280]);  _unsafe_view_6 = None\n",
      "        addmm: f16[512, 1280] = torch.ops.aten.addmm.default(arg10_1, view_8, t_3);  arg10_1 = view_8 = t_3 = None\n",
      "        view_9: f16[2, 256, 1280] = torch.ops.aten.view.default(addmm, [2, 256, 1280]);  addmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:490, code: hidden_states = self.attn1(norm_hidden_states, attention_mask=attention_mask) + hidden_states\n",
      "        add: f16[2, 256, 1280] = torch.ops.aten.add.Tensor(view_9, view);  view_9 = view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:495, code: self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)\n",
      "        native_layer_norm_1 = torch.ops.aten.native_layer_norm.default(add, [1280], arg11_1, arg12_1, 1e-05);  arg11_1 = arg12_1 = None\n",
      "        getitem_6: f16[2, 256, 1280] = native_layer_norm_1[0]\n",
      "        getitem_7: f32[2, 256, 1] = native_layer_norm_1[1]\n",
      "        getitem_8: f32[2, 256, 1] = native_layer_norm_1[2];  native_layer_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t_4: f16[1280, 1280] = torch.ops.aten.t.default(arg13_1);  arg13_1 = None\n",
      "        view_10: f16[512, 1280] = torch.ops.aten.view.default(getitem_6, [512, 1280]);  getitem_6 = None\n",
      "        mm_3: f16[512, 1280] = torch.ops.aten.mm.default(view_10, t_4);  view_10 = t_4 = None\n",
      "        _unsafe_view_7: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(mm_3, [2, 256, 1280]);  mm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_11: f16[2, 256, 8, 160] = torch.ops.aten.view.default(_unsafe_view_7, [2, 256, 8, 160]);  _unsafe_view_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_5: f16[2, 8, 256, 160] = torch.ops.aten.permute.default(view_11, [0, 2, 1, 3]);  view_11 = None\n",
      "        clone_4: f16[2, 8, 256, 160] = torch.ops.aten.clone.default(permute_5, memory_format = torch.contiguous_format);  permute_5 = None\n",
      "        _unsafe_view_8: f16[16, 256, 160] = torch.ops.aten._unsafe_view.default(clone_4, [16, 256, 160]);  clone_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_5: f16[768, 1280] = torch.ops.aten.t.default(arg14_1);  arg14_1 = None\n",
      "        view_12: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768])\n",
      "        mm_4: f16[154, 1280] = torch.ops.aten.mm.default(view_12, t_5);  view_12 = t_5 = None\n",
      "        _unsafe_view_9: f16[2, 77, 1280] = torch.ops.aten._unsafe_view.default(mm_4, [2, 77, 1280]);  mm_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_6: f16[768, 1280] = torch.ops.aten.t.default(arg15_1);  arg15_1 = None\n",
      "        view_13: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768]);  arg27_1 = None\n",
      "        mm_5: f16[154, 1280] = torch.ops.aten.mm.default(view_13, t_6);  view_13 = t_6 = None\n",
      "        _unsafe_view_10: f16[2, 77, 1280] = torch.ops.aten._unsafe_view.default(mm_5, [2, 77, 1280]);  mm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_14: f16[2, 77, 8, 160] = torch.ops.aten.view.default(_unsafe_view_9, [2, 77, 8, 160]);  _unsafe_view_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_6: f16[2, 8, 77, 160] = torch.ops.aten.permute.default(view_14, [0, 2, 1, 3]);  view_14 = None\n",
      "        clone_5: f16[2, 8, 77, 160] = torch.ops.aten.clone.default(permute_6, memory_format = torch.contiguous_format);  permute_6 = None\n",
      "        _unsafe_view_11: f16[16, 77, 160] = torch.ops.aten._unsafe_view.default(clone_5, [16, 77, 160]);  clone_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_15: f16[2, 77, 8, 160] = torch.ops.aten.view.default(_unsafe_view_10, [2, 77, 8, 160]);  _unsafe_view_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_7: f16[2, 8, 77, 160] = torch.ops.aten.permute.default(view_15, [0, 2, 1, 3]);  view_15 = None\n",
      "        clone_6: f16[2, 8, 77, 160] = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None\n",
      "        _unsafe_view_12: f16[16, 77, 160] = torch.ops.aten._unsafe_view.default(clone_6, [16, 77, 160]);  clone_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty_1: f16[16, 256, 77] = torch.ops.aten.empty.memory_format([16, 256, 77], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose_1: f16[16, 160, 77] = torch.ops.aten.transpose.int(_unsafe_view_11, -1, -2);  _unsafe_view_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm_1: f16[16, 256, 77] = torch.ops.aten.baddbmm.default(empty_1, _unsafe_view_8, transpose_1, beta = 0, alpha = 0.07905694150420949);  empty_1 = _unsafe_view_8 = transpose_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax_1: f16[16, 256, 77] = torch.ops.aten._softmax.default(baddbmm_1, -1, False);  baddbmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm_1: f16[16, 256, 160] = torch.ops.aten.bmm.default(_softmax_1, _unsafe_view_12);  _softmax_1 = _unsafe_view_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_16: f16[2, 8, 256, 160] = torch.ops.aten.view.default(bmm_1, [2, 8, 256, 160]);  bmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_8: f16[2, 256, 8, 160] = torch.ops.aten.permute.default(view_16, [0, 2, 1, 3]);  view_16 = None\n",
      "        clone_7: f16[2, 256, 8, 160] = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None\n",
      "        _unsafe_view_13: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(clone_7, [2, 256, 1280]);  clone_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_7: f16[1280, 1280] = torch.ops.aten.t.default(arg16_1);  arg16_1 = None\n",
      "        view_17: f16[512, 1280] = torch.ops.aten.view.default(_unsafe_view_13, [512, 1280]);  _unsafe_view_13 = None\n",
      "        addmm_1: f16[512, 1280] = torch.ops.aten.addmm.default(arg17_1, view_17, t_7);  arg17_1 = view_17 = t_7 = None\n",
      "        view_18: f16[2, 256, 1280] = torch.ops.aten.view.default(addmm_1, [2, 256, 1280]);  addmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:498, code: self.attn2(\n",
      "        add_1: f16[2, 256, 1280] = torch.ops.aten.add.Tensor(view_18, add);  view_18 = add = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        native_layer_norm_2 = torch.ops.aten.native_layer_norm.default(add_1, [1280], arg18_1, arg19_1, 1e-05);  arg18_1 = arg19_1 = None\n",
      "        getitem_9: f16[2, 256, 1280] = native_layer_norm_2[0]\n",
      "        getitem_10: f32[2, 256, 1] = native_layer_norm_2[1]\n",
      "        getitem_11: f32[2, 256, 1] = native_layer_norm_2[2];  native_layer_norm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:820, code: hidden_states, gate = self.proj(hidden_states).chunk(2, dim=-1)\n",
      "        t_8: f16[1280, 10240] = torch.ops.aten.t.default(arg20_1);  arg20_1 = None\n",
      "        view_19: f16[512, 1280] = torch.ops.aten.view.default(getitem_9, [512, 1280]);  getitem_9 = None\n",
      "        addmm_2: f16[512, 10240] = torch.ops.aten.addmm.default(arg21_1, view_19, t_8);  arg21_1 = view_19 = t_8 = None\n",
      "        view_20: f16[2, 256, 10240] = torch.ops.aten.view.default(addmm_2, [2, 256, 10240]);  addmm_2 = None\n",
      "        split = torch.ops.aten.split.Tensor(view_20, 5120, -1);  view_20 = None\n",
      "        getitem_12: f16[2, 256, 5120] = split[0]\n",
      "        getitem_13: f16[2, 256, 5120] = split[1];  split = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:815, code: return F.gelu(gate)\n",
      "        gelu: f16[2, 256, 5120] = torch.ops.aten.gelu.default(getitem_13);  getitem_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:821, code: return hidden_states * self.gelu(gate)\n",
      "        mul: f16[2, 256, 5120] = torch.ops.aten.mul.Tensor(getitem_12, gelu);  getitem_12 = gelu = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:774, code: hidden_states = module(hidden_states)\n",
      "        t_9: f16[5120, 1280] = torch.ops.aten.t.default(arg22_1);  arg22_1 = None\n",
      "        view_21: f16[512, 5120] = torch.ops.aten.view.default(mul, [512, 5120]);  mul = None\n",
      "        addmm_3: f16[512, 1280] = torch.ops.aten.addmm.default(arg23_1, view_21, t_9);  arg23_1 = view_21 = t_9 = None\n",
      "        view_22: f16[2, 256, 1280] = torch.ops.aten.view.default(addmm_3, [2, 256, 1280]);  addmm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        add_2: f16[2, 256, 1280] = torch.ops.aten.add.Tensor(view_22, add_1);  view_22 = add_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:222, code: hidden_states.reshape(batch, height, weight, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
      "        view_23: f16[2, 16, 16, 1280] = torch.ops.aten.view.default(add_2, [2, 16, 16, 1280]);  add_2 = None\n",
      "        permute_9: f16[2, 1280, 16, 16] = torch.ops.aten.permute.default(view_23, [0, 3, 1, 2]);  view_23 = None\n",
      "        clone_8: f16[2, 1280, 16, 16] = torch.ops.aten.clone.default(permute_9, memory_format = torch.contiguous_format);  permute_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:224, code: hidden_states = self.proj_out(hidden_states)\n",
      "        convolution_1: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(clone_8, arg24_1, arg25_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  clone_8 = arg24_1 = arg25_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:231, code: output = hidden_states + residual\n",
      "        add_3: f16[2, 1280, 16, 16] = torch.ops.aten.add.Tensor(convolution_1, arg26_1);  convolution_1 = arg26_1 = None\n",
      "        return (add_3,)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1280, 1280, 3, 3], arg1_1: f16[1280], arg2_1: f16[2, 1280, 16, 16]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:188, code: hidden_states = self.conv(hidden_states)\n",
      "        convolution: f16[2, 1280, 8, 8] = torch.ops.aten.convolution.default(arg2_1, arg0_1, arg1_1, [2, 2], [1, 1], [1, 1], False, [0, 0], 1);  arg2_1 = arg0_1 = arg1_1 = None\n",
      "        return (convolution,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1280], arg1_1: f16[1280], arg2_1: f16[1280, 1280, 3, 3], arg3_1: f16[1280], arg4_1: f16[1280, 1280], arg5_1: f16[1280], arg6_1: f16[1280], arg7_1: f16[1280], arg8_1: f16[1280, 1280, 3, 3], arg9_1: f16[1280], arg10_1: f16[1280], arg11_1: f16[1280], arg12_1: f16[1280, 1280, 3, 3], arg13_1: f16[1280], arg14_1: f16[1280, 1280], arg15_1: f16[1280], arg16_1: f16[1280], arg17_1: f16[1280], arg18_1: f16[1280, 1280, 3, 3], arg19_1: f16[1280], arg20_1: f16[2, 1280, 8, 8], arg21_1: f16[2, 1280]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg20_1, arg0_1, arg1_1, 2, 1280, 64, 32, 1e-05);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 1280, 8, 8] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu: f16[2, 1280, 8, 8] = torch.ops.aten.silu.default(getitem);  getitem = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution: f16[2, 1280, 8, 8] = torch.ops.aten.convolution.default(silu, arg2_1, arg3_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:474, code: temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
      "        silu_1: f16[2, 1280] = torch.ops.aten.silu.default(arg21_1)\n",
      "        t: f16[1280, 1280] = torch.ops.aten.t.default(arg4_1);  arg4_1 = None\n",
      "        addmm: f16[2, 1280] = torch.ops.aten.addmm.default(arg5_1, silu_1, t);  arg5_1 = silu_1 = t = None\n",
      "        slice_1: f16[2, 1280] = torch.ops.aten.slice.Tensor(addmm, 0, 0, 9223372036854775807);  addmm = None\n",
      "        slice_2: f16[2, 1280] = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807);  slice_1 = None\n",
      "        unsqueeze: f16[2, 1280, 1] = torch.ops.aten.unsqueeze.default(slice_2, 2);  slice_2 = None\n",
      "        unsqueeze_1: f16[2, 1280, 1, 1] = torch.ops.aten.unsqueeze.default(unsqueeze, 3);  unsqueeze = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:477, code: hidden_states = hidden_states + temb\n",
      "        add: f16[2, 1280, 8, 8] = torch.ops.aten.add.Tensor(convolution, unsqueeze_1);  convolution = unsqueeze_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_1 = torch.ops.aten.native_group_norm.default(add, arg6_1, arg7_1, 2, 1280, 64, 32, 1e-05);  add = arg6_1 = arg7_1 = None\n",
      "        getitem_3: f16[2, 1280, 8, 8] = native_group_norm_1[0]\n",
      "        getitem_4: f16[2, 32] = native_group_norm_1[1]\n",
      "        getitem_5: f16[2, 32] = native_group_norm_1[2];  native_group_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_2: f16[2, 1280, 8, 8] = torch.ops.aten.silu.default(getitem_3);  getitem_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_1: f16[2, 1280, 8, 8] = torch.ops.aten.convolution.default(silu_2, arg8_1, arg9_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_2 = arg8_1 = arg9_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_1: f16[2, 1280, 8, 8] = torch.ops.aten.add.Tensor(arg20_1, convolution_1);  arg20_1 = convolution_1 = None\n",
      "        div: f16[2, 1280, 8, 8] = torch.ops.aten.div.Tensor(add_1, 1.0);  add_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm_2 = torch.ops.aten.native_group_norm.default(div, arg10_1, arg11_1, 2, 1280, 64, 32, 1e-05);  arg10_1 = arg11_1 = None\n",
      "        getitem_6: f16[2, 1280, 8, 8] = native_group_norm_2[0]\n",
      "        getitem_7: f16[2, 32] = native_group_norm_2[1]\n",
      "        getitem_8: f16[2, 32] = native_group_norm_2[2];  native_group_norm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_3: f16[2, 1280, 8, 8] = torch.ops.aten.silu.default(getitem_6);  getitem_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution_2: f16[2, 1280, 8, 8] = torch.ops.aten.convolution.default(silu_3, arg12_1, arg13_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_3 = arg12_1 = arg13_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:474, code: temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
      "        silu_4: f16[2, 1280] = torch.ops.aten.silu.default(arg21_1);  arg21_1 = None\n",
      "        t_1: f16[1280, 1280] = torch.ops.aten.t.default(arg14_1);  arg14_1 = None\n",
      "        addmm_1: f16[2, 1280] = torch.ops.aten.addmm.default(arg15_1, silu_4, t_1);  arg15_1 = silu_4 = t_1 = None\n",
      "        slice_3: f16[2, 1280] = torch.ops.aten.slice.Tensor(addmm_1, 0, 0, 9223372036854775807);  addmm_1 = None\n",
      "        slice_4: f16[2, 1280] = torch.ops.aten.slice.Tensor(slice_3, 1, 0, 9223372036854775807);  slice_3 = None\n",
      "        unsqueeze_2: f16[2, 1280, 1] = torch.ops.aten.unsqueeze.default(slice_4, 2);  slice_4 = None\n",
      "        unsqueeze_3: f16[2, 1280, 1, 1] = torch.ops.aten.unsqueeze.default(unsqueeze_2, 3);  unsqueeze_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:477, code: hidden_states = hidden_states + temb\n",
      "        add_2: f16[2, 1280, 8, 8] = torch.ops.aten.add.Tensor(convolution_2, unsqueeze_3);  convolution_2 = unsqueeze_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_3 = torch.ops.aten.native_group_norm.default(add_2, arg16_1, arg17_1, 2, 1280, 64, 32, 1e-05);  add_2 = arg16_1 = arg17_1 = None\n",
      "        getitem_9: f16[2, 1280, 8, 8] = native_group_norm_3[0]\n",
      "        getitem_10: f16[2, 32] = native_group_norm_3[1]\n",
      "        getitem_11: f16[2, 32] = native_group_norm_3[2];  native_group_norm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_5: f16[2, 1280, 8, 8] = torch.ops.aten.silu.default(getitem_9);  getitem_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_3: f16[2, 1280, 8, 8] = torch.ops.aten.convolution.default(silu_5, arg18_1, arg19_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_5 = arg18_1 = arg19_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_3: f16[2, 1280, 8, 8] = torch.ops.aten.add.Tensor(div, convolution_3);  convolution_3 = None\n",
      "        div_1: f16[2, 1280, 8, 8] = torch.ops.aten.div.Tensor(add_3, 1.0);  add_3 = None\n",
      "        return (div_1, div)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1280], arg1_1: f16[1280], arg2_1: f16[1280, 1280, 3, 3], arg3_1: f16[1280], arg4_1: f16[1280, 1280], arg5_1: f16[1280], arg6_1: f16[1280], arg7_1: f16[1280], arg8_1: f16[1280, 1280, 3, 3], arg9_1: f16[1280], arg10_1: f16[2, 1280, 8, 8], arg11_1: f16[2, 1280]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg10_1, arg0_1, arg1_1, 2, 1280, 64, 32, 1e-05);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 1280, 8, 8] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu: f16[2, 1280, 8, 8] = torch.ops.aten.silu.default(getitem);  getitem = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution: f16[2, 1280, 8, 8] = torch.ops.aten.convolution.default(silu, arg2_1, arg3_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:474, code: temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
      "        silu_1: f16[2, 1280] = torch.ops.aten.silu.default(arg11_1);  arg11_1 = None\n",
      "        t: f16[1280, 1280] = torch.ops.aten.t.default(arg4_1);  arg4_1 = None\n",
      "        addmm: f16[2, 1280] = torch.ops.aten.addmm.default(arg5_1, silu_1, t);  arg5_1 = silu_1 = t = None\n",
      "        slice_1: f16[2, 1280] = torch.ops.aten.slice.Tensor(addmm, 0, 0, 9223372036854775807);  addmm = None\n",
      "        slice_2: f16[2, 1280] = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807);  slice_1 = None\n",
      "        unsqueeze: f16[2, 1280, 1] = torch.ops.aten.unsqueeze.default(slice_2, 2);  slice_2 = None\n",
      "        unsqueeze_1: f16[2, 1280, 1, 1] = torch.ops.aten.unsqueeze.default(unsqueeze, 3);  unsqueeze = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:477, code: hidden_states = hidden_states + temb\n",
      "        add: f16[2, 1280, 8, 8] = torch.ops.aten.add.Tensor(convolution, unsqueeze_1);  convolution = unsqueeze_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_1 = torch.ops.aten.native_group_norm.default(add, arg6_1, arg7_1, 2, 1280, 64, 32, 1e-05);  add = arg6_1 = arg7_1 = None\n",
      "        getitem_3: f16[2, 1280, 8, 8] = native_group_norm_1[0]\n",
      "        getitem_4: f16[2, 32] = native_group_norm_1[1]\n",
      "        getitem_5: f16[2, 32] = native_group_norm_1[2];  native_group_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_2: f16[2, 1280, 8, 8] = torch.ops.aten.silu.default(getitem_3);  getitem_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_1: f16[2, 1280, 8, 8] = torch.ops.aten.convolution.default(silu_2, arg8_1, arg9_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_2 = arg8_1 = arg9_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_1: f16[2, 1280, 8, 8] = torch.ops.aten.add.Tensor(arg10_1, convolution_1);  arg10_1 = convolution_1 = None\n",
      "        div: f16[2, 1280, 8, 8] = torch.ops.aten.div.Tensor(add_1, 1);  add_1 = None\n",
      "        return (div,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1280], arg1_1: f16[1280], arg2_1: f16[1280, 1280, 1, 1], arg3_1: f16[1280], arg4_1: f16[1280], arg5_1: f16[1280], arg6_1: f16[1280, 1280], arg7_1: f16[1280, 1280], arg8_1: f16[1280, 1280], arg9_1: f16[1280, 1280], arg10_1: f16[1280], arg11_1: f16[1280], arg12_1: f16[1280], arg13_1: f16[1280, 1280], arg14_1: f16[1280, 768], arg15_1: f16[1280, 768], arg16_1: f16[1280, 1280], arg17_1: f16[1280], arg18_1: f16[1280], arg19_1: f16[1280], arg20_1: f16[10240, 1280], arg21_1: f16[10240], arg22_1: f16[1280, 5120], arg23_1: f16[1280], arg24_1: f16[1280, 1280, 1, 1], arg25_1: f16[1280], arg26_1: f16[2, 1280, 8, 8], arg27_1: f16[2, 77, 768]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:202, code: hidden_states = self.norm(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg26_1, arg0_1, arg1_1, 2, 1280, 64, 32, 1e-06);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 1280, 8, 8] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:204, code: hidden_states = self.proj_in(hidden_states)\n",
      "        convolution: f16[2, 1280, 8, 8] = torch.ops.aten.convolution.default(getitem, arg2_1, arg3_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  getitem = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:206, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * weight, inner_dim)\n",
      "        permute: f16[2, 8, 8, 1280] = torch.ops.aten.permute.default(convolution, [0, 2, 3, 1]);  convolution = None\n",
      "        view: f16[2, 64, 1280] = torch.ops.aten.view.default(permute, [2, 64, 1280]);  permute = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:482, code: self.norm1(hidden_states, timestep) if self.use_ada_layer_norm else self.norm1(hidden_states)\n",
      "        native_layer_norm = torch.ops.aten.native_layer_norm.default(view, [1280], arg4_1, arg5_1, 1e-05);  arg4_1 = arg5_1 = None\n",
      "        getitem_3: f16[2, 64, 1280] = native_layer_norm[0]\n",
      "        getitem_4: f32[2, 64, 1] = native_layer_norm[1]\n",
      "        getitem_5: f32[2, 64, 1] = native_layer_norm[2];  native_layer_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t: f16[1280, 1280] = torch.ops.aten.t.default(arg6_1);  arg6_1 = None\n",
      "        view_1: f16[128, 1280] = torch.ops.aten.view.default(getitem_3, [128, 1280])\n",
      "        mm: f16[128, 1280] = torch.ops.aten.mm.default(view_1, t);  view_1 = t = None\n",
      "        _unsafe_view: f16[2, 64, 1280] = torch.ops.aten._unsafe_view.default(mm, [2, 64, 1280]);  mm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_2: f16[2, 64, 8, 160] = torch.ops.aten.view.default(_unsafe_view, [2, 64, 8, 160]);  _unsafe_view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_1: f16[2, 8, 64, 160] = torch.ops.aten.permute.default(view_2, [0, 2, 1, 3]);  view_2 = None\n",
      "        clone: f16[2, 8, 64, 160] = torch.ops.aten.clone.default(permute_1, memory_format = torch.contiguous_format);  permute_1 = None\n",
      "        _unsafe_view_1: f16[16, 64, 160] = torch.ops.aten._unsafe_view.default(clone, [16, 64, 160]);  clone = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_1: f16[1280, 1280] = torch.ops.aten.t.default(arg7_1);  arg7_1 = None\n",
      "        view_3: f16[128, 1280] = torch.ops.aten.view.default(getitem_3, [128, 1280])\n",
      "        mm_1: f16[128, 1280] = torch.ops.aten.mm.default(view_3, t_1);  view_3 = t_1 = None\n",
      "        _unsafe_view_2: f16[2, 64, 1280] = torch.ops.aten._unsafe_view.default(mm_1, [2, 64, 1280]);  mm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_2: f16[1280, 1280] = torch.ops.aten.t.default(arg8_1);  arg8_1 = None\n",
      "        view_4: f16[128, 1280] = torch.ops.aten.view.default(getitem_3, [128, 1280]);  getitem_3 = None\n",
      "        mm_2: f16[128, 1280] = torch.ops.aten.mm.default(view_4, t_2);  view_4 = t_2 = None\n",
      "        _unsafe_view_3: f16[2, 64, 1280] = torch.ops.aten._unsafe_view.default(mm_2, [2, 64, 1280]);  mm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_5: f16[2, 64, 8, 160] = torch.ops.aten.view.default(_unsafe_view_2, [2, 64, 8, 160]);  _unsafe_view_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_2: f16[2, 8, 64, 160] = torch.ops.aten.permute.default(view_5, [0, 2, 1, 3]);  view_5 = None\n",
      "        clone_1: f16[2, 8, 64, 160] = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None\n",
      "        _unsafe_view_4: f16[16, 64, 160] = torch.ops.aten._unsafe_view.default(clone_1, [16, 64, 160]);  clone_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_6: f16[2, 64, 8, 160] = torch.ops.aten.view.default(_unsafe_view_3, [2, 64, 8, 160]);  _unsafe_view_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_3: f16[2, 8, 64, 160] = torch.ops.aten.permute.default(view_6, [0, 2, 1, 3]);  view_6 = None\n",
      "        clone_2: f16[2, 8, 64, 160] = torch.ops.aten.clone.default(permute_3, memory_format = torch.contiguous_format);  permute_3 = None\n",
      "        _unsafe_view_5: f16[16, 64, 160] = torch.ops.aten._unsafe_view.default(clone_2, [16, 64, 160]);  clone_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty: f16[16, 64, 64] = torch.ops.aten.empty.memory_format([16, 64, 64], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose: f16[16, 160, 64] = torch.ops.aten.transpose.int(_unsafe_view_4, -1, -2);  _unsafe_view_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm: f16[16, 64, 64] = torch.ops.aten.baddbmm.default(empty, _unsafe_view_1, transpose, beta = 0, alpha = 0.07905694150420949);  empty = _unsafe_view_1 = transpose = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax: f16[16, 64, 64] = torch.ops.aten._softmax.default(baddbmm, -1, False);  baddbmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm: f16[16, 64, 160] = torch.ops.aten.bmm.default(_softmax, _unsafe_view_5);  _softmax = _unsafe_view_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_7: f16[2, 8, 64, 160] = torch.ops.aten.view.default(bmm, [2, 8, 64, 160]);  bmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_4: f16[2, 64, 8, 160] = torch.ops.aten.permute.default(view_7, [0, 2, 1, 3]);  view_7 = None\n",
      "        clone_3: f16[2, 64, 8, 160] = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None\n",
      "        _unsafe_view_6: f16[2, 64, 1280] = torch.ops.aten._unsafe_view.default(clone_3, [2, 64, 1280]);  clone_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_3: f16[1280, 1280] = torch.ops.aten.t.default(arg9_1);  arg9_1 = None\n",
      "        view_8: f16[128, 1280] = torch.ops.aten.view.default(_unsafe_view_6, [128, 1280]);  _unsafe_view_6 = None\n",
      "        addmm: f16[128, 1280] = torch.ops.aten.addmm.default(arg10_1, view_8, t_3);  arg10_1 = view_8 = t_3 = None\n",
      "        view_9: f16[2, 64, 1280] = torch.ops.aten.view.default(addmm, [2, 64, 1280]);  addmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:490, code: hidden_states = self.attn1(norm_hidden_states, attention_mask=attention_mask) + hidden_states\n",
      "        add: f16[2, 64, 1280] = torch.ops.aten.add.Tensor(view_9, view);  view_9 = view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:495, code: self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)\n",
      "        native_layer_norm_1 = torch.ops.aten.native_layer_norm.default(add, [1280], arg11_1, arg12_1, 1e-05);  arg11_1 = arg12_1 = None\n",
      "        getitem_6: f16[2, 64, 1280] = native_layer_norm_1[0]\n",
      "        getitem_7: f32[2, 64, 1] = native_layer_norm_1[1]\n",
      "        getitem_8: f32[2, 64, 1] = native_layer_norm_1[2];  native_layer_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t_4: f16[1280, 1280] = torch.ops.aten.t.default(arg13_1);  arg13_1 = None\n",
      "        view_10: f16[128, 1280] = torch.ops.aten.view.default(getitem_6, [128, 1280]);  getitem_6 = None\n",
      "        mm_3: f16[128, 1280] = torch.ops.aten.mm.default(view_10, t_4);  view_10 = t_4 = None\n",
      "        _unsafe_view_7: f16[2, 64, 1280] = torch.ops.aten._unsafe_view.default(mm_3, [2, 64, 1280]);  mm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_11: f16[2, 64, 8, 160] = torch.ops.aten.view.default(_unsafe_view_7, [2, 64, 8, 160]);  _unsafe_view_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_5: f16[2, 8, 64, 160] = torch.ops.aten.permute.default(view_11, [0, 2, 1, 3]);  view_11 = None\n",
      "        clone_4: f16[2, 8, 64, 160] = torch.ops.aten.clone.default(permute_5, memory_format = torch.contiguous_format);  permute_5 = None\n",
      "        _unsafe_view_8: f16[16, 64, 160] = torch.ops.aten._unsafe_view.default(clone_4, [16, 64, 160]);  clone_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_5: f16[768, 1280] = torch.ops.aten.t.default(arg14_1);  arg14_1 = None\n",
      "        view_12: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768])\n",
      "        mm_4: f16[154, 1280] = torch.ops.aten.mm.default(view_12, t_5);  view_12 = t_5 = None\n",
      "        _unsafe_view_9: f16[2, 77, 1280] = torch.ops.aten._unsafe_view.default(mm_4, [2, 77, 1280]);  mm_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_6: f16[768, 1280] = torch.ops.aten.t.default(arg15_1);  arg15_1 = None\n",
      "        view_13: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768]);  arg27_1 = None\n",
      "        mm_5: f16[154, 1280] = torch.ops.aten.mm.default(view_13, t_6);  view_13 = t_6 = None\n",
      "        _unsafe_view_10: f16[2, 77, 1280] = torch.ops.aten._unsafe_view.default(mm_5, [2, 77, 1280]);  mm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_14: f16[2, 77, 8, 160] = torch.ops.aten.view.default(_unsafe_view_9, [2, 77, 8, 160]);  _unsafe_view_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_6: f16[2, 8, 77, 160] = torch.ops.aten.permute.default(view_14, [0, 2, 1, 3]);  view_14 = None\n",
      "        clone_5: f16[2, 8, 77, 160] = torch.ops.aten.clone.default(permute_6, memory_format = torch.contiguous_format);  permute_6 = None\n",
      "        _unsafe_view_11: f16[16, 77, 160] = torch.ops.aten._unsafe_view.default(clone_5, [16, 77, 160]);  clone_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_15: f16[2, 77, 8, 160] = torch.ops.aten.view.default(_unsafe_view_10, [2, 77, 8, 160]);  _unsafe_view_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_7: f16[2, 8, 77, 160] = torch.ops.aten.permute.default(view_15, [0, 2, 1, 3]);  view_15 = None\n",
      "        clone_6: f16[2, 8, 77, 160] = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None\n",
      "        _unsafe_view_12: f16[16, 77, 160] = torch.ops.aten._unsafe_view.default(clone_6, [16, 77, 160]);  clone_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty_1: f16[16, 64, 77] = torch.ops.aten.empty.memory_format([16, 64, 77], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose_1: f16[16, 160, 77] = torch.ops.aten.transpose.int(_unsafe_view_11, -1, -2);  _unsafe_view_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm_1: f16[16, 64, 77] = torch.ops.aten.baddbmm.default(empty_1, _unsafe_view_8, transpose_1, beta = 0, alpha = 0.07905694150420949);  empty_1 = _unsafe_view_8 = transpose_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax_1: f16[16, 64, 77] = torch.ops.aten._softmax.default(baddbmm_1, -1, False);  baddbmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm_1: f16[16, 64, 160] = torch.ops.aten.bmm.default(_softmax_1, _unsafe_view_12);  _softmax_1 = _unsafe_view_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_16: f16[2, 8, 64, 160] = torch.ops.aten.view.default(bmm_1, [2, 8, 64, 160]);  bmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_8: f16[2, 64, 8, 160] = torch.ops.aten.permute.default(view_16, [0, 2, 1, 3]);  view_16 = None\n",
      "        clone_7: f16[2, 64, 8, 160] = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None\n",
      "        _unsafe_view_13: f16[2, 64, 1280] = torch.ops.aten._unsafe_view.default(clone_7, [2, 64, 1280]);  clone_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_7: f16[1280, 1280] = torch.ops.aten.t.default(arg16_1);  arg16_1 = None\n",
      "        view_17: f16[128, 1280] = torch.ops.aten.view.default(_unsafe_view_13, [128, 1280]);  _unsafe_view_13 = None\n",
      "        addmm_1: f16[128, 1280] = torch.ops.aten.addmm.default(arg17_1, view_17, t_7);  arg17_1 = view_17 = t_7 = None\n",
      "        view_18: f16[2, 64, 1280] = torch.ops.aten.view.default(addmm_1, [2, 64, 1280]);  addmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:498, code: self.attn2(\n",
      "        add_1: f16[2, 64, 1280] = torch.ops.aten.add.Tensor(view_18, add);  view_18 = add = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        native_layer_norm_2 = torch.ops.aten.native_layer_norm.default(add_1, [1280], arg18_1, arg19_1, 1e-05);  arg18_1 = arg19_1 = None\n",
      "        getitem_9: f16[2, 64, 1280] = native_layer_norm_2[0]\n",
      "        getitem_10: f32[2, 64, 1] = native_layer_norm_2[1]\n",
      "        getitem_11: f32[2, 64, 1] = native_layer_norm_2[2];  native_layer_norm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:820, code: hidden_states, gate = self.proj(hidden_states).chunk(2, dim=-1)\n",
      "        t_8: f16[1280, 10240] = torch.ops.aten.t.default(arg20_1);  arg20_1 = None\n",
      "        view_19: f16[128, 1280] = torch.ops.aten.view.default(getitem_9, [128, 1280]);  getitem_9 = None\n",
      "        addmm_2: f16[128, 10240] = torch.ops.aten.addmm.default(arg21_1, view_19, t_8);  arg21_1 = view_19 = t_8 = None\n",
      "        view_20: f16[2, 64, 10240] = torch.ops.aten.view.default(addmm_2, [2, 64, 10240]);  addmm_2 = None\n",
      "        split = torch.ops.aten.split.Tensor(view_20, 5120, -1);  view_20 = None\n",
      "        getitem_12: f16[2, 64, 5120] = split[0]\n",
      "        getitem_13: f16[2, 64, 5120] = split[1];  split = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:815, code: return F.gelu(gate)\n",
      "        gelu: f16[2, 64, 5120] = torch.ops.aten.gelu.default(getitem_13);  getitem_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:821, code: return hidden_states * self.gelu(gate)\n",
      "        mul: f16[2, 64, 5120] = torch.ops.aten.mul.Tensor(getitem_12, gelu);  getitem_12 = gelu = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:774, code: hidden_states = module(hidden_states)\n",
      "        t_9: f16[5120, 1280] = torch.ops.aten.t.default(arg22_1);  arg22_1 = None\n",
      "        view_21: f16[128, 5120] = torch.ops.aten.view.default(mul, [128, 5120]);  mul = None\n",
      "        addmm_3: f16[128, 1280] = torch.ops.aten.addmm.default(arg23_1, view_21, t_9);  arg23_1 = view_21 = t_9 = None\n",
      "        view_22: f16[2, 64, 1280] = torch.ops.aten.view.default(addmm_3, [2, 64, 1280]);  addmm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        add_2: f16[2, 64, 1280] = torch.ops.aten.add.Tensor(view_22, add_1);  view_22 = add_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:222, code: hidden_states.reshape(batch, height, weight, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
      "        view_23: f16[2, 8, 8, 1280] = torch.ops.aten.view.default(add_2, [2, 8, 8, 1280]);  add_2 = None\n",
      "        permute_9: f16[2, 1280, 8, 8] = torch.ops.aten.permute.default(view_23, [0, 3, 1, 2]);  view_23 = None\n",
      "        clone_8: f16[2, 1280, 8, 8] = torch.ops.aten.clone.default(permute_9, memory_format = torch.contiguous_format);  permute_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:224, code: hidden_states = self.proj_out(hidden_states)\n",
      "        convolution_1: f16[2, 1280, 8, 8] = torch.ops.aten.convolution.default(clone_8, arg24_1, arg25_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  clone_8 = arg24_1 = arg25_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:231, code: output = hidden_states + residual\n",
      "        add_3: f16[2, 1280, 8, 8] = torch.ops.aten.add.Tensor(convolution_1, arg26_1);  convolution_1 = arg26_1 = None\n",
      "        return (add_3,)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1280], arg1_1: f16[1280], arg2_1: f16[1280, 1280, 3, 3], arg3_1: f16[1280], arg4_1: f16[1280, 1280], arg5_1: f16[1280], arg6_1: f16[1280], arg7_1: f16[1280], arg8_1: f16[1280, 1280, 3, 3], arg9_1: f16[1280], arg10_1: f16[2, 1280, 8, 8], arg11_1: f16[2, 1280]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg10_1, arg0_1, arg1_1, 2, 1280, 64, 32, 1e-05);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 1280, 8, 8] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu: f16[2, 1280, 8, 8] = torch.ops.aten.silu.default(getitem);  getitem = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution: f16[2, 1280, 8, 8] = torch.ops.aten.convolution.default(silu, arg2_1, arg3_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:474, code: temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
      "        silu_1: f16[2, 1280] = torch.ops.aten.silu.default(arg11_1);  arg11_1 = None\n",
      "        t: f16[1280, 1280] = torch.ops.aten.t.default(arg4_1);  arg4_1 = None\n",
      "        addmm: f16[2, 1280] = torch.ops.aten.addmm.default(arg5_1, silu_1, t);  arg5_1 = silu_1 = t = None\n",
      "        slice_1: f16[2, 1280] = torch.ops.aten.slice.Tensor(addmm, 0, 0, 9223372036854775807);  addmm = None\n",
      "        slice_2: f16[2, 1280] = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807);  slice_1 = None\n",
      "        unsqueeze: f16[2, 1280, 1] = torch.ops.aten.unsqueeze.default(slice_2, 2);  slice_2 = None\n",
      "        unsqueeze_1: f16[2, 1280, 1, 1] = torch.ops.aten.unsqueeze.default(unsqueeze, 3);  unsqueeze = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:477, code: hidden_states = hidden_states + temb\n",
      "        add: f16[2, 1280, 8, 8] = torch.ops.aten.add.Tensor(convolution, unsqueeze_1);  convolution = unsqueeze_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_1 = torch.ops.aten.native_group_norm.default(add, arg6_1, arg7_1, 2, 1280, 64, 32, 1e-05);  add = arg6_1 = arg7_1 = None\n",
      "        getitem_3: f16[2, 1280, 8, 8] = native_group_norm_1[0]\n",
      "        getitem_4: f16[2, 32] = native_group_norm_1[1]\n",
      "        getitem_5: f16[2, 32] = native_group_norm_1[2];  native_group_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_2: f16[2, 1280, 8, 8] = torch.ops.aten.silu.default(getitem_3);  getitem_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_1: f16[2, 1280, 8, 8] = torch.ops.aten.convolution.default(silu_2, arg8_1, arg9_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_2 = arg8_1 = arg9_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_1: f16[2, 1280, 8, 8] = torch.ops.aten.add.Tensor(arg10_1, convolution_1);  arg10_1 = convolution_1 = None\n",
      "        div: f16[2, 1280, 8, 8] = torch.ops.aten.div.Tensor(add_1, 1);  add_1 = None\n",
      "        return (div,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[2560], arg1_1: f16[2560], arg2_1: f16[1280, 2560, 3, 3], arg3_1: f16[1280], arg4_1: f16[1280, 1280], arg5_1: f16[1280], arg6_1: f16[1280], arg7_1: f16[1280], arg8_1: f16[1280, 1280, 3, 3], arg9_1: f16[1280], arg10_1: f16[1280, 2560, 1, 1], arg11_1: f16[1280], arg12_1: f16[2560], arg13_1: f16[2560], arg14_1: f16[1280, 2560, 3, 3], arg15_1: f16[1280], arg16_1: f16[1280, 1280], arg17_1: f16[1280], arg18_1: f16[1280], arg19_1: f16[1280], arg20_1: f16[1280, 1280, 3, 3], arg21_1: f16[1280], arg22_1: f16[1280, 2560, 1, 1], arg23_1: f16[1280], arg24_1: f16[2560], arg25_1: f16[2560], arg26_1: f16[1280, 2560, 3, 3], arg27_1: f16[1280], arg28_1: f16[1280, 1280], arg29_1: f16[1280], arg30_1: f16[1280], arg31_1: f16[1280], arg32_1: f16[1280, 1280, 3, 3], arg33_1: f16[1280], arg34_1: f16[1280, 2560, 1, 1], arg35_1: f16[1280], arg36_1: f16[1280, 1280, 3, 3], arg37_1: f16[1280], arg38_1: f16[2, 1280, 8, 8], arg39_1: f16[2, 1280, 8, 8], arg40_1: f16[2, 1280, 8, 8], arg41_1: f16[2, 1280, 8, 8], arg42_1: f16[2, 1280]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/unet_2d_blocks.py:1625, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
      "        cat: f16[2, 2560, 8, 8] = torch.ops.aten.cat.default([arg38_1, arg41_1], 1);  arg38_1 = arg41_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(cat, arg0_1, arg1_1, 2, 2560, 64, 32, 1e-05);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 2560, 8, 8] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu: f16[2, 2560, 8, 8] = torch.ops.aten.silu.default(getitem);  getitem = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution: f16[2, 1280, 8, 8] = torch.ops.aten.convolution.default(silu, arg2_1, arg3_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:474, code: temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
      "        silu_1: f16[2, 1280] = torch.ops.aten.silu.default(arg42_1)\n",
      "        t: f16[1280, 1280] = torch.ops.aten.t.default(arg4_1);  arg4_1 = None\n",
      "        addmm: f16[2, 1280] = torch.ops.aten.addmm.default(arg5_1, silu_1, t);  arg5_1 = silu_1 = t = None\n",
      "        slice_1: f16[2, 1280] = torch.ops.aten.slice.Tensor(addmm, 0, 0, 9223372036854775807);  addmm = None\n",
      "        slice_2: f16[2, 1280] = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807);  slice_1 = None\n",
      "        unsqueeze: f16[2, 1280, 1] = torch.ops.aten.unsqueeze.default(slice_2, 2);  slice_2 = None\n",
      "        unsqueeze_1: f16[2, 1280, 1, 1] = torch.ops.aten.unsqueeze.default(unsqueeze, 3);  unsqueeze = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:477, code: hidden_states = hidden_states + temb\n",
      "        add: f16[2, 1280, 8, 8] = torch.ops.aten.add.Tensor(convolution, unsqueeze_1);  convolution = unsqueeze_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_1 = torch.ops.aten.native_group_norm.default(add, arg6_1, arg7_1, 2, 1280, 64, 32, 1e-05);  add = arg6_1 = arg7_1 = None\n",
      "        getitem_3: f16[2, 1280, 8, 8] = native_group_norm_1[0]\n",
      "        getitem_4: f16[2, 32] = native_group_norm_1[1]\n",
      "        getitem_5: f16[2, 32] = native_group_norm_1[2];  native_group_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_2: f16[2, 1280, 8, 8] = torch.ops.aten.silu.default(getitem_3);  getitem_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_1: f16[2, 1280, 8, 8] = torch.ops.aten.convolution.default(silu_2, arg8_1, arg9_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_2 = arg8_1 = arg9_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:491, code: input_tensor = self.conv_shortcut(input_tensor)\n",
      "        convolution_2: f16[2, 1280, 8, 8] = torch.ops.aten.convolution.default(cat, arg10_1, arg11_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  cat = arg10_1 = arg11_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_1: f16[2, 1280, 8, 8] = torch.ops.aten.add.Tensor(convolution_2, convolution_1);  convolution_2 = convolution_1 = None\n",
      "        div: f16[2, 1280, 8, 8] = torch.ops.aten.div.Tensor(add_1, 1.0);  add_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/unet_2d_blocks.py:1625, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
      "        cat_1: f16[2, 2560, 8, 8] = torch.ops.aten.cat.default([div, arg40_1], 1);  div = arg40_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm_2 = torch.ops.aten.native_group_norm.default(cat_1, arg12_1, arg13_1, 2, 2560, 64, 32, 1e-05);  arg12_1 = arg13_1 = None\n",
      "        getitem_6: f16[2, 2560, 8, 8] = native_group_norm_2[0]\n",
      "        getitem_7: f16[2, 32] = native_group_norm_2[1]\n",
      "        getitem_8: f16[2, 32] = native_group_norm_2[2];  native_group_norm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_3: f16[2, 2560, 8, 8] = torch.ops.aten.silu.default(getitem_6);  getitem_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution_3: f16[2, 1280, 8, 8] = torch.ops.aten.convolution.default(silu_3, arg14_1, arg15_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_3 = arg14_1 = arg15_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:474, code: temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
      "        silu_4: f16[2, 1280] = torch.ops.aten.silu.default(arg42_1)\n",
      "        t_1: f16[1280, 1280] = torch.ops.aten.t.default(arg16_1);  arg16_1 = None\n",
      "        addmm_1: f16[2, 1280] = torch.ops.aten.addmm.default(arg17_1, silu_4, t_1);  arg17_1 = silu_4 = t_1 = None\n",
      "        slice_3: f16[2, 1280] = torch.ops.aten.slice.Tensor(addmm_1, 0, 0, 9223372036854775807);  addmm_1 = None\n",
      "        slice_4: f16[2, 1280] = torch.ops.aten.slice.Tensor(slice_3, 1, 0, 9223372036854775807);  slice_3 = None\n",
      "        unsqueeze_2: f16[2, 1280, 1] = torch.ops.aten.unsqueeze.default(slice_4, 2);  slice_4 = None\n",
      "        unsqueeze_3: f16[2, 1280, 1, 1] = torch.ops.aten.unsqueeze.default(unsqueeze_2, 3);  unsqueeze_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:477, code: hidden_states = hidden_states + temb\n",
      "        add_2: f16[2, 1280, 8, 8] = torch.ops.aten.add.Tensor(convolution_3, unsqueeze_3);  convolution_3 = unsqueeze_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_3 = torch.ops.aten.native_group_norm.default(add_2, arg18_1, arg19_1, 2, 1280, 64, 32, 1e-05);  add_2 = arg18_1 = arg19_1 = None\n",
      "        getitem_9: f16[2, 1280, 8, 8] = native_group_norm_3[0]\n",
      "        getitem_10: f16[2, 32] = native_group_norm_3[1]\n",
      "        getitem_11: f16[2, 32] = native_group_norm_3[2];  native_group_norm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_5: f16[2, 1280, 8, 8] = torch.ops.aten.silu.default(getitem_9);  getitem_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_4: f16[2, 1280, 8, 8] = torch.ops.aten.convolution.default(silu_5, arg20_1, arg21_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_5 = arg20_1 = arg21_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:491, code: input_tensor = self.conv_shortcut(input_tensor)\n",
      "        convolution_5: f16[2, 1280, 8, 8] = torch.ops.aten.convolution.default(cat_1, arg22_1, arg23_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  cat_1 = arg22_1 = arg23_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_3: f16[2, 1280, 8, 8] = torch.ops.aten.add.Tensor(convolution_5, convolution_4);  convolution_5 = convolution_4 = None\n",
      "        div_1: f16[2, 1280, 8, 8] = torch.ops.aten.div.Tensor(add_3, 1.0);  add_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/unet_2d_blocks.py:1625, code: hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n",
      "        cat_2: f16[2, 2560, 8, 8] = torch.ops.aten.cat.default([div_1, arg39_1], 1);  div_1 = arg39_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm_4 = torch.ops.aten.native_group_norm.default(cat_2, arg24_1, arg25_1, 2, 2560, 64, 32, 1e-05);  arg24_1 = arg25_1 = None\n",
      "        getitem_12: f16[2, 2560, 8, 8] = native_group_norm_4[0]\n",
      "        getitem_13: f16[2, 32] = native_group_norm_4[1]\n",
      "        getitem_14: f16[2, 32] = native_group_norm_4[2];  native_group_norm_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_6: f16[2, 2560, 8, 8] = torch.ops.aten.silu.default(getitem_12);  getitem_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution_6: f16[2, 1280, 8, 8] = torch.ops.aten.convolution.default(silu_6, arg26_1, arg27_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_6 = arg26_1 = arg27_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:474, code: temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
      "        silu_7: f16[2, 1280] = torch.ops.aten.silu.default(arg42_1);  arg42_1 = None\n",
      "        t_2: f16[1280, 1280] = torch.ops.aten.t.default(arg28_1);  arg28_1 = None\n",
      "        addmm_2: f16[2, 1280] = torch.ops.aten.addmm.default(arg29_1, silu_7, t_2);  arg29_1 = silu_7 = t_2 = None\n",
      "        slice_5: f16[2, 1280] = torch.ops.aten.slice.Tensor(addmm_2, 0, 0, 9223372036854775807);  addmm_2 = None\n",
      "        slice_6: f16[2, 1280] = torch.ops.aten.slice.Tensor(slice_5, 1, 0, 9223372036854775807);  slice_5 = None\n",
      "        unsqueeze_4: f16[2, 1280, 1] = torch.ops.aten.unsqueeze.default(slice_6, 2);  slice_6 = None\n",
      "        unsqueeze_5: f16[2, 1280, 1, 1] = torch.ops.aten.unsqueeze.default(unsqueeze_4, 3);  unsqueeze_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:477, code: hidden_states = hidden_states + temb\n",
      "        add_4: f16[2, 1280, 8, 8] = torch.ops.aten.add.Tensor(convolution_6, unsqueeze_5);  convolution_6 = unsqueeze_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_5 = torch.ops.aten.native_group_norm.default(add_4, arg30_1, arg31_1, 2, 1280, 64, 32, 1e-05);  add_4 = arg30_1 = arg31_1 = None\n",
      "        getitem_15: f16[2, 1280, 8, 8] = native_group_norm_5[0]\n",
      "        getitem_16: f16[2, 32] = native_group_norm_5[1]\n",
      "        getitem_17: f16[2, 32] = native_group_norm_5[2];  native_group_norm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_8: f16[2, 1280, 8, 8] = torch.ops.aten.silu.default(getitem_15);  getitem_15 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_7: f16[2, 1280, 8, 8] = torch.ops.aten.convolution.default(silu_8, arg32_1, arg33_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_8 = arg32_1 = arg33_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:491, code: input_tensor = self.conv_shortcut(input_tensor)\n",
      "        convolution_8: f16[2, 1280, 8, 8] = torch.ops.aten.convolution.default(cat_2, arg34_1, arg35_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  cat_2 = arg34_1 = arg35_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_5: f16[2, 1280, 8, 8] = torch.ops.aten.add.Tensor(convolution_8, convolution_7);  convolution_8 = convolution_7 = None\n",
      "        div_2: f16[2, 1280, 8, 8] = torch.ops.aten.div.Tensor(add_5, 1.0);  add_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:128, code: hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n",
      "        upsample_nearest2d: f16[2, 1280, 16, 16] = torch.ops.aten.upsample_nearest2d.default(div_2, [16, 16], 2.0, 2.0);  div_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:139, code: hidden_states = self.conv(hidden_states)\n",
      "        convolution_9: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(upsample_nearest2d, arg36_1, arg37_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  upsample_nearest2d = arg36_1 = arg37_1 = None\n",
      "        return (convolution_9,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[2560], arg1_1: f16[2560], arg2_1: f16[1280, 2560, 3, 3], arg3_1: f16[1280], arg4_1: f16[1280, 1280], arg5_1: f16[1280], arg6_1: f16[1280], arg7_1: f16[1280], arg8_1: f16[1280, 1280, 3, 3], arg9_1: f16[1280], arg10_1: f16[1280, 2560, 1, 1], arg11_1: f16[1280], arg12_1: f16[2, 2560, 16, 16], arg13_1: f16[2, 1280]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg12_1, arg0_1, arg1_1, 2, 2560, 256, 32, 1e-05);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 2560, 16, 16] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu: f16[2, 2560, 16, 16] = torch.ops.aten.silu.default(getitem);  getitem = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(silu, arg2_1, arg3_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:474, code: temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
      "        silu_1: f16[2, 1280] = torch.ops.aten.silu.default(arg13_1);  arg13_1 = None\n",
      "        t: f16[1280, 1280] = torch.ops.aten.t.default(arg4_1);  arg4_1 = None\n",
      "        addmm: f16[2, 1280] = torch.ops.aten.addmm.default(arg5_1, silu_1, t);  arg5_1 = silu_1 = t = None\n",
      "        slice_1: f16[2, 1280] = torch.ops.aten.slice.Tensor(addmm, 0, 0, 9223372036854775807);  addmm = None\n",
      "        slice_2: f16[2, 1280] = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807);  slice_1 = None\n",
      "        unsqueeze: f16[2, 1280, 1] = torch.ops.aten.unsqueeze.default(slice_2, 2);  slice_2 = None\n",
      "        unsqueeze_1: f16[2, 1280, 1, 1] = torch.ops.aten.unsqueeze.default(unsqueeze, 3);  unsqueeze = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:477, code: hidden_states = hidden_states + temb\n",
      "        add: f16[2, 1280, 16, 16] = torch.ops.aten.add.Tensor(convolution, unsqueeze_1);  convolution = unsqueeze_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_1 = torch.ops.aten.native_group_norm.default(add, arg6_1, arg7_1, 2, 1280, 256, 32, 1e-05);  add = arg6_1 = arg7_1 = None\n",
      "        getitem_3: f16[2, 1280, 16, 16] = native_group_norm_1[0]\n",
      "        getitem_4: f16[2, 32] = native_group_norm_1[1]\n",
      "        getitem_5: f16[2, 32] = native_group_norm_1[2];  native_group_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_2: f16[2, 1280, 16, 16] = torch.ops.aten.silu.default(getitem_3);  getitem_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_1: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(silu_2, arg8_1, arg9_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_2 = arg8_1 = arg9_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:491, code: input_tensor = self.conv_shortcut(input_tensor)\n",
      "        convolution_2: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(arg12_1, arg10_1, arg11_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  arg12_1 = arg10_1 = arg11_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_1: f16[2, 1280, 16, 16] = torch.ops.aten.add.Tensor(convolution_2, convolution_1);  convolution_2 = convolution_1 = None\n",
      "        div: f16[2, 1280, 16, 16] = torch.ops.aten.div.Tensor(add_1, 1.0);  add_1 = None\n",
      "        return (div,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1280], arg1_1: f16[1280], arg2_1: f16[1280, 1280, 1, 1], arg3_1: f16[1280], arg4_1: f16[1280], arg5_1: f16[1280], arg6_1: f16[1280, 1280], arg7_1: f16[1280, 1280], arg8_1: f16[1280, 1280], arg9_1: f16[1280, 1280], arg10_1: f16[1280], arg11_1: f16[1280], arg12_1: f16[1280], arg13_1: f16[1280, 1280], arg14_1: f16[1280, 768], arg15_1: f16[1280, 768], arg16_1: f16[1280, 1280], arg17_1: f16[1280], arg18_1: f16[1280], arg19_1: f16[1280], arg20_1: f16[10240, 1280], arg21_1: f16[10240], arg22_1: f16[1280, 5120], arg23_1: f16[1280], arg24_1: f16[1280, 1280, 1, 1], arg25_1: f16[1280], arg26_1: f16[2, 1280, 16, 16], arg27_1: f16[2, 77, 768]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:202, code: hidden_states = self.norm(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg26_1, arg0_1, arg1_1, 2, 1280, 256, 32, 1e-06);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 1280, 16, 16] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:204, code: hidden_states = self.proj_in(hidden_states)\n",
      "        convolution: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(getitem, arg2_1, arg3_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  getitem = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:206, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * weight, inner_dim)\n",
      "        permute: f16[2, 16, 16, 1280] = torch.ops.aten.permute.default(convolution, [0, 2, 3, 1]);  convolution = None\n",
      "        view: f16[2, 256, 1280] = torch.ops.aten.view.default(permute, [2, 256, 1280]);  permute = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:482, code: self.norm1(hidden_states, timestep) if self.use_ada_layer_norm else self.norm1(hidden_states)\n",
      "        native_layer_norm = torch.ops.aten.native_layer_norm.default(view, [1280], arg4_1, arg5_1, 1e-05);  arg4_1 = arg5_1 = None\n",
      "        getitem_3: f16[2, 256, 1280] = native_layer_norm[0]\n",
      "        getitem_4: f32[2, 256, 1] = native_layer_norm[1]\n",
      "        getitem_5: f32[2, 256, 1] = native_layer_norm[2];  native_layer_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t: f16[1280, 1280] = torch.ops.aten.t.default(arg6_1);  arg6_1 = None\n",
      "        view_1: f16[512, 1280] = torch.ops.aten.view.default(getitem_3, [512, 1280])\n",
      "        mm: f16[512, 1280] = torch.ops.aten.mm.default(view_1, t);  view_1 = t = None\n",
      "        _unsafe_view: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(mm, [2, 256, 1280]);  mm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_2: f16[2, 256, 8, 160] = torch.ops.aten.view.default(_unsafe_view, [2, 256, 8, 160]);  _unsafe_view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_1: f16[2, 8, 256, 160] = torch.ops.aten.permute.default(view_2, [0, 2, 1, 3]);  view_2 = None\n",
      "        clone: f16[2, 8, 256, 160] = torch.ops.aten.clone.default(permute_1, memory_format = torch.contiguous_format);  permute_1 = None\n",
      "        _unsafe_view_1: f16[16, 256, 160] = torch.ops.aten._unsafe_view.default(clone, [16, 256, 160]);  clone = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_1: f16[1280, 1280] = torch.ops.aten.t.default(arg7_1);  arg7_1 = None\n",
      "        view_3: f16[512, 1280] = torch.ops.aten.view.default(getitem_3, [512, 1280])\n",
      "        mm_1: f16[512, 1280] = torch.ops.aten.mm.default(view_3, t_1);  view_3 = t_1 = None\n",
      "        _unsafe_view_2: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(mm_1, [2, 256, 1280]);  mm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_2: f16[1280, 1280] = torch.ops.aten.t.default(arg8_1);  arg8_1 = None\n",
      "        view_4: f16[512, 1280] = torch.ops.aten.view.default(getitem_3, [512, 1280]);  getitem_3 = None\n",
      "        mm_2: f16[512, 1280] = torch.ops.aten.mm.default(view_4, t_2);  view_4 = t_2 = None\n",
      "        _unsafe_view_3: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(mm_2, [2, 256, 1280]);  mm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_5: f16[2, 256, 8, 160] = torch.ops.aten.view.default(_unsafe_view_2, [2, 256, 8, 160]);  _unsafe_view_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_2: f16[2, 8, 256, 160] = torch.ops.aten.permute.default(view_5, [0, 2, 1, 3]);  view_5 = None\n",
      "        clone_1: f16[2, 8, 256, 160] = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None\n",
      "        _unsafe_view_4: f16[16, 256, 160] = torch.ops.aten._unsafe_view.default(clone_1, [16, 256, 160]);  clone_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_6: f16[2, 256, 8, 160] = torch.ops.aten.view.default(_unsafe_view_3, [2, 256, 8, 160]);  _unsafe_view_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_3: f16[2, 8, 256, 160] = torch.ops.aten.permute.default(view_6, [0, 2, 1, 3]);  view_6 = None\n",
      "        clone_2: f16[2, 8, 256, 160] = torch.ops.aten.clone.default(permute_3, memory_format = torch.contiguous_format);  permute_3 = None\n",
      "        _unsafe_view_5: f16[16, 256, 160] = torch.ops.aten._unsafe_view.default(clone_2, [16, 256, 160]);  clone_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty: f16[16, 256, 256] = torch.ops.aten.empty.memory_format([16, 256, 256], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose: f16[16, 160, 256] = torch.ops.aten.transpose.int(_unsafe_view_4, -1, -2);  _unsafe_view_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm: f16[16, 256, 256] = torch.ops.aten.baddbmm.default(empty, _unsafe_view_1, transpose, beta = 0, alpha = 0.07905694150420949);  empty = _unsafe_view_1 = transpose = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax: f16[16, 256, 256] = torch.ops.aten._softmax.default(baddbmm, -1, False);  baddbmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm: f16[16, 256, 160] = torch.ops.aten.bmm.default(_softmax, _unsafe_view_5);  _softmax = _unsafe_view_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_7: f16[2, 8, 256, 160] = torch.ops.aten.view.default(bmm, [2, 8, 256, 160]);  bmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_4: f16[2, 256, 8, 160] = torch.ops.aten.permute.default(view_7, [0, 2, 1, 3]);  view_7 = None\n",
      "        clone_3: f16[2, 256, 8, 160] = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None\n",
      "        _unsafe_view_6: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(clone_3, [2, 256, 1280]);  clone_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_3: f16[1280, 1280] = torch.ops.aten.t.default(arg9_1);  arg9_1 = None\n",
      "        view_8: f16[512, 1280] = torch.ops.aten.view.default(_unsafe_view_6, [512, 1280]);  _unsafe_view_6 = None\n",
      "        addmm: f16[512, 1280] = torch.ops.aten.addmm.default(arg10_1, view_8, t_3);  arg10_1 = view_8 = t_3 = None\n",
      "        view_9: f16[2, 256, 1280] = torch.ops.aten.view.default(addmm, [2, 256, 1280]);  addmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:490, code: hidden_states = self.attn1(norm_hidden_states, attention_mask=attention_mask) + hidden_states\n",
      "        add: f16[2, 256, 1280] = torch.ops.aten.add.Tensor(view_9, view);  view_9 = view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:495, code: self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)\n",
      "        native_layer_norm_1 = torch.ops.aten.native_layer_norm.default(add, [1280], arg11_1, arg12_1, 1e-05);  arg11_1 = arg12_1 = None\n",
      "        getitem_6: f16[2, 256, 1280] = native_layer_norm_1[0]\n",
      "        getitem_7: f32[2, 256, 1] = native_layer_norm_1[1]\n",
      "        getitem_8: f32[2, 256, 1] = native_layer_norm_1[2];  native_layer_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t_4: f16[1280, 1280] = torch.ops.aten.t.default(arg13_1);  arg13_1 = None\n",
      "        view_10: f16[512, 1280] = torch.ops.aten.view.default(getitem_6, [512, 1280]);  getitem_6 = None\n",
      "        mm_3: f16[512, 1280] = torch.ops.aten.mm.default(view_10, t_4);  view_10 = t_4 = None\n",
      "        _unsafe_view_7: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(mm_3, [2, 256, 1280]);  mm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_11: f16[2, 256, 8, 160] = torch.ops.aten.view.default(_unsafe_view_7, [2, 256, 8, 160]);  _unsafe_view_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_5: f16[2, 8, 256, 160] = torch.ops.aten.permute.default(view_11, [0, 2, 1, 3]);  view_11 = None\n",
      "        clone_4: f16[2, 8, 256, 160] = torch.ops.aten.clone.default(permute_5, memory_format = torch.contiguous_format);  permute_5 = None\n",
      "        _unsafe_view_8: f16[16, 256, 160] = torch.ops.aten._unsafe_view.default(clone_4, [16, 256, 160]);  clone_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_5: f16[768, 1280] = torch.ops.aten.t.default(arg14_1);  arg14_1 = None\n",
      "        view_12: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768])\n",
      "        mm_4: f16[154, 1280] = torch.ops.aten.mm.default(view_12, t_5);  view_12 = t_5 = None\n",
      "        _unsafe_view_9: f16[2, 77, 1280] = torch.ops.aten._unsafe_view.default(mm_4, [2, 77, 1280]);  mm_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_6: f16[768, 1280] = torch.ops.aten.t.default(arg15_1);  arg15_1 = None\n",
      "        view_13: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768]);  arg27_1 = None\n",
      "        mm_5: f16[154, 1280] = torch.ops.aten.mm.default(view_13, t_6);  view_13 = t_6 = None\n",
      "        _unsafe_view_10: f16[2, 77, 1280] = torch.ops.aten._unsafe_view.default(mm_5, [2, 77, 1280]);  mm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_14: f16[2, 77, 8, 160] = torch.ops.aten.view.default(_unsafe_view_9, [2, 77, 8, 160]);  _unsafe_view_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_6: f16[2, 8, 77, 160] = torch.ops.aten.permute.default(view_14, [0, 2, 1, 3]);  view_14 = None\n",
      "        clone_5: f16[2, 8, 77, 160] = torch.ops.aten.clone.default(permute_6, memory_format = torch.contiguous_format);  permute_6 = None\n",
      "        _unsafe_view_11: f16[16, 77, 160] = torch.ops.aten._unsafe_view.default(clone_5, [16, 77, 160]);  clone_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_15: f16[2, 77, 8, 160] = torch.ops.aten.view.default(_unsafe_view_10, [2, 77, 8, 160]);  _unsafe_view_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_7: f16[2, 8, 77, 160] = torch.ops.aten.permute.default(view_15, [0, 2, 1, 3]);  view_15 = None\n",
      "        clone_6: f16[2, 8, 77, 160] = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None\n",
      "        _unsafe_view_12: f16[16, 77, 160] = torch.ops.aten._unsafe_view.default(clone_6, [16, 77, 160]);  clone_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty_1: f16[16, 256, 77] = torch.ops.aten.empty.memory_format([16, 256, 77], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose_1: f16[16, 160, 77] = torch.ops.aten.transpose.int(_unsafe_view_11, -1, -2);  _unsafe_view_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm_1: f16[16, 256, 77] = torch.ops.aten.baddbmm.default(empty_1, _unsafe_view_8, transpose_1, beta = 0, alpha = 0.07905694150420949);  empty_1 = _unsafe_view_8 = transpose_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax_1: f16[16, 256, 77] = torch.ops.aten._softmax.default(baddbmm_1, -1, False);  baddbmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm_1: f16[16, 256, 160] = torch.ops.aten.bmm.default(_softmax_1, _unsafe_view_12);  _softmax_1 = _unsafe_view_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_16: f16[2, 8, 256, 160] = torch.ops.aten.view.default(bmm_1, [2, 8, 256, 160]);  bmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_8: f16[2, 256, 8, 160] = torch.ops.aten.permute.default(view_16, [0, 2, 1, 3]);  view_16 = None\n",
      "        clone_7: f16[2, 256, 8, 160] = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None\n",
      "        _unsafe_view_13: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(clone_7, [2, 256, 1280]);  clone_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_7: f16[1280, 1280] = torch.ops.aten.t.default(arg16_1);  arg16_1 = None\n",
      "        view_17: f16[512, 1280] = torch.ops.aten.view.default(_unsafe_view_13, [512, 1280]);  _unsafe_view_13 = None\n",
      "        addmm_1: f16[512, 1280] = torch.ops.aten.addmm.default(arg17_1, view_17, t_7);  arg17_1 = view_17 = t_7 = None\n",
      "        view_18: f16[2, 256, 1280] = torch.ops.aten.view.default(addmm_1, [2, 256, 1280]);  addmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:498, code: self.attn2(\n",
      "        add_1: f16[2, 256, 1280] = torch.ops.aten.add.Tensor(view_18, add);  view_18 = add = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        native_layer_norm_2 = torch.ops.aten.native_layer_norm.default(add_1, [1280], arg18_1, arg19_1, 1e-05);  arg18_1 = arg19_1 = None\n",
      "        getitem_9: f16[2, 256, 1280] = native_layer_norm_2[0]\n",
      "        getitem_10: f32[2, 256, 1] = native_layer_norm_2[1]\n",
      "        getitem_11: f32[2, 256, 1] = native_layer_norm_2[2];  native_layer_norm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:820, code: hidden_states, gate = self.proj(hidden_states).chunk(2, dim=-1)\n",
      "        t_8: f16[1280, 10240] = torch.ops.aten.t.default(arg20_1);  arg20_1 = None\n",
      "        view_19: f16[512, 1280] = torch.ops.aten.view.default(getitem_9, [512, 1280]);  getitem_9 = None\n",
      "        addmm_2: f16[512, 10240] = torch.ops.aten.addmm.default(arg21_1, view_19, t_8);  arg21_1 = view_19 = t_8 = None\n",
      "        view_20: f16[2, 256, 10240] = torch.ops.aten.view.default(addmm_2, [2, 256, 10240]);  addmm_2 = None\n",
      "        split = torch.ops.aten.split.Tensor(view_20, 5120, -1);  view_20 = None\n",
      "        getitem_12: f16[2, 256, 5120] = split[0]\n",
      "        getitem_13: f16[2, 256, 5120] = split[1];  split = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:815, code: return F.gelu(gate)\n",
      "        gelu: f16[2, 256, 5120] = torch.ops.aten.gelu.default(getitem_13);  getitem_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:821, code: return hidden_states * self.gelu(gate)\n",
      "        mul: f16[2, 256, 5120] = torch.ops.aten.mul.Tensor(getitem_12, gelu);  getitem_12 = gelu = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:774, code: hidden_states = module(hidden_states)\n",
      "        t_9: f16[5120, 1280] = torch.ops.aten.t.default(arg22_1);  arg22_1 = None\n",
      "        view_21: f16[512, 5120] = torch.ops.aten.view.default(mul, [512, 5120]);  mul = None\n",
      "        addmm_3: f16[512, 1280] = torch.ops.aten.addmm.default(arg23_1, view_21, t_9);  arg23_1 = view_21 = t_9 = None\n",
      "        view_22: f16[2, 256, 1280] = torch.ops.aten.view.default(addmm_3, [2, 256, 1280]);  addmm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        add_2: f16[2, 256, 1280] = torch.ops.aten.add.Tensor(view_22, add_1);  view_22 = add_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:222, code: hidden_states.reshape(batch, height, weight, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
      "        view_23: f16[2, 16, 16, 1280] = torch.ops.aten.view.default(add_2, [2, 16, 16, 1280]);  add_2 = None\n",
      "        permute_9: f16[2, 1280, 16, 16] = torch.ops.aten.permute.default(view_23, [0, 3, 1, 2]);  view_23 = None\n",
      "        clone_8: f16[2, 1280, 16, 16] = torch.ops.aten.clone.default(permute_9, memory_format = torch.contiguous_format);  permute_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:224, code: hidden_states = self.proj_out(hidden_states)\n",
      "        convolution_1: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(clone_8, arg24_1, arg25_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  clone_8 = arg24_1 = arg25_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:231, code: output = hidden_states + residual\n",
      "        add_3: f16[2, 1280, 16, 16] = torch.ops.aten.add.Tensor(convolution_1, arg26_1);  convolution_1 = arg26_1 = None\n",
      "        return (add_3,)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[2560], arg1_1: f16[2560], arg2_1: f16[1280, 2560, 3, 3], arg3_1: f16[1280], arg4_1: f16[1280, 1280], arg5_1: f16[1280], arg6_1: f16[1280], arg7_1: f16[1280], arg8_1: f16[1280, 1280, 3, 3], arg9_1: f16[1280], arg10_1: f16[1280, 2560, 1, 1], arg11_1: f16[1280], arg12_1: f16[2, 2560, 16, 16], arg13_1: f16[2, 1280]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg12_1, arg0_1, arg1_1, 2, 2560, 256, 32, 1e-05);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 2560, 16, 16] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu: f16[2, 2560, 16, 16] = torch.ops.aten.silu.default(getitem);  getitem = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(silu, arg2_1, arg3_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:474, code: temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
      "        silu_1: f16[2, 1280] = torch.ops.aten.silu.default(arg13_1);  arg13_1 = None\n",
      "        t: f16[1280, 1280] = torch.ops.aten.t.default(arg4_1);  arg4_1 = None\n",
      "        addmm: f16[2, 1280] = torch.ops.aten.addmm.default(arg5_1, silu_1, t);  arg5_1 = silu_1 = t = None\n",
      "        slice_1: f16[2, 1280] = torch.ops.aten.slice.Tensor(addmm, 0, 0, 9223372036854775807);  addmm = None\n",
      "        slice_2: f16[2, 1280] = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807);  slice_1 = None\n",
      "        unsqueeze: f16[2, 1280, 1] = torch.ops.aten.unsqueeze.default(slice_2, 2);  slice_2 = None\n",
      "        unsqueeze_1: f16[2, 1280, 1, 1] = torch.ops.aten.unsqueeze.default(unsqueeze, 3);  unsqueeze = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:477, code: hidden_states = hidden_states + temb\n",
      "        add: f16[2, 1280, 16, 16] = torch.ops.aten.add.Tensor(convolution, unsqueeze_1);  convolution = unsqueeze_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_1 = torch.ops.aten.native_group_norm.default(add, arg6_1, arg7_1, 2, 1280, 256, 32, 1e-05);  add = arg6_1 = arg7_1 = None\n",
      "        getitem_3: f16[2, 1280, 16, 16] = native_group_norm_1[0]\n",
      "        getitem_4: f16[2, 32] = native_group_norm_1[1]\n",
      "        getitem_5: f16[2, 32] = native_group_norm_1[2];  native_group_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_2: f16[2, 1280, 16, 16] = torch.ops.aten.silu.default(getitem_3);  getitem_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_1: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(silu_2, arg8_1, arg9_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_2 = arg8_1 = arg9_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:491, code: input_tensor = self.conv_shortcut(input_tensor)\n",
      "        convolution_2: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(arg12_1, arg10_1, arg11_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  arg12_1 = arg10_1 = arg11_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_1: f16[2, 1280, 16, 16] = torch.ops.aten.add.Tensor(convolution_2, convolution_1);  convolution_2 = convolution_1 = None\n",
      "        div: f16[2, 1280, 16, 16] = torch.ops.aten.div.Tensor(add_1, 1.0);  add_1 = None\n",
      "        return (div,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1280], arg1_1: f16[1280], arg2_1: f16[1280, 1280, 1, 1], arg3_1: f16[1280], arg4_1: f16[1280], arg5_1: f16[1280], arg6_1: f16[1280, 1280], arg7_1: f16[1280, 1280], arg8_1: f16[1280, 1280], arg9_1: f16[1280, 1280], arg10_1: f16[1280], arg11_1: f16[1280], arg12_1: f16[1280], arg13_1: f16[1280, 1280], arg14_1: f16[1280, 768], arg15_1: f16[1280, 768], arg16_1: f16[1280, 1280], arg17_1: f16[1280], arg18_1: f16[1280], arg19_1: f16[1280], arg20_1: f16[10240, 1280], arg21_1: f16[10240], arg22_1: f16[1280, 5120], arg23_1: f16[1280], arg24_1: f16[1280, 1280, 1, 1], arg25_1: f16[1280], arg26_1: f16[2, 1280, 16, 16], arg27_1: f16[2, 77, 768]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:202, code: hidden_states = self.norm(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg26_1, arg0_1, arg1_1, 2, 1280, 256, 32, 1e-06);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 1280, 16, 16] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:204, code: hidden_states = self.proj_in(hidden_states)\n",
      "        convolution: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(getitem, arg2_1, arg3_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  getitem = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:206, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * weight, inner_dim)\n",
      "        permute: f16[2, 16, 16, 1280] = torch.ops.aten.permute.default(convolution, [0, 2, 3, 1]);  convolution = None\n",
      "        view: f16[2, 256, 1280] = torch.ops.aten.view.default(permute, [2, 256, 1280]);  permute = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:482, code: self.norm1(hidden_states, timestep) if self.use_ada_layer_norm else self.norm1(hidden_states)\n",
      "        native_layer_norm = torch.ops.aten.native_layer_norm.default(view, [1280], arg4_1, arg5_1, 1e-05);  arg4_1 = arg5_1 = None\n",
      "        getitem_3: f16[2, 256, 1280] = native_layer_norm[0]\n",
      "        getitem_4: f32[2, 256, 1] = native_layer_norm[1]\n",
      "        getitem_5: f32[2, 256, 1] = native_layer_norm[2];  native_layer_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t: f16[1280, 1280] = torch.ops.aten.t.default(arg6_1);  arg6_1 = None\n",
      "        view_1: f16[512, 1280] = torch.ops.aten.view.default(getitem_3, [512, 1280])\n",
      "        mm: f16[512, 1280] = torch.ops.aten.mm.default(view_1, t);  view_1 = t = None\n",
      "        _unsafe_view: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(mm, [2, 256, 1280]);  mm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_2: f16[2, 256, 8, 160] = torch.ops.aten.view.default(_unsafe_view, [2, 256, 8, 160]);  _unsafe_view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_1: f16[2, 8, 256, 160] = torch.ops.aten.permute.default(view_2, [0, 2, 1, 3]);  view_2 = None\n",
      "        clone: f16[2, 8, 256, 160] = torch.ops.aten.clone.default(permute_1, memory_format = torch.contiguous_format);  permute_1 = None\n",
      "        _unsafe_view_1: f16[16, 256, 160] = torch.ops.aten._unsafe_view.default(clone, [16, 256, 160]);  clone = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_1: f16[1280, 1280] = torch.ops.aten.t.default(arg7_1);  arg7_1 = None\n",
      "        view_3: f16[512, 1280] = torch.ops.aten.view.default(getitem_3, [512, 1280])\n",
      "        mm_1: f16[512, 1280] = torch.ops.aten.mm.default(view_3, t_1);  view_3 = t_1 = None\n",
      "        _unsafe_view_2: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(mm_1, [2, 256, 1280]);  mm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_2: f16[1280, 1280] = torch.ops.aten.t.default(arg8_1);  arg8_1 = None\n",
      "        view_4: f16[512, 1280] = torch.ops.aten.view.default(getitem_3, [512, 1280]);  getitem_3 = None\n",
      "        mm_2: f16[512, 1280] = torch.ops.aten.mm.default(view_4, t_2);  view_4 = t_2 = None\n",
      "        _unsafe_view_3: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(mm_2, [2, 256, 1280]);  mm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_5: f16[2, 256, 8, 160] = torch.ops.aten.view.default(_unsafe_view_2, [2, 256, 8, 160]);  _unsafe_view_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_2: f16[2, 8, 256, 160] = torch.ops.aten.permute.default(view_5, [0, 2, 1, 3]);  view_5 = None\n",
      "        clone_1: f16[2, 8, 256, 160] = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None\n",
      "        _unsafe_view_4: f16[16, 256, 160] = torch.ops.aten._unsafe_view.default(clone_1, [16, 256, 160]);  clone_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_6: f16[2, 256, 8, 160] = torch.ops.aten.view.default(_unsafe_view_3, [2, 256, 8, 160]);  _unsafe_view_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_3: f16[2, 8, 256, 160] = torch.ops.aten.permute.default(view_6, [0, 2, 1, 3]);  view_6 = None\n",
      "        clone_2: f16[2, 8, 256, 160] = torch.ops.aten.clone.default(permute_3, memory_format = torch.contiguous_format);  permute_3 = None\n",
      "        _unsafe_view_5: f16[16, 256, 160] = torch.ops.aten._unsafe_view.default(clone_2, [16, 256, 160]);  clone_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty: f16[16, 256, 256] = torch.ops.aten.empty.memory_format([16, 256, 256], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose: f16[16, 160, 256] = torch.ops.aten.transpose.int(_unsafe_view_4, -1, -2);  _unsafe_view_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm: f16[16, 256, 256] = torch.ops.aten.baddbmm.default(empty, _unsafe_view_1, transpose, beta = 0, alpha = 0.07905694150420949);  empty = _unsafe_view_1 = transpose = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax: f16[16, 256, 256] = torch.ops.aten._softmax.default(baddbmm, -1, False);  baddbmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm: f16[16, 256, 160] = torch.ops.aten.bmm.default(_softmax, _unsafe_view_5);  _softmax = _unsafe_view_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_7: f16[2, 8, 256, 160] = torch.ops.aten.view.default(bmm, [2, 8, 256, 160]);  bmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_4: f16[2, 256, 8, 160] = torch.ops.aten.permute.default(view_7, [0, 2, 1, 3]);  view_7 = None\n",
      "        clone_3: f16[2, 256, 8, 160] = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None\n",
      "        _unsafe_view_6: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(clone_3, [2, 256, 1280]);  clone_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_3: f16[1280, 1280] = torch.ops.aten.t.default(arg9_1);  arg9_1 = None\n",
      "        view_8: f16[512, 1280] = torch.ops.aten.view.default(_unsafe_view_6, [512, 1280]);  _unsafe_view_6 = None\n",
      "        addmm: f16[512, 1280] = torch.ops.aten.addmm.default(arg10_1, view_8, t_3);  arg10_1 = view_8 = t_3 = None\n",
      "        view_9: f16[2, 256, 1280] = torch.ops.aten.view.default(addmm, [2, 256, 1280]);  addmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:490, code: hidden_states = self.attn1(norm_hidden_states, attention_mask=attention_mask) + hidden_states\n",
      "        add: f16[2, 256, 1280] = torch.ops.aten.add.Tensor(view_9, view);  view_9 = view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:495, code: self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)\n",
      "        native_layer_norm_1 = torch.ops.aten.native_layer_norm.default(add, [1280], arg11_1, arg12_1, 1e-05);  arg11_1 = arg12_1 = None\n",
      "        getitem_6: f16[2, 256, 1280] = native_layer_norm_1[0]\n",
      "        getitem_7: f32[2, 256, 1] = native_layer_norm_1[1]\n",
      "        getitem_8: f32[2, 256, 1] = native_layer_norm_1[2];  native_layer_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t_4: f16[1280, 1280] = torch.ops.aten.t.default(arg13_1);  arg13_1 = None\n",
      "        view_10: f16[512, 1280] = torch.ops.aten.view.default(getitem_6, [512, 1280]);  getitem_6 = None\n",
      "        mm_3: f16[512, 1280] = torch.ops.aten.mm.default(view_10, t_4);  view_10 = t_4 = None\n",
      "        _unsafe_view_7: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(mm_3, [2, 256, 1280]);  mm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_11: f16[2, 256, 8, 160] = torch.ops.aten.view.default(_unsafe_view_7, [2, 256, 8, 160]);  _unsafe_view_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_5: f16[2, 8, 256, 160] = torch.ops.aten.permute.default(view_11, [0, 2, 1, 3]);  view_11 = None\n",
      "        clone_4: f16[2, 8, 256, 160] = torch.ops.aten.clone.default(permute_5, memory_format = torch.contiguous_format);  permute_5 = None\n",
      "        _unsafe_view_8: f16[16, 256, 160] = torch.ops.aten._unsafe_view.default(clone_4, [16, 256, 160]);  clone_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_5: f16[768, 1280] = torch.ops.aten.t.default(arg14_1);  arg14_1 = None\n",
      "        view_12: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768])\n",
      "        mm_4: f16[154, 1280] = torch.ops.aten.mm.default(view_12, t_5);  view_12 = t_5 = None\n",
      "        _unsafe_view_9: f16[2, 77, 1280] = torch.ops.aten._unsafe_view.default(mm_4, [2, 77, 1280]);  mm_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_6: f16[768, 1280] = torch.ops.aten.t.default(arg15_1);  arg15_1 = None\n",
      "        view_13: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768]);  arg27_1 = None\n",
      "        mm_5: f16[154, 1280] = torch.ops.aten.mm.default(view_13, t_6);  view_13 = t_6 = None\n",
      "        _unsafe_view_10: f16[2, 77, 1280] = torch.ops.aten._unsafe_view.default(mm_5, [2, 77, 1280]);  mm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_14: f16[2, 77, 8, 160] = torch.ops.aten.view.default(_unsafe_view_9, [2, 77, 8, 160]);  _unsafe_view_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_6: f16[2, 8, 77, 160] = torch.ops.aten.permute.default(view_14, [0, 2, 1, 3]);  view_14 = None\n",
      "        clone_5: f16[2, 8, 77, 160] = torch.ops.aten.clone.default(permute_6, memory_format = torch.contiguous_format);  permute_6 = None\n",
      "        _unsafe_view_11: f16[16, 77, 160] = torch.ops.aten._unsafe_view.default(clone_5, [16, 77, 160]);  clone_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_15: f16[2, 77, 8, 160] = torch.ops.aten.view.default(_unsafe_view_10, [2, 77, 8, 160]);  _unsafe_view_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_7: f16[2, 8, 77, 160] = torch.ops.aten.permute.default(view_15, [0, 2, 1, 3]);  view_15 = None\n",
      "        clone_6: f16[2, 8, 77, 160] = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None\n",
      "        _unsafe_view_12: f16[16, 77, 160] = torch.ops.aten._unsafe_view.default(clone_6, [16, 77, 160]);  clone_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty_1: f16[16, 256, 77] = torch.ops.aten.empty.memory_format([16, 256, 77], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose_1: f16[16, 160, 77] = torch.ops.aten.transpose.int(_unsafe_view_11, -1, -2);  _unsafe_view_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm_1: f16[16, 256, 77] = torch.ops.aten.baddbmm.default(empty_1, _unsafe_view_8, transpose_1, beta = 0, alpha = 0.07905694150420949);  empty_1 = _unsafe_view_8 = transpose_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax_1: f16[16, 256, 77] = torch.ops.aten._softmax.default(baddbmm_1, -1, False);  baddbmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm_1: f16[16, 256, 160] = torch.ops.aten.bmm.default(_softmax_1, _unsafe_view_12);  _softmax_1 = _unsafe_view_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_16: f16[2, 8, 256, 160] = torch.ops.aten.view.default(bmm_1, [2, 8, 256, 160]);  bmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_8: f16[2, 256, 8, 160] = torch.ops.aten.permute.default(view_16, [0, 2, 1, 3]);  view_16 = None\n",
      "        clone_7: f16[2, 256, 8, 160] = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None\n",
      "        _unsafe_view_13: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(clone_7, [2, 256, 1280]);  clone_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_7: f16[1280, 1280] = torch.ops.aten.t.default(arg16_1);  arg16_1 = None\n",
      "        view_17: f16[512, 1280] = torch.ops.aten.view.default(_unsafe_view_13, [512, 1280]);  _unsafe_view_13 = None\n",
      "        addmm_1: f16[512, 1280] = torch.ops.aten.addmm.default(arg17_1, view_17, t_7);  arg17_1 = view_17 = t_7 = None\n",
      "        view_18: f16[2, 256, 1280] = torch.ops.aten.view.default(addmm_1, [2, 256, 1280]);  addmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:498, code: self.attn2(\n",
      "        add_1: f16[2, 256, 1280] = torch.ops.aten.add.Tensor(view_18, add);  view_18 = add = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        native_layer_norm_2 = torch.ops.aten.native_layer_norm.default(add_1, [1280], arg18_1, arg19_1, 1e-05);  arg18_1 = arg19_1 = None\n",
      "        getitem_9: f16[2, 256, 1280] = native_layer_norm_2[0]\n",
      "        getitem_10: f32[2, 256, 1] = native_layer_norm_2[1]\n",
      "        getitem_11: f32[2, 256, 1] = native_layer_norm_2[2];  native_layer_norm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:820, code: hidden_states, gate = self.proj(hidden_states).chunk(2, dim=-1)\n",
      "        t_8: f16[1280, 10240] = torch.ops.aten.t.default(arg20_1);  arg20_1 = None\n",
      "        view_19: f16[512, 1280] = torch.ops.aten.view.default(getitem_9, [512, 1280]);  getitem_9 = None\n",
      "        addmm_2: f16[512, 10240] = torch.ops.aten.addmm.default(arg21_1, view_19, t_8);  arg21_1 = view_19 = t_8 = None\n",
      "        view_20: f16[2, 256, 10240] = torch.ops.aten.view.default(addmm_2, [2, 256, 10240]);  addmm_2 = None\n",
      "        split = torch.ops.aten.split.Tensor(view_20, 5120, -1);  view_20 = None\n",
      "        getitem_12: f16[2, 256, 5120] = split[0]\n",
      "        getitem_13: f16[2, 256, 5120] = split[1];  split = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:815, code: return F.gelu(gate)\n",
      "        gelu: f16[2, 256, 5120] = torch.ops.aten.gelu.default(getitem_13);  getitem_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:821, code: return hidden_states * self.gelu(gate)\n",
      "        mul: f16[2, 256, 5120] = torch.ops.aten.mul.Tensor(getitem_12, gelu);  getitem_12 = gelu = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:774, code: hidden_states = module(hidden_states)\n",
      "        t_9: f16[5120, 1280] = torch.ops.aten.t.default(arg22_1);  arg22_1 = None\n",
      "        view_21: f16[512, 5120] = torch.ops.aten.view.default(mul, [512, 5120]);  mul = None\n",
      "        addmm_3: f16[512, 1280] = torch.ops.aten.addmm.default(arg23_1, view_21, t_9);  arg23_1 = view_21 = t_9 = None\n",
      "        view_22: f16[2, 256, 1280] = torch.ops.aten.view.default(addmm_3, [2, 256, 1280]);  addmm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        add_2: f16[2, 256, 1280] = torch.ops.aten.add.Tensor(view_22, add_1);  view_22 = add_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:222, code: hidden_states.reshape(batch, height, weight, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
      "        view_23: f16[2, 16, 16, 1280] = torch.ops.aten.view.default(add_2, [2, 16, 16, 1280]);  add_2 = None\n",
      "        permute_9: f16[2, 1280, 16, 16] = torch.ops.aten.permute.default(view_23, [0, 3, 1, 2]);  view_23 = None\n",
      "        clone_8: f16[2, 1280, 16, 16] = torch.ops.aten.clone.default(permute_9, memory_format = torch.contiguous_format);  permute_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:224, code: hidden_states = self.proj_out(hidden_states)\n",
      "        convolution_1: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(clone_8, arg24_1, arg25_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  clone_8 = arg24_1 = arg25_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:231, code: output = hidden_states + residual\n",
      "        add_3: f16[2, 1280, 16, 16] = torch.ops.aten.add.Tensor(convolution_1, arg26_1);  convolution_1 = arg26_1 = None\n",
      "        return (add_3,)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1920], arg1_1: f16[1920], arg2_1: f16[1280, 1920, 3, 3], arg3_1: f16[1280], arg4_1: f16[1280, 1280], arg5_1: f16[1280], arg6_1: f16[1280], arg7_1: f16[1280], arg8_1: f16[1280, 1280, 3, 3], arg9_1: f16[1280], arg10_1: f16[1280, 1920, 1, 1], arg11_1: f16[1280], arg12_1: f16[2, 1920, 16, 16], arg13_1: f16[2, 1280]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg12_1, arg0_1, arg1_1, 2, 1920, 256, 32, 1e-05);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 1920, 16, 16] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu: f16[2, 1920, 16, 16] = torch.ops.aten.silu.default(getitem);  getitem = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(silu, arg2_1, arg3_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:474, code: temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
      "        silu_1: f16[2, 1280] = torch.ops.aten.silu.default(arg13_1);  arg13_1 = None\n",
      "        t: f16[1280, 1280] = torch.ops.aten.t.default(arg4_1);  arg4_1 = None\n",
      "        addmm: f16[2, 1280] = torch.ops.aten.addmm.default(arg5_1, silu_1, t);  arg5_1 = silu_1 = t = None\n",
      "        slice_1: f16[2, 1280] = torch.ops.aten.slice.Tensor(addmm, 0, 0, 9223372036854775807);  addmm = None\n",
      "        slice_2: f16[2, 1280] = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807);  slice_1 = None\n",
      "        unsqueeze: f16[2, 1280, 1] = torch.ops.aten.unsqueeze.default(slice_2, 2);  slice_2 = None\n",
      "        unsqueeze_1: f16[2, 1280, 1, 1] = torch.ops.aten.unsqueeze.default(unsqueeze, 3);  unsqueeze = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:477, code: hidden_states = hidden_states + temb\n",
      "        add: f16[2, 1280, 16, 16] = torch.ops.aten.add.Tensor(convolution, unsqueeze_1);  convolution = unsqueeze_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_1 = torch.ops.aten.native_group_norm.default(add, arg6_1, arg7_1, 2, 1280, 256, 32, 1e-05);  add = arg6_1 = arg7_1 = None\n",
      "        getitem_3: f16[2, 1280, 16, 16] = native_group_norm_1[0]\n",
      "        getitem_4: f16[2, 32] = native_group_norm_1[1]\n",
      "        getitem_5: f16[2, 32] = native_group_norm_1[2];  native_group_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_2: f16[2, 1280, 16, 16] = torch.ops.aten.silu.default(getitem_3);  getitem_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_1: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(silu_2, arg8_1, arg9_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_2 = arg8_1 = arg9_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:491, code: input_tensor = self.conv_shortcut(input_tensor)\n",
      "        convolution_2: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(arg12_1, arg10_1, arg11_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  arg12_1 = arg10_1 = arg11_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_1: f16[2, 1280, 16, 16] = torch.ops.aten.add.Tensor(convolution_2, convolution_1);  convolution_2 = convolution_1 = None\n",
      "        div: f16[2, 1280, 16, 16] = torch.ops.aten.div.Tensor(add_1, 1.0);  add_1 = None\n",
      "        return (div,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1280], arg1_1: f16[1280], arg2_1: f16[1280, 1280, 1, 1], arg3_1: f16[1280], arg4_1: f16[1280], arg5_1: f16[1280], arg6_1: f16[1280, 1280], arg7_1: f16[1280, 1280], arg8_1: f16[1280, 1280], arg9_1: f16[1280, 1280], arg10_1: f16[1280], arg11_1: f16[1280], arg12_1: f16[1280], arg13_1: f16[1280, 1280], arg14_1: f16[1280, 768], arg15_1: f16[1280, 768], arg16_1: f16[1280, 1280], arg17_1: f16[1280], arg18_1: f16[1280], arg19_1: f16[1280], arg20_1: f16[10240, 1280], arg21_1: f16[10240], arg22_1: f16[1280, 5120], arg23_1: f16[1280], arg24_1: f16[1280, 1280, 1, 1], arg25_1: f16[1280], arg26_1: f16[2, 1280, 16, 16], arg27_1: f16[2, 77, 768]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:202, code: hidden_states = self.norm(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg26_1, arg0_1, arg1_1, 2, 1280, 256, 32, 1e-06);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 1280, 16, 16] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:204, code: hidden_states = self.proj_in(hidden_states)\n",
      "        convolution: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(getitem, arg2_1, arg3_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  getitem = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:206, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * weight, inner_dim)\n",
      "        permute: f16[2, 16, 16, 1280] = torch.ops.aten.permute.default(convolution, [0, 2, 3, 1]);  convolution = None\n",
      "        view: f16[2, 256, 1280] = torch.ops.aten.view.default(permute, [2, 256, 1280]);  permute = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:482, code: self.norm1(hidden_states, timestep) if self.use_ada_layer_norm else self.norm1(hidden_states)\n",
      "        native_layer_norm = torch.ops.aten.native_layer_norm.default(view, [1280], arg4_1, arg5_1, 1e-05);  arg4_1 = arg5_1 = None\n",
      "        getitem_3: f16[2, 256, 1280] = native_layer_norm[0]\n",
      "        getitem_4: f32[2, 256, 1] = native_layer_norm[1]\n",
      "        getitem_5: f32[2, 256, 1] = native_layer_norm[2];  native_layer_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t: f16[1280, 1280] = torch.ops.aten.t.default(arg6_1);  arg6_1 = None\n",
      "        view_1: f16[512, 1280] = torch.ops.aten.view.default(getitem_3, [512, 1280])\n",
      "        mm: f16[512, 1280] = torch.ops.aten.mm.default(view_1, t);  view_1 = t = None\n",
      "        _unsafe_view: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(mm, [2, 256, 1280]);  mm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_2: f16[2, 256, 8, 160] = torch.ops.aten.view.default(_unsafe_view, [2, 256, 8, 160]);  _unsafe_view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_1: f16[2, 8, 256, 160] = torch.ops.aten.permute.default(view_2, [0, 2, 1, 3]);  view_2 = None\n",
      "        clone: f16[2, 8, 256, 160] = torch.ops.aten.clone.default(permute_1, memory_format = torch.contiguous_format);  permute_1 = None\n",
      "        _unsafe_view_1: f16[16, 256, 160] = torch.ops.aten._unsafe_view.default(clone, [16, 256, 160]);  clone = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_1: f16[1280, 1280] = torch.ops.aten.t.default(arg7_1);  arg7_1 = None\n",
      "        view_3: f16[512, 1280] = torch.ops.aten.view.default(getitem_3, [512, 1280])\n",
      "        mm_1: f16[512, 1280] = torch.ops.aten.mm.default(view_3, t_1);  view_3 = t_1 = None\n",
      "        _unsafe_view_2: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(mm_1, [2, 256, 1280]);  mm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_2: f16[1280, 1280] = torch.ops.aten.t.default(arg8_1);  arg8_1 = None\n",
      "        view_4: f16[512, 1280] = torch.ops.aten.view.default(getitem_3, [512, 1280]);  getitem_3 = None\n",
      "        mm_2: f16[512, 1280] = torch.ops.aten.mm.default(view_4, t_2);  view_4 = t_2 = None\n",
      "        _unsafe_view_3: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(mm_2, [2, 256, 1280]);  mm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_5: f16[2, 256, 8, 160] = torch.ops.aten.view.default(_unsafe_view_2, [2, 256, 8, 160]);  _unsafe_view_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_2: f16[2, 8, 256, 160] = torch.ops.aten.permute.default(view_5, [0, 2, 1, 3]);  view_5 = None\n",
      "        clone_1: f16[2, 8, 256, 160] = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None\n",
      "        _unsafe_view_4: f16[16, 256, 160] = torch.ops.aten._unsafe_view.default(clone_1, [16, 256, 160]);  clone_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_6: f16[2, 256, 8, 160] = torch.ops.aten.view.default(_unsafe_view_3, [2, 256, 8, 160]);  _unsafe_view_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_3: f16[2, 8, 256, 160] = torch.ops.aten.permute.default(view_6, [0, 2, 1, 3]);  view_6 = None\n",
      "        clone_2: f16[2, 8, 256, 160] = torch.ops.aten.clone.default(permute_3, memory_format = torch.contiguous_format);  permute_3 = None\n",
      "        _unsafe_view_5: f16[16, 256, 160] = torch.ops.aten._unsafe_view.default(clone_2, [16, 256, 160]);  clone_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty: f16[16, 256, 256] = torch.ops.aten.empty.memory_format([16, 256, 256], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose: f16[16, 160, 256] = torch.ops.aten.transpose.int(_unsafe_view_4, -1, -2);  _unsafe_view_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm: f16[16, 256, 256] = torch.ops.aten.baddbmm.default(empty, _unsafe_view_1, transpose, beta = 0, alpha = 0.07905694150420949);  empty = _unsafe_view_1 = transpose = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax: f16[16, 256, 256] = torch.ops.aten._softmax.default(baddbmm, -1, False);  baddbmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm: f16[16, 256, 160] = torch.ops.aten.bmm.default(_softmax, _unsafe_view_5);  _softmax = _unsafe_view_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_7: f16[2, 8, 256, 160] = torch.ops.aten.view.default(bmm, [2, 8, 256, 160]);  bmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_4: f16[2, 256, 8, 160] = torch.ops.aten.permute.default(view_7, [0, 2, 1, 3]);  view_7 = None\n",
      "        clone_3: f16[2, 256, 8, 160] = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None\n",
      "        _unsafe_view_6: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(clone_3, [2, 256, 1280]);  clone_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_3: f16[1280, 1280] = torch.ops.aten.t.default(arg9_1);  arg9_1 = None\n",
      "        view_8: f16[512, 1280] = torch.ops.aten.view.default(_unsafe_view_6, [512, 1280]);  _unsafe_view_6 = None\n",
      "        addmm: f16[512, 1280] = torch.ops.aten.addmm.default(arg10_1, view_8, t_3);  arg10_1 = view_8 = t_3 = None\n",
      "        view_9: f16[2, 256, 1280] = torch.ops.aten.view.default(addmm, [2, 256, 1280]);  addmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:490, code: hidden_states = self.attn1(norm_hidden_states, attention_mask=attention_mask) + hidden_states\n",
      "        add: f16[2, 256, 1280] = torch.ops.aten.add.Tensor(view_9, view);  view_9 = view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:495, code: self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)\n",
      "        native_layer_norm_1 = torch.ops.aten.native_layer_norm.default(add, [1280], arg11_1, arg12_1, 1e-05);  arg11_1 = arg12_1 = None\n",
      "        getitem_6: f16[2, 256, 1280] = native_layer_norm_1[0]\n",
      "        getitem_7: f32[2, 256, 1] = native_layer_norm_1[1]\n",
      "        getitem_8: f32[2, 256, 1] = native_layer_norm_1[2];  native_layer_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t_4: f16[1280, 1280] = torch.ops.aten.t.default(arg13_1);  arg13_1 = None\n",
      "        view_10: f16[512, 1280] = torch.ops.aten.view.default(getitem_6, [512, 1280]);  getitem_6 = None\n",
      "        mm_3: f16[512, 1280] = torch.ops.aten.mm.default(view_10, t_4);  view_10 = t_4 = None\n",
      "        _unsafe_view_7: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(mm_3, [2, 256, 1280]);  mm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_11: f16[2, 256, 8, 160] = torch.ops.aten.view.default(_unsafe_view_7, [2, 256, 8, 160]);  _unsafe_view_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_5: f16[2, 8, 256, 160] = torch.ops.aten.permute.default(view_11, [0, 2, 1, 3]);  view_11 = None\n",
      "        clone_4: f16[2, 8, 256, 160] = torch.ops.aten.clone.default(permute_5, memory_format = torch.contiguous_format);  permute_5 = None\n",
      "        _unsafe_view_8: f16[16, 256, 160] = torch.ops.aten._unsafe_view.default(clone_4, [16, 256, 160]);  clone_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_5: f16[768, 1280] = torch.ops.aten.t.default(arg14_1);  arg14_1 = None\n",
      "        view_12: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768])\n",
      "        mm_4: f16[154, 1280] = torch.ops.aten.mm.default(view_12, t_5);  view_12 = t_5 = None\n",
      "        _unsafe_view_9: f16[2, 77, 1280] = torch.ops.aten._unsafe_view.default(mm_4, [2, 77, 1280]);  mm_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_6: f16[768, 1280] = torch.ops.aten.t.default(arg15_1);  arg15_1 = None\n",
      "        view_13: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768]);  arg27_1 = None\n",
      "        mm_5: f16[154, 1280] = torch.ops.aten.mm.default(view_13, t_6);  view_13 = t_6 = None\n",
      "        _unsafe_view_10: f16[2, 77, 1280] = torch.ops.aten._unsafe_view.default(mm_5, [2, 77, 1280]);  mm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_14: f16[2, 77, 8, 160] = torch.ops.aten.view.default(_unsafe_view_9, [2, 77, 8, 160]);  _unsafe_view_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_6: f16[2, 8, 77, 160] = torch.ops.aten.permute.default(view_14, [0, 2, 1, 3]);  view_14 = None\n",
      "        clone_5: f16[2, 8, 77, 160] = torch.ops.aten.clone.default(permute_6, memory_format = torch.contiguous_format);  permute_6 = None\n",
      "        _unsafe_view_11: f16[16, 77, 160] = torch.ops.aten._unsafe_view.default(clone_5, [16, 77, 160]);  clone_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_15: f16[2, 77, 8, 160] = torch.ops.aten.view.default(_unsafe_view_10, [2, 77, 8, 160]);  _unsafe_view_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_7: f16[2, 8, 77, 160] = torch.ops.aten.permute.default(view_15, [0, 2, 1, 3]);  view_15 = None\n",
      "        clone_6: f16[2, 8, 77, 160] = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None\n",
      "        _unsafe_view_12: f16[16, 77, 160] = torch.ops.aten._unsafe_view.default(clone_6, [16, 77, 160]);  clone_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty_1: f16[16, 256, 77] = torch.ops.aten.empty.memory_format([16, 256, 77], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose_1: f16[16, 160, 77] = torch.ops.aten.transpose.int(_unsafe_view_11, -1, -2);  _unsafe_view_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm_1: f16[16, 256, 77] = torch.ops.aten.baddbmm.default(empty_1, _unsafe_view_8, transpose_1, beta = 0, alpha = 0.07905694150420949);  empty_1 = _unsafe_view_8 = transpose_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax_1: f16[16, 256, 77] = torch.ops.aten._softmax.default(baddbmm_1, -1, False);  baddbmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm_1: f16[16, 256, 160] = torch.ops.aten.bmm.default(_softmax_1, _unsafe_view_12);  _softmax_1 = _unsafe_view_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_16: f16[2, 8, 256, 160] = torch.ops.aten.view.default(bmm_1, [2, 8, 256, 160]);  bmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_8: f16[2, 256, 8, 160] = torch.ops.aten.permute.default(view_16, [0, 2, 1, 3]);  view_16 = None\n",
      "        clone_7: f16[2, 256, 8, 160] = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None\n",
      "        _unsafe_view_13: f16[2, 256, 1280] = torch.ops.aten._unsafe_view.default(clone_7, [2, 256, 1280]);  clone_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_7: f16[1280, 1280] = torch.ops.aten.t.default(arg16_1);  arg16_1 = None\n",
      "        view_17: f16[512, 1280] = torch.ops.aten.view.default(_unsafe_view_13, [512, 1280]);  _unsafe_view_13 = None\n",
      "        addmm_1: f16[512, 1280] = torch.ops.aten.addmm.default(arg17_1, view_17, t_7);  arg17_1 = view_17 = t_7 = None\n",
      "        view_18: f16[2, 256, 1280] = torch.ops.aten.view.default(addmm_1, [2, 256, 1280]);  addmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:498, code: self.attn2(\n",
      "        add_1: f16[2, 256, 1280] = torch.ops.aten.add.Tensor(view_18, add);  view_18 = add = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        native_layer_norm_2 = torch.ops.aten.native_layer_norm.default(add_1, [1280], arg18_1, arg19_1, 1e-05);  arg18_1 = arg19_1 = None\n",
      "        getitem_9: f16[2, 256, 1280] = native_layer_norm_2[0]\n",
      "        getitem_10: f32[2, 256, 1] = native_layer_norm_2[1]\n",
      "        getitem_11: f32[2, 256, 1] = native_layer_norm_2[2];  native_layer_norm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:820, code: hidden_states, gate = self.proj(hidden_states).chunk(2, dim=-1)\n",
      "        t_8: f16[1280, 10240] = torch.ops.aten.t.default(arg20_1);  arg20_1 = None\n",
      "        view_19: f16[512, 1280] = torch.ops.aten.view.default(getitem_9, [512, 1280]);  getitem_9 = None\n",
      "        addmm_2: f16[512, 10240] = torch.ops.aten.addmm.default(arg21_1, view_19, t_8);  arg21_1 = view_19 = t_8 = None\n",
      "        view_20: f16[2, 256, 10240] = torch.ops.aten.view.default(addmm_2, [2, 256, 10240]);  addmm_2 = None\n",
      "        split = torch.ops.aten.split.Tensor(view_20, 5120, -1);  view_20 = None\n",
      "        getitem_12: f16[2, 256, 5120] = split[0]\n",
      "        getitem_13: f16[2, 256, 5120] = split[1];  split = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:815, code: return F.gelu(gate)\n",
      "        gelu: f16[2, 256, 5120] = torch.ops.aten.gelu.default(getitem_13);  getitem_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:821, code: return hidden_states * self.gelu(gate)\n",
      "        mul: f16[2, 256, 5120] = torch.ops.aten.mul.Tensor(getitem_12, gelu);  getitem_12 = gelu = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:774, code: hidden_states = module(hidden_states)\n",
      "        t_9: f16[5120, 1280] = torch.ops.aten.t.default(arg22_1);  arg22_1 = None\n",
      "        view_21: f16[512, 5120] = torch.ops.aten.view.default(mul, [512, 5120]);  mul = None\n",
      "        addmm_3: f16[512, 1280] = torch.ops.aten.addmm.default(arg23_1, view_21, t_9);  arg23_1 = view_21 = t_9 = None\n",
      "        view_22: f16[2, 256, 1280] = torch.ops.aten.view.default(addmm_3, [2, 256, 1280]);  addmm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        add_2: f16[2, 256, 1280] = torch.ops.aten.add.Tensor(view_22, add_1);  view_22 = add_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:222, code: hidden_states.reshape(batch, height, weight, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
      "        view_23: f16[2, 16, 16, 1280] = torch.ops.aten.view.default(add_2, [2, 16, 16, 1280]);  add_2 = None\n",
      "        permute_9: f16[2, 1280, 16, 16] = torch.ops.aten.permute.default(view_23, [0, 3, 1, 2]);  view_23 = None\n",
      "        clone_8: f16[2, 1280, 16, 16] = torch.ops.aten.clone.default(permute_9, memory_format = torch.contiguous_format);  permute_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:224, code: hidden_states = self.proj_out(hidden_states)\n",
      "        convolution_1: f16[2, 1280, 16, 16] = torch.ops.aten.convolution.default(clone_8, arg24_1, arg25_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  clone_8 = arg24_1 = arg25_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:231, code: output = hidden_states + residual\n",
      "        add_3: f16[2, 1280, 16, 16] = torch.ops.aten.add.Tensor(convolution_1, arg26_1);  convolution_1 = arg26_1 = None\n",
      "        return (add_3,)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1280, 1280, 3, 3], arg1_1: f16[1280], arg2_1: f16[2, 1280, 16, 16]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:128, code: hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n",
      "        upsample_nearest2d: f16[2, 1280, 32, 32] = torch.ops.aten.upsample_nearest2d.default(arg2_1, [32, 32], 2.0, 2.0);  arg2_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:139, code: hidden_states = self.conv(hidden_states)\n",
      "        convolution: f16[2, 1280, 32, 32] = torch.ops.aten.convolution.default(upsample_nearest2d, arg0_1, arg1_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  upsample_nearest2d = arg0_1 = arg1_1 = None\n",
      "        return (convolution,)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1920], arg1_1: f16[1920], arg2_1: f16[640, 1920, 3, 3], arg3_1: f16[640], arg4_1: f16[640, 1280], arg5_1: f16[640], arg6_1: f16[640], arg7_1: f16[640], arg8_1: f16[640, 640, 3, 3], arg9_1: f16[640], arg10_1: f16[640, 1920, 1, 1], arg11_1: f16[640], arg12_1: f16[2, 1920, 32, 32], arg13_1: f16[2, 1280]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg12_1, arg0_1, arg1_1, 2, 1920, 1024, 32, 1e-05);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 1920, 32, 32] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu: f16[2, 1920, 32, 32] = torch.ops.aten.silu.default(getitem);  getitem = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(silu, arg2_1, arg3_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:474, code: temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
      "        silu_1: f16[2, 1280] = torch.ops.aten.silu.default(arg13_1);  arg13_1 = None\n",
      "        t: f16[1280, 640] = torch.ops.aten.t.default(arg4_1);  arg4_1 = None\n",
      "        addmm: f16[2, 640] = torch.ops.aten.addmm.default(arg5_1, silu_1, t);  arg5_1 = silu_1 = t = None\n",
      "        slice_1: f16[2, 640] = torch.ops.aten.slice.Tensor(addmm, 0, 0, 9223372036854775807);  addmm = None\n",
      "        slice_2: f16[2, 640] = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807);  slice_1 = None\n",
      "        unsqueeze: f16[2, 640, 1] = torch.ops.aten.unsqueeze.default(slice_2, 2);  slice_2 = None\n",
      "        unsqueeze_1: f16[2, 640, 1, 1] = torch.ops.aten.unsqueeze.default(unsqueeze, 3);  unsqueeze = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:477, code: hidden_states = hidden_states + temb\n",
      "        add: f16[2, 640, 32, 32] = torch.ops.aten.add.Tensor(convolution, unsqueeze_1);  convolution = unsqueeze_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_1 = torch.ops.aten.native_group_norm.default(add, arg6_1, arg7_1, 2, 640, 1024, 32, 1e-05);  add = arg6_1 = arg7_1 = None\n",
      "        getitem_3: f16[2, 640, 32, 32] = native_group_norm_1[0]\n",
      "        getitem_4: f16[2, 32] = native_group_norm_1[1]\n",
      "        getitem_5: f16[2, 32] = native_group_norm_1[2];  native_group_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_2: f16[2, 640, 32, 32] = torch.ops.aten.silu.default(getitem_3);  getitem_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_1: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(silu_2, arg8_1, arg9_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_2 = arg8_1 = arg9_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:491, code: input_tensor = self.conv_shortcut(input_tensor)\n",
      "        convolution_2: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(arg12_1, arg10_1, arg11_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  arg12_1 = arg10_1 = arg11_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_1: f16[2, 640, 32, 32] = torch.ops.aten.add.Tensor(convolution_2, convolution_1);  convolution_2 = convolution_1 = None\n",
      "        div: f16[2, 640, 32, 32] = torch.ops.aten.div.Tensor(add_1, 1.0);  add_1 = None\n",
      "        return (div,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[640], arg1_1: f16[640], arg2_1: f16[640, 640, 1, 1], arg3_1: f16[640], arg4_1: f16[640], arg5_1: f16[640], arg6_1: f16[640, 640], arg7_1: f16[640, 640], arg8_1: f16[640, 640], arg9_1: f16[640, 640], arg10_1: f16[640], arg11_1: f16[640], arg12_1: f16[640], arg13_1: f16[640, 640], arg14_1: f16[640, 768], arg15_1: f16[640, 768], arg16_1: f16[640, 640], arg17_1: f16[640], arg18_1: f16[640], arg19_1: f16[640], arg20_1: f16[5120, 640], arg21_1: f16[5120], arg22_1: f16[640, 2560], arg23_1: f16[640], arg24_1: f16[640, 640, 1, 1], arg25_1: f16[640], arg26_1: f16[2, 640, 32, 32], arg27_1: f16[2, 77, 768]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:202, code: hidden_states = self.norm(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg26_1, arg0_1, arg1_1, 2, 640, 1024, 32, 1e-06);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 640, 32, 32] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:204, code: hidden_states = self.proj_in(hidden_states)\n",
      "        convolution: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(getitem, arg2_1, arg3_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  getitem = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:206, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * weight, inner_dim)\n",
      "        permute: f16[2, 32, 32, 640] = torch.ops.aten.permute.default(convolution, [0, 2, 3, 1]);  convolution = None\n",
      "        view: f16[2, 1024, 640] = torch.ops.aten.view.default(permute, [2, 1024, 640]);  permute = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:482, code: self.norm1(hidden_states, timestep) if self.use_ada_layer_norm else self.norm1(hidden_states)\n",
      "        native_layer_norm = torch.ops.aten.native_layer_norm.default(view, [640], arg4_1, arg5_1, 1e-05);  arg4_1 = arg5_1 = None\n",
      "        getitem_3: f16[2, 1024, 640] = native_layer_norm[0]\n",
      "        getitem_4: f32[2, 1024, 1] = native_layer_norm[1]\n",
      "        getitem_5: f32[2, 1024, 1] = native_layer_norm[2];  native_layer_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t: f16[640, 640] = torch.ops.aten.t.default(arg6_1);  arg6_1 = None\n",
      "        view_1: f16[2048, 640] = torch.ops.aten.view.default(getitem_3, [2048, 640])\n",
      "        mm: f16[2048, 640] = torch.ops.aten.mm.default(view_1, t);  view_1 = t = None\n",
      "        _unsafe_view: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(mm, [2, 1024, 640]);  mm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_2: f16[2, 1024, 8, 80] = torch.ops.aten.view.default(_unsafe_view, [2, 1024, 8, 80]);  _unsafe_view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_1: f16[2, 8, 1024, 80] = torch.ops.aten.permute.default(view_2, [0, 2, 1, 3]);  view_2 = None\n",
      "        clone: f16[2, 8, 1024, 80] = torch.ops.aten.clone.default(permute_1, memory_format = torch.contiguous_format);  permute_1 = None\n",
      "        _unsafe_view_1: f16[16, 1024, 80] = torch.ops.aten._unsafe_view.default(clone, [16, 1024, 80]);  clone = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_1: f16[640, 640] = torch.ops.aten.t.default(arg7_1);  arg7_1 = None\n",
      "        view_3: f16[2048, 640] = torch.ops.aten.view.default(getitem_3, [2048, 640])\n",
      "        mm_1: f16[2048, 640] = torch.ops.aten.mm.default(view_3, t_1);  view_3 = t_1 = None\n",
      "        _unsafe_view_2: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(mm_1, [2, 1024, 640]);  mm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_2: f16[640, 640] = torch.ops.aten.t.default(arg8_1);  arg8_1 = None\n",
      "        view_4: f16[2048, 640] = torch.ops.aten.view.default(getitem_3, [2048, 640]);  getitem_3 = None\n",
      "        mm_2: f16[2048, 640] = torch.ops.aten.mm.default(view_4, t_2);  view_4 = t_2 = None\n",
      "        _unsafe_view_3: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(mm_2, [2, 1024, 640]);  mm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_5: f16[2, 1024, 8, 80] = torch.ops.aten.view.default(_unsafe_view_2, [2, 1024, 8, 80]);  _unsafe_view_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_2: f16[2, 8, 1024, 80] = torch.ops.aten.permute.default(view_5, [0, 2, 1, 3]);  view_5 = None\n",
      "        clone_1: f16[2, 8, 1024, 80] = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None\n",
      "        _unsafe_view_4: f16[16, 1024, 80] = torch.ops.aten._unsafe_view.default(clone_1, [16, 1024, 80]);  clone_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_6: f16[2, 1024, 8, 80] = torch.ops.aten.view.default(_unsafe_view_3, [2, 1024, 8, 80]);  _unsafe_view_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_3: f16[2, 8, 1024, 80] = torch.ops.aten.permute.default(view_6, [0, 2, 1, 3]);  view_6 = None\n",
      "        clone_2: f16[2, 8, 1024, 80] = torch.ops.aten.clone.default(permute_3, memory_format = torch.contiguous_format);  permute_3 = None\n",
      "        _unsafe_view_5: f16[16, 1024, 80] = torch.ops.aten._unsafe_view.default(clone_2, [16, 1024, 80]);  clone_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty: f16[16, 1024, 1024] = torch.ops.aten.empty.memory_format([16, 1024, 1024], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose: f16[16, 80, 1024] = torch.ops.aten.transpose.int(_unsafe_view_4, -1, -2);  _unsafe_view_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm: f16[16, 1024, 1024] = torch.ops.aten.baddbmm.default(empty, _unsafe_view_1, transpose, beta = 0, alpha = 0.11180339887498948);  empty = _unsafe_view_1 = transpose = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax: f16[16, 1024, 1024] = torch.ops.aten._softmax.default(baddbmm, -1, False);  baddbmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm: f16[16, 1024, 80] = torch.ops.aten.bmm.default(_softmax, _unsafe_view_5);  _softmax = _unsafe_view_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_7: f16[2, 8, 1024, 80] = torch.ops.aten.view.default(bmm, [2, 8, 1024, 80]);  bmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_4: f16[2, 1024, 8, 80] = torch.ops.aten.permute.default(view_7, [0, 2, 1, 3]);  view_7 = None\n",
      "        clone_3: f16[2, 1024, 8, 80] = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None\n",
      "        _unsafe_view_6: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(clone_3, [2, 1024, 640]);  clone_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_3: f16[640, 640] = torch.ops.aten.t.default(arg9_1);  arg9_1 = None\n",
      "        view_8: f16[2048, 640] = torch.ops.aten.view.default(_unsafe_view_6, [2048, 640]);  _unsafe_view_6 = None\n",
      "        addmm: f16[2048, 640] = torch.ops.aten.addmm.default(arg10_1, view_8, t_3);  arg10_1 = view_8 = t_3 = None\n",
      "        view_9: f16[2, 1024, 640] = torch.ops.aten.view.default(addmm, [2, 1024, 640]);  addmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:490, code: hidden_states = self.attn1(norm_hidden_states, attention_mask=attention_mask) + hidden_states\n",
      "        add: f16[2, 1024, 640] = torch.ops.aten.add.Tensor(view_9, view);  view_9 = view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:495, code: self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)\n",
      "        native_layer_norm_1 = torch.ops.aten.native_layer_norm.default(add, [640], arg11_1, arg12_1, 1e-05);  arg11_1 = arg12_1 = None\n",
      "        getitem_6: f16[2, 1024, 640] = native_layer_norm_1[0]\n",
      "        getitem_7: f32[2, 1024, 1] = native_layer_norm_1[1]\n",
      "        getitem_8: f32[2, 1024, 1] = native_layer_norm_1[2];  native_layer_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t_4: f16[640, 640] = torch.ops.aten.t.default(arg13_1);  arg13_1 = None\n",
      "        view_10: f16[2048, 640] = torch.ops.aten.view.default(getitem_6, [2048, 640]);  getitem_6 = None\n",
      "        mm_3: f16[2048, 640] = torch.ops.aten.mm.default(view_10, t_4);  view_10 = t_4 = None\n",
      "        _unsafe_view_7: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(mm_3, [2, 1024, 640]);  mm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_11: f16[2, 1024, 8, 80] = torch.ops.aten.view.default(_unsafe_view_7, [2, 1024, 8, 80]);  _unsafe_view_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_5: f16[2, 8, 1024, 80] = torch.ops.aten.permute.default(view_11, [0, 2, 1, 3]);  view_11 = None\n",
      "        clone_4: f16[2, 8, 1024, 80] = torch.ops.aten.clone.default(permute_5, memory_format = torch.contiguous_format);  permute_5 = None\n",
      "        _unsafe_view_8: f16[16, 1024, 80] = torch.ops.aten._unsafe_view.default(clone_4, [16, 1024, 80]);  clone_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_5: f16[768, 640] = torch.ops.aten.t.default(arg14_1);  arg14_1 = None\n",
      "        view_12: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768])\n",
      "        mm_4: f16[154, 640] = torch.ops.aten.mm.default(view_12, t_5);  view_12 = t_5 = None\n",
      "        _unsafe_view_9: f16[2, 77, 640] = torch.ops.aten._unsafe_view.default(mm_4, [2, 77, 640]);  mm_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_6: f16[768, 640] = torch.ops.aten.t.default(arg15_1);  arg15_1 = None\n",
      "        view_13: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768]);  arg27_1 = None\n",
      "        mm_5: f16[154, 640] = torch.ops.aten.mm.default(view_13, t_6);  view_13 = t_6 = None\n",
      "        _unsafe_view_10: f16[2, 77, 640] = torch.ops.aten._unsafe_view.default(mm_5, [2, 77, 640]);  mm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_14: f16[2, 77, 8, 80] = torch.ops.aten.view.default(_unsafe_view_9, [2, 77, 8, 80]);  _unsafe_view_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_6: f16[2, 8, 77, 80] = torch.ops.aten.permute.default(view_14, [0, 2, 1, 3]);  view_14 = None\n",
      "        clone_5: f16[2, 8, 77, 80] = torch.ops.aten.clone.default(permute_6, memory_format = torch.contiguous_format);  permute_6 = None\n",
      "        _unsafe_view_11: f16[16, 77, 80] = torch.ops.aten._unsafe_view.default(clone_5, [16, 77, 80]);  clone_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_15: f16[2, 77, 8, 80] = torch.ops.aten.view.default(_unsafe_view_10, [2, 77, 8, 80]);  _unsafe_view_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_7: f16[2, 8, 77, 80] = torch.ops.aten.permute.default(view_15, [0, 2, 1, 3]);  view_15 = None\n",
      "        clone_6: f16[2, 8, 77, 80] = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None\n",
      "        _unsafe_view_12: f16[16, 77, 80] = torch.ops.aten._unsafe_view.default(clone_6, [16, 77, 80]);  clone_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty_1: f16[16, 1024, 77] = torch.ops.aten.empty.memory_format([16, 1024, 77], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose_1: f16[16, 80, 77] = torch.ops.aten.transpose.int(_unsafe_view_11, -1, -2);  _unsafe_view_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm_1: f16[16, 1024, 77] = torch.ops.aten.baddbmm.default(empty_1, _unsafe_view_8, transpose_1, beta = 0, alpha = 0.11180339887498948);  empty_1 = _unsafe_view_8 = transpose_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax_1: f16[16, 1024, 77] = torch.ops.aten._softmax.default(baddbmm_1, -1, False);  baddbmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm_1: f16[16, 1024, 80] = torch.ops.aten.bmm.default(_softmax_1, _unsafe_view_12);  _softmax_1 = _unsafe_view_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_16: f16[2, 8, 1024, 80] = torch.ops.aten.view.default(bmm_1, [2, 8, 1024, 80]);  bmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_8: f16[2, 1024, 8, 80] = torch.ops.aten.permute.default(view_16, [0, 2, 1, 3]);  view_16 = None\n",
      "        clone_7: f16[2, 1024, 8, 80] = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None\n",
      "        _unsafe_view_13: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(clone_7, [2, 1024, 640]);  clone_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_7: f16[640, 640] = torch.ops.aten.t.default(arg16_1);  arg16_1 = None\n",
      "        view_17: f16[2048, 640] = torch.ops.aten.view.default(_unsafe_view_13, [2048, 640]);  _unsafe_view_13 = None\n",
      "        addmm_1: f16[2048, 640] = torch.ops.aten.addmm.default(arg17_1, view_17, t_7);  arg17_1 = view_17 = t_7 = None\n",
      "        view_18: f16[2, 1024, 640] = torch.ops.aten.view.default(addmm_1, [2, 1024, 640]);  addmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:498, code: self.attn2(\n",
      "        add_1: f16[2, 1024, 640] = torch.ops.aten.add.Tensor(view_18, add);  view_18 = add = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        native_layer_norm_2 = torch.ops.aten.native_layer_norm.default(add_1, [640], arg18_1, arg19_1, 1e-05);  arg18_1 = arg19_1 = None\n",
      "        getitem_9: f16[2, 1024, 640] = native_layer_norm_2[0]\n",
      "        getitem_10: f32[2, 1024, 1] = native_layer_norm_2[1]\n",
      "        getitem_11: f32[2, 1024, 1] = native_layer_norm_2[2];  native_layer_norm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:820, code: hidden_states, gate = self.proj(hidden_states).chunk(2, dim=-1)\n",
      "        t_8: f16[640, 5120] = torch.ops.aten.t.default(arg20_1);  arg20_1 = None\n",
      "        view_19: f16[2048, 640] = torch.ops.aten.view.default(getitem_9, [2048, 640]);  getitem_9 = None\n",
      "        addmm_2: f16[2048, 5120] = torch.ops.aten.addmm.default(arg21_1, view_19, t_8);  arg21_1 = view_19 = t_8 = None\n",
      "        view_20: f16[2, 1024, 5120] = torch.ops.aten.view.default(addmm_2, [2, 1024, 5120]);  addmm_2 = None\n",
      "        split = torch.ops.aten.split.Tensor(view_20, 2560, -1);  view_20 = None\n",
      "        getitem_12: f16[2, 1024, 2560] = split[0]\n",
      "        getitem_13: f16[2, 1024, 2560] = split[1];  split = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:815, code: return F.gelu(gate)\n",
      "        gelu: f16[2, 1024, 2560] = torch.ops.aten.gelu.default(getitem_13);  getitem_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:821, code: return hidden_states * self.gelu(gate)\n",
      "        mul: f16[2, 1024, 2560] = torch.ops.aten.mul.Tensor(getitem_12, gelu);  getitem_12 = gelu = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:774, code: hidden_states = module(hidden_states)\n",
      "        t_9: f16[2560, 640] = torch.ops.aten.t.default(arg22_1);  arg22_1 = None\n",
      "        view_21: f16[2048, 2560] = torch.ops.aten.view.default(mul, [2048, 2560]);  mul = None\n",
      "        addmm_3: f16[2048, 640] = torch.ops.aten.addmm.default(arg23_1, view_21, t_9);  arg23_1 = view_21 = t_9 = None\n",
      "        view_22: f16[2, 1024, 640] = torch.ops.aten.view.default(addmm_3, [2, 1024, 640]);  addmm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        add_2: f16[2, 1024, 640] = torch.ops.aten.add.Tensor(view_22, add_1);  view_22 = add_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:222, code: hidden_states.reshape(batch, height, weight, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
      "        view_23: f16[2, 32, 32, 640] = torch.ops.aten.view.default(add_2, [2, 32, 32, 640]);  add_2 = None\n",
      "        permute_9: f16[2, 640, 32, 32] = torch.ops.aten.permute.default(view_23, [0, 3, 1, 2]);  view_23 = None\n",
      "        clone_8: f16[2, 640, 32, 32] = torch.ops.aten.clone.default(permute_9, memory_format = torch.contiguous_format);  permute_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:224, code: hidden_states = self.proj_out(hidden_states)\n",
      "        convolution_1: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(clone_8, arg24_1, arg25_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  clone_8 = arg24_1 = arg25_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:231, code: output = hidden_states + residual\n",
      "        add_3: f16[2, 640, 32, 32] = torch.ops.aten.add.Tensor(convolution_1, arg26_1);  convolution_1 = arg26_1 = None\n",
      "        return (add_3,)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1280], arg1_1: f16[1280], arg2_1: f16[640, 1280, 3, 3], arg3_1: f16[640], arg4_1: f16[640, 1280], arg5_1: f16[640], arg6_1: f16[640], arg7_1: f16[640], arg8_1: f16[640, 640, 3, 3], arg9_1: f16[640], arg10_1: f16[640, 1280, 1, 1], arg11_1: f16[640], arg12_1: f16[2, 1280, 32, 32], arg13_1: f16[2, 1280]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg12_1, arg0_1, arg1_1, 2, 1280, 1024, 32, 1e-05);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 1280, 32, 32] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu: f16[2, 1280, 32, 32] = torch.ops.aten.silu.default(getitem);  getitem = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(silu, arg2_1, arg3_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:474, code: temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
      "        silu_1: f16[2, 1280] = torch.ops.aten.silu.default(arg13_1);  arg13_1 = None\n",
      "        t: f16[1280, 640] = torch.ops.aten.t.default(arg4_1);  arg4_1 = None\n",
      "        addmm: f16[2, 640] = torch.ops.aten.addmm.default(arg5_1, silu_1, t);  arg5_1 = silu_1 = t = None\n",
      "        slice_1: f16[2, 640] = torch.ops.aten.slice.Tensor(addmm, 0, 0, 9223372036854775807);  addmm = None\n",
      "        slice_2: f16[2, 640] = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807);  slice_1 = None\n",
      "        unsqueeze: f16[2, 640, 1] = torch.ops.aten.unsqueeze.default(slice_2, 2);  slice_2 = None\n",
      "        unsqueeze_1: f16[2, 640, 1, 1] = torch.ops.aten.unsqueeze.default(unsqueeze, 3);  unsqueeze = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:477, code: hidden_states = hidden_states + temb\n",
      "        add: f16[2, 640, 32, 32] = torch.ops.aten.add.Tensor(convolution, unsqueeze_1);  convolution = unsqueeze_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_1 = torch.ops.aten.native_group_norm.default(add, arg6_1, arg7_1, 2, 640, 1024, 32, 1e-05);  add = arg6_1 = arg7_1 = None\n",
      "        getitem_3: f16[2, 640, 32, 32] = native_group_norm_1[0]\n",
      "        getitem_4: f16[2, 32] = native_group_norm_1[1]\n",
      "        getitem_5: f16[2, 32] = native_group_norm_1[2];  native_group_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_2: f16[2, 640, 32, 32] = torch.ops.aten.silu.default(getitem_3);  getitem_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_1: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(silu_2, arg8_1, arg9_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_2 = arg8_1 = arg9_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:491, code: input_tensor = self.conv_shortcut(input_tensor)\n",
      "        convolution_2: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(arg12_1, arg10_1, arg11_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  arg12_1 = arg10_1 = arg11_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_1: f16[2, 640, 32, 32] = torch.ops.aten.add.Tensor(convolution_2, convolution_1);  convolution_2 = convolution_1 = None\n",
      "        div: f16[2, 640, 32, 32] = torch.ops.aten.div.Tensor(add_1, 1.0);  add_1 = None\n",
      "        return (div,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[640], arg1_1: f16[640], arg2_1: f16[640, 640, 1, 1], arg3_1: f16[640], arg4_1: f16[640], arg5_1: f16[640], arg6_1: f16[640, 640], arg7_1: f16[640, 640], arg8_1: f16[640, 640], arg9_1: f16[640, 640], arg10_1: f16[640], arg11_1: f16[640], arg12_1: f16[640], arg13_1: f16[640, 640], arg14_1: f16[640, 768], arg15_1: f16[640, 768], arg16_1: f16[640, 640], arg17_1: f16[640], arg18_1: f16[640], arg19_1: f16[640], arg20_1: f16[5120, 640], arg21_1: f16[5120], arg22_1: f16[640, 2560], arg23_1: f16[640], arg24_1: f16[640, 640, 1, 1], arg25_1: f16[640], arg26_1: f16[2, 640, 32, 32], arg27_1: f16[2, 77, 768]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:202, code: hidden_states = self.norm(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg26_1, arg0_1, arg1_1, 2, 640, 1024, 32, 1e-06);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 640, 32, 32] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:204, code: hidden_states = self.proj_in(hidden_states)\n",
      "        convolution: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(getitem, arg2_1, arg3_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  getitem = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:206, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * weight, inner_dim)\n",
      "        permute: f16[2, 32, 32, 640] = torch.ops.aten.permute.default(convolution, [0, 2, 3, 1]);  convolution = None\n",
      "        view: f16[2, 1024, 640] = torch.ops.aten.view.default(permute, [2, 1024, 640]);  permute = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:482, code: self.norm1(hidden_states, timestep) if self.use_ada_layer_norm else self.norm1(hidden_states)\n",
      "        native_layer_norm = torch.ops.aten.native_layer_norm.default(view, [640], arg4_1, arg5_1, 1e-05);  arg4_1 = arg5_1 = None\n",
      "        getitem_3: f16[2, 1024, 640] = native_layer_norm[0]\n",
      "        getitem_4: f32[2, 1024, 1] = native_layer_norm[1]\n",
      "        getitem_5: f32[2, 1024, 1] = native_layer_norm[2];  native_layer_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t: f16[640, 640] = torch.ops.aten.t.default(arg6_1);  arg6_1 = None\n",
      "        view_1: f16[2048, 640] = torch.ops.aten.view.default(getitem_3, [2048, 640])\n",
      "        mm: f16[2048, 640] = torch.ops.aten.mm.default(view_1, t);  view_1 = t = None\n",
      "        _unsafe_view: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(mm, [2, 1024, 640]);  mm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_2: f16[2, 1024, 8, 80] = torch.ops.aten.view.default(_unsafe_view, [2, 1024, 8, 80]);  _unsafe_view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_1: f16[2, 8, 1024, 80] = torch.ops.aten.permute.default(view_2, [0, 2, 1, 3]);  view_2 = None\n",
      "        clone: f16[2, 8, 1024, 80] = torch.ops.aten.clone.default(permute_1, memory_format = torch.contiguous_format);  permute_1 = None\n",
      "        _unsafe_view_1: f16[16, 1024, 80] = torch.ops.aten._unsafe_view.default(clone, [16, 1024, 80]);  clone = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_1: f16[640, 640] = torch.ops.aten.t.default(arg7_1);  arg7_1 = None\n",
      "        view_3: f16[2048, 640] = torch.ops.aten.view.default(getitem_3, [2048, 640])\n",
      "        mm_1: f16[2048, 640] = torch.ops.aten.mm.default(view_3, t_1);  view_3 = t_1 = None\n",
      "        _unsafe_view_2: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(mm_1, [2, 1024, 640]);  mm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_2: f16[640, 640] = torch.ops.aten.t.default(arg8_1);  arg8_1 = None\n",
      "        view_4: f16[2048, 640] = torch.ops.aten.view.default(getitem_3, [2048, 640]);  getitem_3 = None\n",
      "        mm_2: f16[2048, 640] = torch.ops.aten.mm.default(view_4, t_2);  view_4 = t_2 = None\n",
      "        _unsafe_view_3: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(mm_2, [2, 1024, 640]);  mm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_5: f16[2, 1024, 8, 80] = torch.ops.aten.view.default(_unsafe_view_2, [2, 1024, 8, 80]);  _unsafe_view_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_2: f16[2, 8, 1024, 80] = torch.ops.aten.permute.default(view_5, [0, 2, 1, 3]);  view_5 = None\n",
      "        clone_1: f16[2, 8, 1024, 80] = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None\n",
      "        _unsafe_view_4: f16[16, 1024, 80] = torch.ops.aten._unsafe_view.default(clone_1, [16, 1024, 80]);  clone_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_6: f16[2, 1024, 8, 80] = torch.ops.aten.view.default(_unsafe_view_3, [2, 1024, 8, 80]);  _unsafe_view_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_3: f16[2, 8, 1024, 80] = torch.ops.aten.permute.default(view_6, [0, 2, 1, 3]);  view_6 = None\n",
      "        clone_2: f16[2, 8, 1024, 80] = torch.ops.aten.clone.default(permute_3, memory_format = torch.contiguous_format);  permute_3 = None\n",
      "        _unsafe_view_5: f16[16, 1024, 80] = torch.ops.aten._unsafe_view.default(clone_2, [16, 1024, 80]);  clone_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty: f16[16, 1024, 1024] = torch.ops.aten.empty.memory_format([16, 1024, 1024], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose: f16[16, 80, 1024] = torch.ops.aten.transpose.int(_unsafe_view_4, -1, -2);  _unsafe_view_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm: f16[16, 1024, 1024] = torch.ops.aten.baddbmm.default(empty, _unsafe_view_1, transpose, beta = 0, alpha = 0.11180339887498948);  empty = _unsafe_view_1 = transpose = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax: f16[16, 1024, 1024] = torch.ops.aten._softmax.default(baddbmm, -1, False);  baddbmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm: f16[16, 1024, 80] = torch.ops.aten.bmm.default(_softmax, _unsafe_view_5);  _softmax = _unsafe_view_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_7: f16[2, 8, 1024, 80] = torch.ops.aten.view.default(bmm, [2, 8, 1024, 80]);  bmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_4: f16[2, 1024, 8, 80] = torch.ops.aten.permute.default(view_7, [0, 2, 1, 3]);  view_7 = None\n",
      "        clone_3: f16[2, 1024, 8, 80] = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None\n",
      "        _unsafe_view_6: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(clone_3, [2, 1024, 640]);  clone_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_3: f16[640, 640] = torch.ops.aten.t.default(arg9_1);  arg9_1 = None\n",
      "        view_8: f16[2048, 640] = torch.ops.aten.view.default(_unsafe_view_6, [2048, 640]);  _unsafe_view_6 = None\n",
      "        addmm: f16[2048, 640] = torch.ops.aten.addmm.default(arg10_1, view_8, t_3);  arg10_1 = view_8 = t_3 = None\n",
      "        view_9: f16[2, 1024, 640] = torch.ops.aten.view.default(addmm, [2, 1024, 640]);  addmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:490, code: hidden_states = self.attn1(norm_hidden_states, attention_mask=attention_mask) + hidden_states\n",
      "        add: f16[2, 1024, 640] = torch.ops.aten.add.Tensor(view_9, view);  view_9 = view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:495, code: self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)\n",
      "        native_layer_norm_1 = torch.ops.aten.native_layer_norm.default(add, [640], arg11_1, arg12_1, 1e-05);  arg11_1 = arg12_1 = None\n",
      "        getitem_6: f16[2, 1024, 640] = native_layer_norm_1[0]\n",
      "        getitem_7: f32[2, 1024, 1] = native_layer_norm_1[1]\n",
      "        getitem_8: f32[2, 1024, 1] = native_layer_norm_1[2];  native_layer_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t_4: f16[640, 640] = torch.ops.aten.t.default(arg13_1);  arg13_1 = None\n",
      "        view_10: f16[2048, 640] = torch.ops.aten.view.default(getitem_6, [2048, 640]);  getitem_6 = None\n",
      "        mm_3: f16[2048, 640] = torch.ops.aten.mm.default(view_10, t_4);  view_10 = t_4 = None\n",
      "        _unsafe_view_7: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(mm_3, [2, 1024, 640]);  mm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_11: f16[2, 1024, 8, 80] = torch.ops.aten.view.default(_unsafe_view_7, [2, 1024, 8, 80]);  _unsafe_view_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_5: f16[2, 8, 1024, 80] = torch.ops.aten.permute.default(view_11, [0, 2, 1, 3]);  view_11 = None\n",
      "        clone_4: f16[2, 8, 1024, 80] = torch.ops.aten.clone.default(permute_5, memory_format = torch.contiguous_format);  permute_5 = None\n",
      "        _unsafe_view_8: f16[16, 1024, 80] = torch.ops.aten._unsafe_view.default(clone_4, [16, 1024, 80]);  clone_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_5: f16[768, 640] = torch.ops.aten.t.default(arg14_1);  arg14_1 = None\n",
      "        view_12: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768])\n",
      "        mm_4: f16[154, 640] = torch.ops.aten.mm.default(view_12, t_5);  view_12 = t_5 = None\n",
      "        _unsafe_view_9: f16[2, 77, 640] = torch.ops.aten._unsafe_view.default(mm_4, [2, 77, 640]);  mm_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_6: f16[768, 640] = torch.ops.aten.t.default(arg15_1);  arg15_1 = None\n",
      "        view_13: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768]);  arg27_1 = None\n",
      "        mm_5: f16[154, 640] = torch.ops.aten.mm.default(view_13, t_6);  view_13 = t_6 = None\n",
      "        _unsafe_view_10: f16[2, 77, 640] = torch.ops.aten._unsafe_view.default(mm_5, [2, 77, 640]);  mm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_14: f16[2, 77, 8, 80] = torch.ops.aten.view.default(_unsafe_view_9, [2, 77, 8, 80]);  _unsafe_view_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_6: f16[2, 8, 77, 80] = torch.ops.aten.permute.default(view_14, [0, 2, 1, 3]);  view_14 = None\n",
      "        clone_5: f16[2, 8, 77, 80] = torch.ops.aten.clone.default(permute_6, memory_format = torch.contiguous_format);  permute_6 = None\n",
      "        _unsafe_view_11: f16[16, 77, 80] = torch.ops.aten._unsafe_view.default(clone_5, [16, 77, 80]);  clone_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_15: f16[2, 77, 8, 80] = torch.ops.aten.view.default(_unsafe_view_10, [2, 77, 8, 80]);  _unsafe_view_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_7: f16[2, 8, 77, 80] = torch.ops.aten.permute.default(view_15, [0, 2, 1, 3]);  view_15 = None\n",
      "        clone_6: f16[2, 8, 77, 80] = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None\n",
      "        _unsafe_view_12: f16[16, 77, 80] = torch.ops.aten._unsafe_view.default(clone_6, [16, 77, 80]);  clone_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty_1: f16[16, 1024, 77] = torch.ops.aten.empty.memory_format([16, 1024, 77], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose_1: f16[16, 80, 77] = torch.ops.aten.transpose.int(_unsafe_view_11, -1, -2);  _unsafe_view_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm_1: f16[16, 1024, 77] = torch.ops.aten.baddbmm.default(empty_1, _unsafe_view_8, transpose_1, beta = 0, alpha = 0.11180339887498948);  empty_1 = _unsafe_view_8 = transpose_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax_1: f16[16, 1024, 77] = torch.ops.aten._softmax.default(baddbmm_1, -1, False);  baddbmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm_1: f16[16, 1024, 80] = torch.ops.aten.bmm.default(_softmax_1, _unsafe_view_12);  _softmax_1 = _unsafe_view_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_16: f16[2, 8, 1024, 80] = torch.ops.aten.view.default(bmm_1, [2, 8, 1024, 80]);  bmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_8: f16[2, 1024, 8, 80] = torch.ops.aten.permute.default(view_16, [0, 2, 1, 3]);  view_16 = None\n",
      "        clone_7: f16[2, 1024, 8, 80] = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None\n",
      "        _unsafe_view_13: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(clone_7, [2, 1024, 640]);  clone_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_7: f16[640, 640] = torch.ops.aten.t.default(arg16_1);  arg16_1 = None\n",
      "        view_17: f16[2048, 640] = torch.ops.aten.view.default(_unsafe_view_13, [2048, 640]);  _unsafe_view_13 = None\n",
      "        addmm_1: f16[2048, 640] = torch.ops.aten.addmm.default(arg17_1, view_17, t_7);  arg17_1 = view_17 = t_7 = None\n",
      "        view_18: f16[2, 1024, 640] = torch.ops.aten.view.default(addmm_1, [2, 1024, 640]);  addmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:498, code: self.attn2(\n",
      "        add_1: f16[2, 1024, 640] = torch.ops.aten.add.Tensor(view_18, add);  view_18 = add = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        native_layer_norm_2 = torch.ops.aten.native_layer_norm.default(add_1, [640], arg18_1, arg19_1, 1e-05);  arg18_1 = arg19_1 = None\n",
      "        getitem_9: f16[2, 1024, 640] = native_layer_norm_2[0]\n",
      "        getitem_10: f32[2, 1024, 1] = native_layer_norm_2[1]\n",
      "        getitem_11: f32[2, 1024, 1] = native_layer_norm_2[2];  native_layer_norm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:820, code: hidden_states, gate = self.proj(hidden_states).chunk(2, dim=-1)\n",
      "        t_8: f16[640, 5120] = torch.ops.aten.t.default(arg20_1);  arg20_1 = None\n",
      "        view_19: f16[2048, 640] = torch.ops.aten.view.default(getitem_9, [2048, 640]);  getitem_9 = None\n",
      "        addmm_2: f16[2048, 5120] = torch.ops.aten.addmm.default(arg21_1, view_19, t_8);  arg21_1 = view_19 = t_8 = None\n",
      "        view_20: f16[2, 1024, 5120] = torch.ops.aten.view.default(addmm_2, [2, 1024, 5120]);  addmm_2 = None\n",
      "        split = torch.ops.aten.split.Tensor(view_20, 2560, -1);  view_20 = None\n",
      "        getitem_12: f16[2, 1024, 2560] = split[0]\n",
      "        getitem_13: f16[2, 1024, 2560] = split[1];  split = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:815, code: return F.gelu(gate)\n",
      "        gelu: f16[2, 1024, 2560] = torch.ops.aten.gelu.default(getitem_13);  getitem_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:821, code: return hidden_states * self.gelu(gate)\n",
      "        mul: f16[2, 1024, 2560] = torch.ops.aten.mul.Tensor(getitem_12, gelu);  getitem_12 = gelu = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:774, code: hidden_states = module(hidden_states)\n",
      "        t_9: f16[2560, 640] = torch.ops.aten.t.default(arg22_1);  arg22_1 = None\n",
      "        view_21: f16[2048, 2560] = torch.ops.aten.view.default(mul, [2048, 2560]);  mul = None\n",
      "        addmm_3: f16[2048, 640] = torch.ops.aten.addmm.default(arg23_1, view_21, t_9);  arg23_1 = view_21 = t_9 = None\n",
      "        view_22: f16[2, 1024, 640] = torch.ops.aten.view.default(addmm_3, [2, 1024, 640]);  addmm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        add_2: f16[2, 1024, 640] = torch.ops.aten.add.Tensor(view_22, add_1);  view_22 = add_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:222, code: hidden_states.reshape(batch, height, weight, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
      "        view_23: f16[2, 32, 32, 640] = torch.ops.aten.view.default(add_2, [2, 32, 32, 640]);  add_2 = None\n",
      "        permute_9: f16[2, 640, 32, 32] = torch.ops.aten.permute.default(view_23, [0, 3, 1, 2]);  view_23 = None\n",
      "        clone_8: f16[2, 640, 32, 32] = torch.ops.aten.clone.default(permute_9, memory_format = torch.contiguous_format);  permute_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:224, code: hidden_states = self.proj_out(hidden_states)\n",
      "        convolution_1: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(clone_8, arg24_1, arg25_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  clone_8 = arg24_1 = arg25_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:231, code: output = hidden_states + residual\n",
      "        add_3: f16[2, 640, 32, 32] = torch.ops.aten.add.Tensor(convolution_1, arg26_1);  convolution_1 = arg26_1 = None\n",
      "        return (add_3,)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[960], arg1_1: f16[960], arg2_1: f16[640, 960, 3, 3], arg3_1: f16[640], arg4_1: f16[640, 1280], arg5_1: f16[640], arg6_1: f16[640], arg7_1: f16[640], arg8_1: f16[640, 640, 3, 3], arg9_1: f16[640], arg10_1: f16[640, 960, 1, 1], arg11_1: f16[640], arg12_1: f16[2, 960, 32, 32], arg13_1: f16[2, 1280]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg12_1, arg0_1, arg1_1, 2, 960, 1024, 32, 1e-05);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 960, 32, 32] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu: f16[2, 960, 32, 32] = torch.ops.aten.silu.default(getitem);  getitem = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(silu, arg2_1, arg3_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:474, code: temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
      "        silu_1: f16[2, 1280] = torch.ops.aten.silu.default(arg13_1);  arg13_1 = None\n",
      "        t: f16[1280, 640] = torch.ops.aten.t.default(arg4_1);  arg4_1 = None\n",
      "        addmm: f16[2, 640] = torch.ops.aten.addmm.default(arg5_1, silu_1, t);  arg5_1 = silu_1 = t = None\n",
      "        slice_1: f16[2, 640] = torch.ops.aten.slice.Tensor(addmm, 0, 0, 9223372036854775807);  addmm = None\n",
      "        slice_2: f16[2, 640] = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807);  slice_1 = None\n",
      "        unsqueeze: f16[2, 640, 1] = torch.ops.aten.unsqueeze.default(slice_2, 2);  slice_2 = None\n",
      "        unsqueeze_1: f16[2, 640, 1, 1] = torch.ops.aten.unsqueeze.default(unsqueeze, 3);  unsqueeze = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:477, code: hidden_states = hidden_states + temb\n",
      "        add: f16[2, 640, 32, 32] = torch.ops.aten.add.Tensor(convolution, unsqueeze_1);  convolution = unsqueeze_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_1 = torch.ops.aten.native_group_norm.default(add, arg6_1, arg7_1, 2, 640, 1024, 32, 1e-05);  add = arg6_1 = arg7_1 = None\n",
      "        getitem_3: f16[2, 640, 32, 32] = native_group_norm_1[0]\n",
      "        getitem_4: f16[2, 32] = native_group_norm_1[1]\n",
      "        getitem_5: f16[2, 32] = native_group_norm_1[2];  native_group_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_2: f16[2, 640, 32, 32] = torch.ops.aten.silu.default(getitem_3);  getitem_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_1: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(silu_2, arg8_1, arg9_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_2 = arg8_1 = arg9_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:491, code: input_tensor = self.conv_shortcut(input_tensor)\n",
      "        convolution_2: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(arg12_1, arg10_1, arg11_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  arg12_1 = arg10_1 = arg11_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_1: f16[2, 640, 32, 32] = torch.ops.aten.add.Tensor(convolution_2, convolution_1);  convolution_2 = convolution_1 = None\n",
      "        div: f16[2, 640, 32, 32] = torch.ops.aten.div.Tensor(add_1, 1.0);  add_1 = None\n",
      "        return (div,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[640], arg1_1: f16[640], arg2_1: f16[640, 640, 1, 1], arg3_1: f16[640], arg4_1: f16[640], arg5_1: f16[640], arg6_1: f16[640, 640], arg7_1: f16[640, 640], arg8_1: f16[640, 640], arg9_1: f16[640, 640], arg10_1: f16[640], arg11_1: f16[640], arg12_1: f16[640], arg13_1: f16[640, 640], arg14_1: f16[640, 768], arg15_1: f16[640, 768], arg16_1: f16[640, 640], arg17_1: f16[640], arg18_1: f16[640], arg19_1: f16[640], arg20_1: f16[5120, 640], arg21_1: f16[5120], arg22_1: f16[640, 2560], arg23_1: f16[640], arg24_1: f16[640, 640, 1, 1], arg25_1: f16[640], arg26_1: f16[2, 640, 32, 32], arg27_1: f16[2, 77, 768]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:202, code: hidden_states = self.norm(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg26_1, arg0_1, arg1_1, 2, 640, 1024, 32, 1e-06);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 640, 32, 32] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:204, code: hidden_states = self.proj_in(hidden_states)\n",
      "        convolution: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(getitem, arg2_1, arg3_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  getitem = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:206, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * weight, inner_dim)\n",
      "        permute: f16[2, 32, 32, 640] = torch.ops.aten.permute.default(convolution, [0, 2, 3, 1]);  convolution = None\n",
      "        view: f16[2, 1024, 640] = torch.ops.aten.view.default(permute, [2, 1024, 640]);  permute = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:482, code: self.norm1(hidden_states, timestep) if self.use_ada_layer_norm else self.norm1(hidden_states)\n",
      "        native_layer_norm = torch.ops.aten.native_layer_norm.default(view, [640], arg4_1, arg5_1, 1e-05);  arg4_1 = arg5_1 = None\n",
      "        getitem_3: f16[2, 1024, 640] = native_layer_norm[0]\n",
      "        getitem_4: f32[2, 1024, 1] = native_layer_norm[1]\n",
      "        getitem_5: f32[2, 1024, 1] = native_layer_norm[2];  native_layer_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t: f16[640, 640] = torch.ops.aten.t.default(arg6_1);  arg6_1 = None\n",
      "        view_1: f16[2048, 640] = torch.ops.aten.view.default(getitem_3, [2048, 640])\n",
      "        mm: f16[2048, 640] = torch.ops.aten.mm.default(view_1, t);  view_1 = t = None\n",
      "        _unsafe_view: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(mm, [2, 1024, 640]);  mm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_2: f16[2, 1024, 8, 80] = torch.ops.aten.view.default(_unsafe_view, [2, 1024, 8, 80]);  _unsafe_view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_1: f16[2, 8, 1024, 80] = torch.ops.aten.permute.default(view_2, [0, 2, 1, 3]);  view_2 = None\n",
      "        clone: f16[2, 8, 1024, 80] = torch.ops.aten.clone.default(permute_1, memory_format = torch.contiguous_format);  permute_1 = None\n",
      "        _unsafe_view_1: f16[16, 1024, 80] = torch.ops.aten._unsafe_view.default(clone, [16, 1024, 80]);  clone = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_1: f16[640, 640] = torch.ops.aten.t.default(arg7_1);  arg7_1 = None\n",
      "        view_3: f16[2048, 640] = torch.ops.aten.view.default(getitem_3, [2048, 640])\n",
      "        mm_1: f16[2048, 640] = torch.ops.aten.mm.default(view_3, t_1);  view_3 = t_1 = None\n",
      "        _unsafe_view_2: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(mm_1, [2, 1024, 640]);  mm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_2: f16[640, 640] = torch.ops.aten.t.default(arg8_1);  arg8_1 = None\n",
      "        view_4: f16[2048, 640] = torch.ops.aten.view.default(getitem_3, [2048, 640]);  getitem_3 = None\n",
      "        mm_2: f16[2048, 640] = torch.ops.aten.mm.default(view_4, t_2);  view_4 = t_2 = None\n",
      "        _unsafe_view_3: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(mm_2, [2, 1024, 640]);  mm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_5: f16[2, 1024, 8, 80] = torch.ops.aten.view.default(_unsafe_view_2, [2, 1024, 8, 80]);  _unsafe_view_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_2: f16[2, 8, 1024, 80] = torch.ops.aten.permute.default(view_5, [0, 2, 1, 3]);  view_5 = None\n",
      "        clone_1: f16[2, 8, 1024, 80] = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None\n",
      "        _unsafe_view_4: f16[16, 1024, 80] = torch.ops.aten._unsafe_view.default(clone_1, [16, 1024, 80]);  clone_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_6: f16[2, 1024, 8, 80] = torch.ops.aten.view.default(_unsafe_view_3, [2, 1024, 8, 80]);  _unsafe_view_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_3: f16[2, 8, 1024, 80] = torch.ops.aten.permute.default(view_6, [0, 2, 1, 3]);  view_6 = None\n",
      "        clone_2: f16[2, 8, 1024, 80] = torch.ops.aten.clone.default(permute_3, memory_format = torch.contiguous_format);  permute_3 = None\n",
      "        _unsafe_view_5: f16[16, 1024, 80] = torch.ops.aten._unsafe_view.default(clone_2, [16, 1024, 80]);  clone_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty: f16[16, 1024, 1024] = torch.ops.aten.empty.memory_format([16, 1024, 1024], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose: f16[16, 80, 1024] = torch.ops.aten.transpose.int(_unsafe_view_4, -1, -2);  _unsafe_view_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm: f16[16, 1024, 1024] = torch.ops.aten.baddbmm.default(empty, _unsafe_view_1, transpose, beta = 0, alpha = 0.11180339887498948);  empty = _unsafe_view_1 = transpose = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax: f16[16, 1024, 1024] = torch.ops.aten._softmax.default(baddbmm, -1, False);  baddbmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm: f16[16, 1024, 80] = torch.ops.aten.bmm.default(_softmax, _unsafe_view_5);  _softmax = _unsafe_view_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_7: f16[2, 8, 1024, 80] = torch.ops.aten.view.default(bmm, [2, 8, 1024, 80]);  bmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_4: f16[2, 1024, 8, 80] = torch.ops.aten.permute.default(view_7, [0, 2, 1, 3]);  view_7 = None\n",
      "        clone_3: f16[2, 1024, 8, 80] = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None\n",
      "        _unsafe_view_6: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(clone_3, [2, 1024, 640]);  clone_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_3: f16[640, 640] = torch.ops.aten.t.default(arg9_1);  arg9_1 = None\n",
      "        view_8: f16[2048, 640] = torch.ops.aten.view.default(_unsafe_view_6, [2048, 640]);  _unsafe_view_6 = None\n",
      "        addmm: f16[2048, 640] = torch.ops.aten.addmm.default(arg10_1, view_8, t_3);  arg10_1 = view_8 = t_3 = None\n",
      "        view_9: f16[2, 1024, 640] = torch.ops.aten.view.default(addmm, [2, 1024, 640]);  addmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:490, code: hidden_states = self.attn1(norm_hidden_states, attention_mask=attention_mask) + hidden_states\n",
      "        add: f16[2, 1024, 640] = torch.ops.aten.add.Tensor(view_9, view);  view_9 = view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:495, code: self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)\n",
      "        native_layer_norm_1 = torch.ops.aten.native_layer_norm.default(add, [640], arg11_1, arg12_1, 1e-05);  arg11_1 = arg12_1 = None\n",
      "        getitem_6: f16[2, 1024, 640] = native_layer_norm_1[0]\n",
      "        getitem_7: f32[2, 1024, 1] = native_layer_norm_1[1]\n",
      "        getitem_8: f32[2, 1024, 1] = native_layer_norm_1[2];  native_layer_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t_4: f16[640, 640] = torch.ops.aten.t.default(arg13_1);  arg13_1 = None\n",
      "        view_10: f16[2048, 640] = torch.ops.aten.view.default(getitem_6, [2048, 640]);  getitem_6 = None\n",
      "        mm_3: f16[2048, 640] = torch.ops.aten.mm.default(view_10, t_4);  view_10 = t_4 = None\n",
      "        _unsafe_view_7: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(mm_3, [2, 1024, 640]);  mm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_11: f16[2, 1024, 8, 80] = torch.ops.aten.view.default(_unsafe_view_7, [2, 1024, 8, 80]);  _unsafe_view_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_5: f16[2, 8, 1024, 80] = torch.ops.aten.permute.default(view_11, [0, 2, 1, 3]);  view_11 = None\n",
      "        clone_4: f16[2, 8, 1024, 80] = torch.ops.aten.clone.default(permute_5, memory_format = torch.contiguous_format);  permute_5 = None\n",
      "        _unsafe_view_8: f16[16, 1024, 80] = torch.ops.aten._unsafe_view.default(clone_4, [16, 1024, 80]);  clone_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_5: f16[768, 640] = torch.ops.aten.t.default(arg14_1);  arg14_1 = None\n",
      "        view_12: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768])\n",
      "        mm_4: f16[154, 640] = torch.ops.aten.mm.default(view_12, t_5);  view_12 = t_5 = None\n",
      "        _unsafe_view_9: f16[2, 77, 640] = torch.ops.aten._unsafe_view.default(mm_4, [2, 77, 640]);  mm_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_6: f16[768, 640] = torch.ops.aten.t.default(arg15_1);  arg15_1 = None\n",
      "        view_13: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768]);  arg27_1 = None\n",
      "        mm_5: f16[154, 640] = torch.ops.aten.mm.default(view_13, t_6);  view_13 = t_6 = None\n",
      "        _unsafe_view_10: f16[2, 77, 640] = torch.ops.aten._unsafe_view.default(mm_5, [2, 77, 640]);  mm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_14: f16[2, 77, 8, 80] = torch.ops.aten.view.default(_unsafe_view_9, [2, 77, 8, 80]);  _unsafe_view_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_6: f16[2, 8, 77, 80] = torch.ops.aten.permute.default(view_14, [0, 2, 1, 3]);  view_14 = None\n",
      "        clone_5: f16[2, 8, 77, 80] = torch.ops.aten.clone.default(permute_6, memory_format = torch.contiguous_format);  permute_6 = None\n",
      "        _unsafe_view_11: f16[16, 77, 80] = torch.ops.aten._unsafe_view.default(clone_5, [16, 77, 80]);  clone_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_15: f16[2, 77, 8, 80] = torch.ops.aten.view.default(_unsafe_view_10, [2, 77, 8, 80]);  _unsafe_view_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_7: f16[2, 8, 77, 80] = torch.ops.aten.permute.default(view_15, [0, 2, 1, 3]);  view_15 = None\n",
      "        clone_6: f16[2, 8, 77, 80] = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None\n",
      "        _unsafe_view_12: f16[16, 77, 80] = torch.ops.aten._unsafe_view.default(clone_6, [16, 77, 80]);  clone_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty_1: f16[16, 1024, 77] = torch.ops.aten.empty.memory_format([16, 1024, 77], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose_1: f16[16, 80, 77] = torch.ops.aten.transpose.int(_unsafe_view_11, -1, -2);  _unsafe_view_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm_1: f16[16, 1024, 77] = torch.ops.aten.baddbmm.default(empty_1, _unsafe_view_8, transpose_1, beta = 0, alpha = 0.11180339887498948);  empty_1 = _unsafe_view_8 = transpose_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax_1: f16[16, 1024, 77] = torch.ops.aten._softmax.default(baddbmm_1, -1, False);  baddbmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm_1: f16[16, 1024, 80] = torch.ops.aten.bmm.default(_softmax_1, _unsafe_view_12);  _softmax_1 = _unsafe_view_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_16: f16[2, 8, 1024, 80] = torch.ops.aten.view.default(bmm_1, [2, 8, 1024, 80]);  bmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_8: f16[2, 1024, 8, 80] = torch.ops.aten.permute.default(view_16, [0, 2, 1, 3]);  view_16 = None\n",
      "        clone_7: f16[2, 1024, 8, 80] = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None\n",
      "        _unsafe_view_13: f16[2, 1024, 640] = torch.ops.aten._unsafe_view.default(clone_7, [2, 1024, 640]);  clone_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_7: f16[640, 640] = torch.ops.aten.t.default(arg16_1);  arg16_1 = None\n",
      "        view_17: f16[2048, 640] = torch.ops.aten.view.default(_unsafe_view_13, [2048, 640]);  _unsafe_view_13 = None\n",
      "        addmm_1: f16[2048, 640] = torch.ops.aten.addmm.default(arg17_1, view_17, t_7);  arg17_1 = view_17 = t_7 = None\n",
      "        view_18: f16[2, 1024, 640] = torch.ops.aten.view.default(addmm_1, [2, 1024, 640]);  addmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:498, code: self.attn2(\n",
      "        add_1: f16[2, 1024, 640] = torch.ops.aten.add.Tensor(view_18, add);  view_18 = add = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        native_layer_norm_2 = torch.ops.aten.native_layer_norm.default(add_1, [640], arg18_1, arg19_1, 1e-05);  arg18_1 = arg19_1 = None\n",
      "        getitem_9: f16[2, 1024, 640] = native_layer_norm_2[0]\n",
      "        getitem_10: f32[2, 1024, 1] = native_layer_norm_2[1]\n",
      "        getitem_11: f32[2, 1024, 1] = native_layer_norm_2[2];  native_layer_norm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:820, code: hidden_states, gate = self.proj(hidden_states).chunk(2, dim=-1)\n",
      "        t_8: f16[640, 5120] = torch.ops.aten.t.default(arg20_1);  arg20_1 = None\n",
      "        view_19: f16[2048, 640] = torch.ops.aten.view.default(getitem_9, [2048, 640]);  getitem_9 = None\n",
      "        addmm_2: f16[2048, 5120] = torch.ops.aten.addmm.default(arg21_1, view_19, t_8);  arg21_1 = view_19 = t_8 = None\n",
      "        view_20: f16[2, 1024, 5120] = torch.ops.aten.view.default(addmm_2, [2, 1024, 5120]);  addmm_2 = None\n",
      "        split = torch.ops.aten.split.Tensor(view_20, 2560, -1);  view_20 = None\n",
      "        getitem_12: f16[2, 1024, 2560] = split[0]\n",
      "        getitem_13: f16[2, 1024, 2560] = split[1];  split = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:815, code: return F.gelu(gate)\n",
      "        gelu: f16[2, 1024, 2560] = torch.ops.aten.gelu.default(getitem_13);  getitem_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:821, code: return hidden_states * self.gelu(gate)\n",
      "        mul: f16[2, 1024, 2560] = torch.ops.aten.mul.Tensor(getitem_12, gelu);  getitem_12 = gelu = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:774, code: hidden_states = module(hidden_states)\n",
      "        t_9: f16[2560, 640] = torch.ops.aten.t.default(arg22_1);  arg22_1 = None\n",
      "        view_21: f16[2048, 2560] = torch.ops.aten.view.default(mul, [2048, 2560]);  mul = None\n",
      "        addmm_3: f16[2048, 640] = torch.ops.aten.addmm.default(arg23_1, view_21, t_9);  arg23_1 = view_21 = t_9 = None\n",
      "        view_22: f16[2, 1024, 640] = torch.ops.aten.view.default(addmm_3, [2, 1024, 640]);  addmm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        add_2: f16[2, 1024, 640] = torch.ops.aten.add.Tensor(view_22, add_1);  view_22 = add_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:222, code: hidden_states.reshape(batch, height, weight, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
      "        view_23: f16[2, 32, 32, 640] = torch.ops.aten.view.default(add_2, [2, 32, 32, 640]);  add_2 = None\n",
      "        permute_9: f16[2, 640, 32, 32] = torch.ops.aten.permute.default(view_23, [0, 3, 1, 2]);  view_23 = None\n",
      "        clone_8: f16[2, 640, 32, 32] = torch.ops.aten.clone.default(permute_9, memory_format = torch.contiguous_format);  permute_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:224, code: hidden_states = self.proj_out(hidden_states)\n",
      "        convolution_1: f16[2, 640, 32, 32] = torch.ops.aten.convolution.default(clone_8, arg24_1, arg25_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  clone_8 = arg24_1 = arg25_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:231, code: output = hidden_states + residual\n",
      "        add_3: f16[2, 640, 32, 32] = torch.ops.aten.add.Tensor(convolution_1, arg26_1);  convolution_1 = arg26_1 = None\n",
      "        return (add_3,)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[640, 640, 3, 3], arg1_1: f16[640], arg2_1: f16[2, 640, 32, 32]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:128, code: hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n",
      "        upsample_nearest2d: f16[2, 640, 64, 64] = torch.ops.aten.upsample_nearest2d.default(arg2_1, [64, 64], 2.0, 2.0);  arg2_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:139, code: hidden_states = self.conv(hidden_states)\n",
      "        convolution: f16[2, 640, 64, 64] = torch.ops.aten.convolution.default(upsample_nearest2d, arg0_1, arg1_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  upsample_nearest2d = arg0_1 = arg1_1 = None\n",
      "        return (convolution,)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[960], arg1_1: f16[960], arg2_1: f16[320, 960, 3, 3], arg3_1: f16[320], arg4_1: f16[320, 1280], arg5_1: f16[320], arg6_1: f16[320], arg7_1: f16[320], arg8_1: f16[320, 320, 3, 3], arg9_1: f16[320], arg10_1: f16[320, 960, 1, 1], arg11_1: f16[320], arg12_1: f16[2, 960, 64, 64], arg13_1: f16[2, 1280]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg12_1, arg0_1, arg1_1, 2, 960, 4096, 32, 1e-05);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 960, 64, 64] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu: f16[2, 960, 64, 64] = torch.ops.aten.silu.default(getitem);  getitem = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution: f16[2, 320, 64, 64] = torch.ops.aten.convolution.default(silu, arg2_1, arg3_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:474, code: temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
      "        silu_1: f16[2, 1280] = torch.ops.aten.silu.default(arg13_1);  arg13_1 = None\n",
      "        t: f16[1280, 320] = torch.ops.aten.t.default(arg4_1);  arg4_1 = None\n",
      "        addmm: f16[2, 320] = torch.ops.aten.addmm.default(arg5_1, silu_1, t);  arg5_1 = silu_1 = t = None\n",
      "        slice_1: f16[2, 320] = torch.ops.aten.slice.Tensor(addmm, 0, 0, 9223372036854775807);  addmm = None\n",
      "        slice_2: f16[2, 320] = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807);  slice_1 = None\n",
      "        unsqueeze: f16[2, 320, 1] = torch.ops.aten.unsqueeze.default(slice_2, 2);  slice_2 = None\n",
      "        unsqueeze_1: f16[2, 320, 1, 1] = torch.ops.aten.unsqueeze.default(unsqueeze, 3);  unsqueeze = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:477, code: hidden_states = hidden_states + temb\n",
      "        add: f16[2, 320, 64, 64] = torch.ops.aten.add.Tensor(convolution, unsqueeze_1);  convolution = unsqueeze_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_1 = torch.ops.aten.native_group_norm.default(add, arg6_1, arg7_1, 2, 320, 4096, 32, 1e-05);  add = arg6_1 = arg7_1 = None\n",
      "        getitem_3: f16[2, 320, 64, 64] = native_group_norm_1[0]\n",
      "        getitem_4: f16[2, 32] = native_group_norm_1[1]\n",
      "        getitem_5: f16[2, 32] = native_group_norm_1[2];  native_group_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_2: f16[2, 320, 64, 64] = torch.ops.aten.silu.default(getitem_3);  getitem_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_1: f16[2, 320, 64, 64] = torch.ops.aten.convolution.default(silu_2, arg8_1, arg9_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_2 = arg8_1 = arg9_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:491, code: input_tensor = self.conv_shortcut(input_tensor)\n",
      "        convolution_2: f16[2, 320, 64, 64] = torch.ops.aten.convolution.default(arg12_1, arg10_1, arg11_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  arg12_1 = arg10_1 = arg11_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_1: f16[2, 320, 64, 64] = torch.ops.aten.add.Tensor(convolution_2, convolution_1);  convolution_2 = convolution_1 = None\n",
      "        div: f16[2, 320, 64, 64] = torch.ops.aten.div.Tensor(add_1, 1.0);  add_1 = None\n",
      "        return (div,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[320], arg1_1: f16[320], arg2_1: f16[320, 320, 1, 1], arg3_1: f16[320], arg4_1: f16[320], arg5_1: f16[320], arg6_1: f16[320, 320], arg7_1: f16[320, 320], arg8_1: f16[320, 320], arg9_1: f16[320, 320], arg10_1: f16[320], arg11_1: f16[320], arg12_1: f16[320], arg13_1: f16[320, 320], arg14_1: f16[320, 768], arg15_1: f16[320, 768], arg16_1: f16[320, 320], arg17_1: f16[320], arg18_1: f16[320], arg19_1: f16[320], arg20_1: f16[2560, 320], arg21_1: f16[2560], arg22_1: f16[320, 1280], arg23_1: f16[320], arg24_1: f16[320, 320, 1, 1], arg25_1: f16[320], arg26_1: f16[2, 320, 64, 64], arg27_1: f16[2, 77, 768]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:202, code: hidden_states = self.norm(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg26_1, arg0_1, arg1_1, 2, 320, 4096, 32, 1e-06);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 320, 64, 64] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:204, code: hidden_states = self.proj_in(hidden_states)\n",
      "        convolution: f16[2, 320, 64, 64] = torch.ops.aten.convolution.default(getitem, arg2_1, arg3_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  getitem = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:206, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * weight, inner_dim)\n",
      "        permute: f16[2, 64, 64, 320] = torch.ops.aten.permute.default(convolution, [0, 2, 3, 1]);  convolution = None\n",
      "        view: f16[2, 4096, 320] = torch.ops.aten.view.default(permute, [2, 4096, 320]);  permute = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:482, code: self.norm1(hidden_states, timestep) if self.use_ada_layer_norm else self.norm1(hidden_states)\n",
      "        native_layer_norm = torch.ops.aten.native_layer_norm.default(view, [320], arg4_1, arg5_1, 1e-05);  arg4_1 = arg5_1 = None\n",
      "        getitem_3: f16[2, 4096, 320] = native_layer_norm[0]\n",
      "        getitem_4: f32[2, 4096, 1] = native_layer_norm[1]\n",
      "        getitem_5: f32[2, 4096, 1] = native_layer_norm[2];  native_layer_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t: f16[320, 320] = torch.ops.aten.t.default(arg6_1);  arg6_1 = None\n",
      "        view_1: f16[8192, 320] = torch.ops.aten.view.default(getitem_3, [8192, 320])\n",
      "        mm: f16[8192, 320] = torch.ops.aten.mm.default(view_1, t);  view_1 = t = None\n",
      "        _unsafe_view: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(mm, [2, 4096, 320]);  mm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_2: f16[2, 4096, 8, 40] = torch.ops.aten.view.default(_unsafe_view, [2, 4096, 8, 40]);  _unsafe_view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_1: f16[2, 8, 4096, 40] = torch.ops.aten.permute.default(view_2, [0, 2, 1, 3]);  view_2 = None\n",
      "        clone: f16[2, 8, 4096, 40] = torch.ops.aten.clone.default(permute_1, memory_format = torch.contiguous_format);  permute_1 = None\n",
      "        _unsafe_view_1: f16[16, 4096, 40] = torch.ops.aten._unsafe_view.default(clone, [16, 4096, 40]);  clone = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_1: f16[320, 320] = torch.ops.aten.t.default(arg7_1);  arg7_1 = None\n",
      "        view_3: f16[8192, 320] = torch.ops.aten.view.default(getitem_3, [8192, 320])\n",
      "        mm_1: f16[8192, 320] = torch.ops.aten.mm.default(view_3, t_1);  view_3 = t_1 = None\n",
      "        _unsafe_view_2: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(mm_1, [2, 4096, 320]);  mm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_2: f16[320, 320] = torch.ops.aten.t.default(arg8_1);  arg8_1 = None\n",
      "        view_4: f16[8192, 320] = torch.ops.aten.view.default(getitem_3, [8192, 320]);  getitem_3 = None\n",
      "        mm_2: f16[8192, 320] = torch.ops.aten.mm.default(view_4, t_2);  view_4 = t_2 = None\n",
      "        _unsafe_view_3: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(mm_2, [2, 4096, 320]);  mm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_5: f16[2, 4096, 8, 40] = torch.ops.aten.view.default(_unsafe_view_2, [2, 4096, 8, 40]);  _unsafe_view_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_2: f16[2, 8, 4096, 40] = torch.ops.aten.permute.default(view_5, [0, 2, 1, 3]);  view_5 = None\n",
      "        clone_1: f16[2, 8, 4096, 40] = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None\n",
      "        _unsafe_view_4: f16[16, 4096, 40] = torch.ops.aten._unsafe_view.default(clone_1, [16, 4096, 40]);  clone_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_6: f16[2, 4096, 8, 40] = torch.ops.aten.view.default(_unsafe_view_3, [2, 4096, 8, 40]);  _unsafe_view_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_3: f16[2, 8, 4096, 40] = torch.ops.aten.permute.default(view_6, [0, 2, 1, 3]);  view_6 = None\n",
      "        clone_2: f16[2, 8, 4096, 40] = torch.ops.aten.clone.default(permute_3, memory_format = torch.contiguous_format);  permute_3 = None\n",
      "        _unsafe_view_5: f16[16, 4096, 40] = torch.ops.aten._unsafe_view.default(clone_2, [16, 4096, 40]);  clone_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty: f16[16, 4096, 4096] = torch.ops.aten.empty.memory_format([16, 4096, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose: f16[16, 40, 4096] = torch.ops.aten.transpose.int(_unsafe_view_4, -1, -2);  _unsafe_view_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm: f16[16, 4096, 4096] = torch.ops.aten.baddbmm.default(empty, _unsafe_view_1, transpose, beta = 0, alpha = 0.15811388300841897);  empty = _unsafe_view_1 = transpose = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax: f16[16, 4096, 4096] = torch.ops.aten._softmax.default(baddbmm, -1, False);  baddbmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm: f16[16, 4096, 40] = torch.ops.aten.bmm.default(_softmax, _unsafe_view_5);  _softmax = _unsafe_view_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_7: f16[2, 8, 4096, 40] = torch.ops.aten.view.default(bmm, [2, 8, 4096, 40]);  bmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_4: f16[2, 4096, 8, 40] = torch.ops.aten.permute.default(view_7, [0, 2, 1, 3]);  view_7 = None\n",
      "        clone_3: f16[2, 4096, 8, 40] = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None\n",
      "        _unsafe_view_6: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(clone_3, [2, 4096, 320]);  clone_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_3: f16[320, 320] = torch.ops.aten.t.default(arg9_1);  arg9_1 = None\n",
      "        view_8: f16[8192, 320] = torch.ops.aten.view.default(_unsafe_view_6, [8192, 320]);  _unsafe_view_6 = None\n",
      "        addmm: f16[8192, 320] = torch.ops.aten.addmm.default(arg10_1, view_8, t_3);  arg10_1 = view_8 = t_3 = None\n",
      "        view_9: f16[2, 4096, 320] = torch.ops.aten.view.default(addmm, [2, 4096, 320]);  addmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:490, code: hidden_states = self.attn1(norm_hidden_states, attention_mask=attention_mask) + hidden_states\n",
      "        add: f16[2, 4096, 320] = torch.ops.aten.add.Tensor(view_9, view);  view_9 = view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:495, code: self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)\n",
      "        native_layer_norm_1 = torch.ops.aten.native_layer_norm.default(add, [320], arg11_1, arg12_1, 1e-05);  arg11_1 = arg12_1 = None\n",
      "        getitem_6: f16[2, 4096, 320] = native_layer_norm_1[0]\n",
      "        getitem_7: f32[2, 4096, 1] = native_layer_norm_1[1]\n",
      "        getitem_8: f32[2, 4096, 1] = native_layer_norm_1[2];  native_layer_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t_4: f16[320, 320] = torch.ops.aten.t.default(arg13_1);  arg13_1 = None\n",
      "        view_10: f16[8192, 320] = torch.ops.aten.view.default(getitem_6, [8192, 320]);  getitem_6 = None\n",
      "        mm_3: f16[8192, 320] = torch.ops.aten.mm.default(view_10, t_4);  view_10 = t_4 = None\n",
      "        _unsafe_view_7: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(mm_3, [2, 4096, 320]);  mm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_11: f16[2, 4096, 8, 40] = torch.ops.aten.view.default(_unsafe_view_7, [2, 4096, 8, 40]);  _unsafe_view_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_5: f16[2, 8, 4096, 40] = torch.ops.aten.permute.default(view_11, [0, 2, 1, 3]);  view_11 = None\n",
      "        clone_4: f16[2, 8, 4096, 40] = torch.ops.aten.clone.default(permute_5, memory_format = torch.contiguous_format);  permute_5 = None\n",
      "        _unsafe_view_8: f16[16, 4096, 40] = torch.ops.aten._unsafe_view.default(clone_4, [16, 4096, 40]);  clone_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_5: f16[768, 320] = torch.ops.aten.t.default(arg14_1);  arg14_1 = None\n",
      "        view_12: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768])\n",
      "        mm_4: f16[154, 320] = torch.ops.aten.mm.default(view_12, t_5);  view_12 = t_5 = None\n",
      "        _unsafe_view_9: f16[2, 77, 320] = torch.ops.aten._unsafe_view.default(mm_4, [2, 77, 320]);  mm_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_6: f16[768, 320] = torch.ops.aten.t.default(arg15_1);  arg15_1 = None\n",
      "        view_13: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768]);  arg27_1 = None\n",
      "        mm_5: f16[154, 320] = torch.ops.aten.mm.default(view_13, t_6);  view_13 = t_6 = None\n",
      "        _unsafe_view_10: f16[2, 77, 320] = torch.ops.aten._unsafe_view.default(mm_5, [2, 77, 320]);  mm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_14: f16[2, 77, 8, 40] = torch.ops.aten.view.default(_unsafe_view_9, [2, 77, 8, 40]);  _unsafe_view_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_6: f16[2, 8, 77, 40] = torch.ops.aten.permute.default(view_14, [0, 2, 1, 3]);  view_14 = None\n",
      "        clone_5: f16[2, 8, 77, 40] = torch.ops.aten.clone.default(permute_6, memory_format = torch.contiguous_format);  permute_6 = None\n",
      "        _unsafe_view_11: f16[16, 77, 40] = torch.ops.aten._unsafe_view.default(clone_5, [16, 77, 40]);  clone_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_15: f16[2, 77, 8, 40] = torch.ops.aten.view.default(_unsafe_view_10, [2, 77, 8, 40]);  _unsafe_view_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_7: f16[2, 8, 77, 40] = torch.ops.aten.permute.default(view_15, [0, 2, 1, 3]);  view_15 = None\n",
      "        clone_6: f16[2, 8, 77, 40] = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None\n",
      "        _unsafe_view_12: f16[16, 77, 40] = torch.ops.aten._unsafe_view.default(clone_6, [16, 77, 40]);  clone_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty_1: f16[16, 4096, 77] = torch.ops.aten.empty.memory_format([16, 4096, 77], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose_1: f16[16, 40, 77] = torch.ops.aten.transpose.int(_unsafe_view_11, -1, -2);  _unsafe_view_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm_1: f16[16, 4096, 77] = torch.ops.aten.baddbmm.default(empty_1, _unsafe_view_8, transpose_1, beta = 0, alpha = 0.15811388300841897);  empty_1 = _unsafe_view_8 = transpose_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax_1: f16[16, 4096, 77] = torch.ops.aten._softmax.default(baddbmm_1, -1, False);  baddbmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm_1: f16[16, 4096, 40] = torch.ops.aten.bmm.default(_softmax_1, _unsafe_view_12);  _softmax_1 = _unsafe_view_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_16: f16[2, 8, 4096, 40] = torch.ops.aten.view.default(bmm_1, [2, 8, 4096, 40]);  bmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_8: f16[2, 4096, 8, 40] = torch.ops.aten.permute.default(view_16, [0, 2, 1, 3]);  view_16 = None\n",
      "        clone_7: f16[2, 4096, 8, 40] = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None\n",
      "        _unsafe_view_13: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(clone_7, [2, 4096, 320]);  clone_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_7: f16[320, 320] = torch.ops.aten.t.default(arg16_1);  arg16_1 = None\n",
      "        view_17: f16[8192, 320] = torch.ops.aten.view.default(_unsafe_view_13, [8192, 320]);  _unsafe_view_13 = None\n",
      "        addmm_1: f16[8192, 320] = torch.ops.aten.addmm.default(arg17_1, view_17, t_7);  arg17_1 = view_17 = t_7 = None\n",
      "        view_18: f16[2, 4096, 320] = torch.ops.aten.view.default(addmm_1, [2, 4096, 320]);  addmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:498, code: self.attn2(\n",
      "        add_1: f16[2, 4096, 320] = torch.ops.aten.add.Tensor(view_18, add);  view_18 = add = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        native_layer_norm_2 = torch.ops.aten.native_layer_norm.default(add_1, [320], arg18_1, arg19_1, 1e-05);  arg18_1 = arg19_1 = None\n",
      "        getitem_9: f16[2, 4096, 320] = native_layer_norm_2[0]\n",
      "        getitem_10: f32[2, 4096, 1] = native_layer_norm_2[1]\n",
      "        getitem_11: f32[2, 4096, 1] = native_layer_norm_2[2];  native_layer_norm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:820, code: hidden_states, gate = self.proj(hidden_states).chunk(2, dim=-1)\n",
      "        t_8: f16[320, 2560] = torch.ops.aten.t.default(arg20_1);  arg20_1 = None\n",
      "        view_19: f16[8192, 320] = torch.ops.aten.view.default(getitem_9, [8192, 320]);  getitem_9 = None\n",
      "        addmm_2: f16[8192, 2560] = torch.ops.aten.addmm.default(arg21_1, view_19, t_8);  arg21_1 = view_19 = t_8 = None\n",
      "        view_20: f16[2, 4096, 2560] = torch.ops.aten.view.default(addmm_2, [2, 4096, 2560]);  addmm_2 = None\n",
      "        split = torch.ops.aten.split.Tensor(view_20, 1280, -1);  view_20 = None\n",
      "        getitem_12: f16[2, 4096, 1280] = split[0]\n",
      "        getitem_13: f16[2, 4096, 1280] = split[1];  split = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:815, code: return F.gelu(gate)\n",
      "        gelu: f16[2, 4096, 1280] = torch.ops.aten.gelu.default(getitem_13);  getitem_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:821, code: return hidden_states * self.gelu(gate)\n",
      "        mul: f16[2, 4096, 1280] = torch.ops.aten.mul.Tensor(getitem_12, gelu);  getitem_12 = gelu = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:774, code: hidden_states = module(hidden_states)\n",
      "        t_9: f16[1280, 320] = torch.ops.aten.t.default(arg22_1);  arg22_1 = None\n",
      "        view_21: f16[8192, 1280] = torch.ops.aten.view.default(mul, [8192, 1280]);  mul = None\n",
      "        addmm_3: f16[8192, 320] = torch.ops.aten.addmm.default(arg23_1, view_21, t_9);  arg23_1 = view_21 = t_9 = None\n",
      "        view_22: f16[2, 4096, 320] = torch.ops.aten.view.default(addmm_3, [2, 4096, 320]);  addmm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        add_2: f16[2, 4096, 320] = torch.ops.aten.add.Tensor(view_22, add_1);  view_22 = add_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:222, code: hidden_states.reshape(batch, height, weight, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
      "        view_23: f16[2, 64, 64, 320] = torch.ops.aten.view.default(add_2, [2, 64, 64, 320]);  add_2 = None\n",
      "        permute_9: f16[2, 320, 64, 64] = torch.ops.aten.permute.default(view_23, [0, 3, 1, 2]);  view_23 = None\n",
      "        clone_8: f16[2, 320, 64, 64] = torch.ops.aten.clone.default(permute_9, memory_format = torch.contiguous_format);  permute_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:224, code: hidden_states = self.proj_out(hidden_states)\n",
      "        convolution_1: f16[2, 320, 64, 64] = torch.ops.aten.convolution.default(clone_8, arg24_1, arg25_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  clone_8 = arg24_1 = arg25_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:231, code: output = hidden_states + residual\n",
      "        add_3: f16[2, 320, 64, 64] = torch.ops.aten.add.Tensor(convolution_1, arg26_1);  convolution_1 = arg26_1 = None\n",
      "        return (add_3,)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[640], arg1_1: f16[640], arg2_1: f16[320, 640, 3, 3], arg3_1: f16[320], arg4_1: f16[320, 1280], arg5_1: f16[320], arg6_1: f16[320], arg7_1: f16[320], arg8_1: f16[320, 320, 3, 3], arg9_1: f16[320], arg10_1: f16[320, 640, 1, 1], arg11_1: f16[320], arg12_1: f16[2, 640, 64, 64], arg13_1: f16[2, 1280]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg12_1, arg0_1, arg1_1, 2, 640, 4096, 32, 1e-05);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 640, 64, 64] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu: f16[2, 640, 64, 64] = torch.ops.aten.silu.default(getitem);  getitem = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution: f16[2, 320, 64, 64] = torch.ops.aten.convolution.default(silu, arg2_1, arg3_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:474, code: temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
      "        silu_1: f16[2, 1280] = torch.ops.aten.silu.default(arg13_1);  arg13_1 = None\n",
      "        t: f16[1280, 320] = torch.ops.aten.t.default(arg4_1);  arg4_1 = None\n",
      "        addmm: f16[2, 320] = torch.ops.aten.addmm.default(arg5_1, silu_1, t);  arg5_1 = silu_1 = t = None\n",
      "        slice_1: f16[2, 320] = torch.ops.aten.slice.Tensor(addmm, 0, 0, 9223372036854775807);  addmm = None\n",
      "        slice_2: f16[2, 320] = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807);  slice_1 = None\n",
      "        unsqueeze: f16[2, 320, 1] = torch.ops.aten.unsqueeze.default(slice_2, 2);  slice_2 = None\n",
      "        unsqueeze_1: f16[2, 320, 1, 1] = torch.ops.aten.unsqueeze.default(unsqueeze, 3);  unsqueeze = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:477, code: hidden_states = hidden_states + temb\n",
      "        add: f16[2, 320, 64, 64] = torch.ops.aten.add.Tensor(convolution, unsqueeze_1);  convolution = unsqueeze_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_1 = torch.ops.aten.native_group_norm.default(add, arg6_1, arg7_1, 2, 320, 4096, 32, 1e-05);  add = arg6_1 = arg7_1 = None\n",
      "        getitem_3: f16[2, 320, 64, 64] = native_group_norm_1[0]\n",
      "        getitem_4: f16[2, 32] = native_group_norm_1[1]\n",
      "        getitem_5: f16[2, 32] = native_group_norm_1[2];  native_group_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_2: f16[2, 320, 64, 64] = torch.ops.aten.silu.default(getitem_3);  getitem_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_1: f16[2, 320, 64, 64] = torch.ops.aten.convolution.default(silu_2, arg8_1, arg9_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_2 = arg8_1 = arg9_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:491, code: input_tensor = self.conv_shortcut(input_tensor)\n",
      "        convolution_2: f16[2, 320, 64, 64] = torch.ops.aten.convolution.default(arg12_1, arg10_1, arg11_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  arg12_1 = arg10_1 = arg11_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_1: f16[2, 320, 64, 64] = torch.ops.aten.add.Tensor(convolution_2, convolution_1);  convolution_2 = convolution_1 = None\n",
      "        div: f16[2, 320, 64, 64] = torch.ops.aten.div.Tensor(add_1, 1.0);  add_1 = None\n",
      "        return (div,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[320], arg1_1: f16[320], arg2_1: f16[320, 320, 1, 1], arg3_1: f16[320], arg4_1: f16[320], arg5_1: f16[320], arg6_1: f16[320, 320], arg7_1: f16[320, 320], arg8_1: f16[320, 320], arg9_1: f16[320, 320], arg10_1: f16[320], arg11_1: f16[320], arg12_1: f16[320], arg13_1: f16[320, 320], arg14_1: f16[320, 768], arg15_1: f16[320, 768], arg16_1: f16[320, 320], arg17_1: f16[320], arg18_1: f16[320], arg19_1: f16[320], arg20_1: f16[2560, 320], arg21_1: f16[2560], arg22_1: f16[320, 1280], arg23_1: f16[320], arg24_1: f16[320, 320, 1, 1], arg25_1: f16[320], arg26_1: f16[2, 320, 64, 64], arg27_1: f16[2, 77, 768]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:202, code: hidden_states = self.norm(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg26_1, arg0_1, arg1_1, 2, 320, 4096, 32, 1e-06);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 320, 64, 64] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:204, code: hidden_states = self.proj_in(hidden_states)\n",
      "        convolution: f16[2, 320, 64, 64] = torch.ops.aten.convolution.default(getitem, arg2_1, arg3_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  getitem = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:206, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * weight, inner_dim)\n",
      "        permute: f16[2, 64, 64, 320] = torch.ops.aten.permute.default(convolution, [0, 2, 3, 1]);  convolution = None\n",
      "        view: f16[2, 4096, 320] = torch.ops.aten.view.default(permute, [2, 4096, 320]);  permute = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:482, code: self.norm1(hidden_states, timestep) if self.use_ada_layer_norm else self.norm1(hidden_states)\n",
      "        native_layer_norm = torch.ops.aten.native_layer_norm.default(view, [320], arg4_1, arg5_1, 1e-05);  arg4_1 = arg5_1 = None\n",
      "        getitem_3: f16[2, 4096, 320] = native_layer_norm[0]\n",
      "        getitem_4: f32[2, 4096, 1] = native_layer_norm[1]\n",
      "        getitem_5: f32[2, 4096, 1] = native_layer_norm[2];  native_layer_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t: f16[320, 320] = torch.ops.aten.t.default(arg6_1);  arg6_1 = None\n",
      "        view_1: f16[8192, 320] = torch.ops.aten.view.default(getitem_3, [8192, 320])\n",
      "        mm: f16[8192, 320] = torch.ops.aten.mm.default(view_1, t);  view_1 = t = None\n",
      "        _unsafe_view: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(mm, [2, 4096, 320]);  mm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_2: f16[2, 4096, 8, 40] = torch.ops.aten.view.default(_unsafe_view, [2, 4096, 8, 40]);  _unsafe_view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_1: f16[2, 8, 4096, 40] = torch.ops.aten.permute.default(view_2, [0, 2, 1, 3]);  view_2 = None\n",
      "        clone: f16[2, 8, 4096, 40] = torch.ops.aten.clone.default(permute_1, memory_format = torch.contiguous_format);  permute_1 = None\n",
      "        _unsafe_view_1: f16[16, 4096, 40] = torch.ops.aten._unsafe_view.default(clone, [16, 4096, 40]);  clone = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_1: f16[320, 320] = torch.ops.aten.t.default(arg7_1);  arg7_1 = None\n",
      "        view_3: f16[8192, 320] = torch.ops.aten.view.default(getitem_3, [8192, 320])\n",
      "        mm_1: f16[8192, 320] = torch.ops.aten.mm.default(view_3, t_1);  view_3 = t_1 = None\n",
      "        _unsafe_view_2: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(mm_1, [2, 4096, 320]);  mm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_2: f16[320, 320] = torch.ops.aten.t.default(arg8_1);  arg8_1 = None\n",
      "        view_4: f16[8192, 320] = torch.ops.aten.view.default(getitem_3, [8192, 320]);  getitem_3 = None\n",
      "        mm_2: f16[8192, 320] = torch.ops.aten.mm.default(view_4, t_2);  view_4 = t_2 = None\n",
      "        _unsafe_view_3: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(mm_2, [2, 4096, 320]);  mm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_5: f16[2, 4096, 8, 40] = torch.ops.aten.view.default(_unsafe_view_2, [2, 4096, 8, 40]);  _unsafe_view_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_2: f16[2, 8, 4096, 40] = torch.ops.aten.permute.default(view_5, [0, 2, 1, 3]);  view_5 = None\n",
      "        clone_1: f16[2, 8, 4096, 40] = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None\n",
      "        _unsafe_view_4: f16[16, 4096, 40] = torch.ops.aten._unsafe_view.default(clone_1, [16, 4096, 40]);  clone_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_6: f16[2, 4096, 8, 40] = torch.ops.aten.view.default(_unsafe_view_3, [2, 4096, 8, 40]);  _unsafe_view_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_3: f16[2, 8, 4096, 40] = torch.ops.aten.permute.default(view_6, [0, 2, 1, 3]);  view_6 = None\n",
      "        clone_2: f16[2, 8, 4096, 40] = torch.ops.aten.clone.default(permute_3, memory_format = torch.contiguous_format);  permute_3 = None\n",
      "        _unsafe_view_5: f16[16, 4096, 40] = torch.ops.aten._unsafe_view.default(clone_2, [16, 4096, 40]);  clone_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty: f16[16, 4096, 4096] = torch.ops.aten.empty.memory_format([16, 4096, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose: f16[16, 40, 4096] = torch.ops.aten.transpose.int(_unsafe_view_4, -1, -2);  _unsafe_view_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm: f16[16, 4096, 4096] = torch.ops.aten.baddbmm.default(empty, _unsafe_view_1, transpose, beta = 0, alpha = 0.15811388300841897);  empty = _unsafe_view_1 = transpose = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax: f16[16, 4096, 4096] = torch.ops.aten._softmax.default(baddbmm, -1, False);  baddbmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm: f16[16, 4096, 40] = torch.ops.aten.bmm.default(_softmax, _unsafe_view_5);  _softmax = _unsafe_view_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_7: f16[2, 8, 4096, 40] = torch.ops.aten.view.default(bmm, [2, 8, 4096, 40]);  bmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_4: f16[2, 4096, 8, 40] = torch.ops.aten.permute.default(view_7, [0, 2, 1, 3]);  view_7 = None\n",
      "        clone_3: f16[2, 4096, 8, 40] = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None\n",
      "        _unsafe_view_6: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(clone_3, [2, 4096, 320]);  clone_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_3: f16[320, 320] = torch.ops.aten.t.default(arg9_1);  arg9_1 = None\n",
      "        view_8: f16[8192, 320] = torch.ops.aten.view.default(_unsafe_view_6, [8192, 320]);  _unsafe_view_6 = None\n",
      "        addmm: f16[8192, 320] = torch.ops.aten.addmm.default(arg10_1, view_8, t_3);  arg10_1 = view_8 = t_3 = None\n",
      "        view_9: f16[2, 4096, 320] = torch.ops.aten.view.default(addmm, [2, 4096, 320]);  addmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:490, code: hidden_states = self.attn1(norm_hidden_states, attention_mask=attention_mask) + hidden_states\n",
      "        add: f16[2, 4096, 320] = torch.ops.aten.add.Tensor(view_9, view);  view_9 = view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:495, code: self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)\n",
      "        native_layer_norm_1 = torch.ops.aten.native_layer_norm.default(add, [320], arg11_1, arg12_1, 1e-05);  arg11_1 = arg12_1 = None\n",
      "        getitem_6: f16[2, 4096, 320] = native_layer_norm_1[0]\n",
      "        getitem_7: f32[2, 4096, 1] = native_layer_norm_1[1]\n",
      "        getitem_8: f32[2, 4096, 1] = native_layer_norm_1[2];  native_layer_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t_4: f16[320, 320] = torch.ops.aten.t.default(arg13_1);  arg13_1 = None\n",
      "        view_10: f16[8192, 320] = torch.ops.aten.view.default(getitem_6, [8192, 320]);  getitem_6 = None\n",
      "        mm_3: f16[8192, 320] = torch.ops.aten.mm.default(view_10, t_4);  view_10 = t_4 = None\n",
      "        _unsafe_view_7: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(mm_3, [2, 4096, 320]);  mm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_11: f16[2, 4096, 8, 40] = torch.ops.aten.view.default(_unsafe_view_7, [2, 4096, 8, 40]);  _unsafe_view_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_5: f16[2, 8, 4096, 40] = torch.ops.aten.permute.default(view_11, [0, 2, 1, 3]);  view_11 = None\n",
      "        clone_4: f16[2, 8, 4096, 40] = torch.ops.aten.clone.default(permute_5, memory_format = torch.contiguous_format);  permute_5 = None\n",
      "        _unsafe_view_8: f16[16, 4096, 40] = torch.ops.aten._unsafe_view.default(clone_4, [16, 4096, 40]);  clone_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_5: f16[768, 320] = torch.ops.aten.t.default(arg14_1);  arg14_1 = None\n",
      "        view_12: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768])\n",
      "        mm_4: f16[154, 320] = torch.ops.aten.mm.default(view_12, t_5);  view_12 = t_5 = None\n",
      "        _unsafe_view_9: f16[2, 77, 320] = torch.ops.aten._unsafe_view.default(mm_4, [2, 77, 320]);  mm_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_6: f16[768, 320] = torch.ops.aten.t.default(arg15_1);  arg15_1 = None\n",
      "        view_13: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768]);  arg27_1 = None\n",
      "        mm_5: f16[154, 320] = torch.ops.aten.mm.default(view_13, t_6);  view_13 = t_6 = None\n",
      "        _unsafe_view_10: f16[2, 77, 320] = torch.ops.aten._unsafe_view.default(mm_5, [2, 77, 320]);  mm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_14: f16[2, 77, 8, 40] = torch.ops.aten.view.default(_unsafe_view_9, [2, 77, 8, 40]);  _unsafe_view_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_6: f16[2, 8, 77, 40] = torch.ops.aten.permute.default(view_14, [0, 2, 1, 3]);  view_14 = None\n",
      "        clone_5: f16[2, 8, 77, 40] = torch.ops.aten.clone.default(permute_6, memory_format = torch.contiguous_format);  permute_6 = None\n",
      "        _unsafe_view_11: f16[16, 77, 40] = torch.ops.aten._unsafe_view.default(clone_5, [16, 77, 40]);  clone_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_15: f16[2, 77, 8, 40] = torch.ops.aten.view.default(_unsafe_view_10, [2, 77, 8, 40]);  _unsafe_view_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_7: f16[2, 8, 77, 40] = torch.ops.aten.permute.default(view_15, [0, 2, 1, 3]);  view_15 = None\n",
      "        clone_6: f16[2, 8, 77, 40] = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None\n",
      "        _unsafe_view_12: f16[16, 77, 40] = torch.ops.aten._unsafe_view.default(clone_6, [16, 77, 40]);  clone_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty_1: f16[16, 4096, 77] = torch.ops.aten.empty.memory_format([16, 4096, 77], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose_1: f16[16, 40, 77] = torch.ops.aten.transpose.int(_unsafe_view_11, -1, -2);  _unsafe_view_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm_1: f16[16, 4096, 77] = torch.ops.aten.baddbmm.default(empty_1, _unsafe_view_8, transpose_1, beta = 0, alpha = 0.15811388300841897);  empty_1 = _unsafe_view_8 = transpose_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax_1: f16[16, 4096, 77] = torch.ops.aten._softmax.default(baddbmm_1, -1, False);  baddbmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm_1: f16[16, 4096, 40] = torch.ops.aten.bmm.default(_softmax_1, _unsafe_view_12);  _softmax_1 = _unsafe_view_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_16: f16[2, 8, 4096, 40] = torch.ops.aten.view.default(bmm_1, [2, 8, 4096, 40]);  bmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_8: f16[2, 4096, 8, 40] = torch.ops.aten.permute.default(view_16, [0, 2, 1, 3]);  view_16 = None\n",
      "        clone_7: f16[2, 4096, 8, 40] = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None\n",
      "        _unsafe_view_13: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(clone_7, [2, 4096, 320]);  clone_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_7: f16[320, 320] = torch.ops.aten.t.default(arg16_1);  arg16_1 = None\n",
      "        view_17: f16[8192, 320] = torch.ops.aten.view.default(_unsafe_view_13, [8192, 320]);  _unsafe_view_13 = None\n",
      "        addmm_1: f16[8192, 320] = torch.ops.aten.addmm.default(arg17_1, view_17, t_7);  arg17_1 = view_17 = t_7 = None\n",
      "        view_18: f16[2, 4096, 320] = torch.ops.aten.view.default(addmm_1, [2, 4096, 320]);  addmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:498, code: self.attn2(\n",
      "        add_1: f16[2, 4096, 320] = torch.ops.aten.add.Tensor(view_18, add);  view_18 = add = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        native_layer_norm_2 = torch.ops.aten.native_layer_norm.default(add_1, [320], arg18_1, arg19_1, 1e-05);  arg18_1 = arg19_1 = None\n",
      "        getitem_9: f16[2, 4096, 320] = native_layer_norm_2[0]\n",
      "        getitem_10: f32[2, 4096, 1] = native_layer_norm_2[1]\n",
      "        getitem_11: f32[2, 4096, 1] = native_layer_norm_2[2];  native_layer_norm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:820, code: hidden_states, gate = self.proj(hidden_states).chunk(2, dim=-1)\n",
      "        t_8: f16[320, 2560] = torch.ops.aten.t.default(arg20_1);  arg20_1 = None\n",
      "        view_19: f16[8192, 320] = torch.ops.aten.view.default(getitem_9, [8192, 320]);  getitem_9 = None\n",
      "        addmm_2: f16[8192, 2560] = torch.ops.aten.addmm.default(arg21_1, view_19, t_8);  arg21_1 = view_19 = t_8 = None\n",
      "        view_20: f16[2, 4096, 2560] = torch.ops.aten.view.default(addmm_2, [2, 4096, 2560]);  addmm_2 = None\n",
      "        split = torch.ops.aten.split.Tensor(view_20, 1280, -1);  view_20 = None\n",
      "        getitem_12: f16[2, 4096, 1280] = split[0]\n",
      "        getitem_13: f16[2, 4096, 1280] = split[1];  split = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:815, code: return F.gelu(gate)\n",
      "        gelu: f16[2, 4096, 1280] = torch.ops.aten.gelu.default(getitem_13);  getitem_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:821, code: return hidden_states * self.gelu(gate)\n",
      "        mul: f16[2, 4096, 1280] = torch.ops.aten.mul.Tensor(getitem_12, gelu);  getitem_12 = gelu = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:774, code: hidden_states = module(hidden_states)\n",
      "        t_9: f16[1280, 320] = torch.ops.aten.t.default(arg22_1);  arg22_1 = None\n",
      "        view_21: f16[8192, 1280] = torch.ops.aten.view.default(mul, [8192, 1280]);  mul = None\n",
      "        addmm_3: f16[8192, 320] = torch.ops.aten.addmm.default(arg23_1, view_21, t_9);  arg23_1 = view_21 = t_9 = None\n",
      "        view_22: f16[2, 4096, 320] = torch.ops.aten.view.default(addmm_3, [2, 4096, 320]);  addmm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        add_2: f16[2, 4096, 320] = torch.ops.aten.add.Tensor(view_22, add_1);  view_22 = add_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:222, code: hidden_states.reshape(batch, height, weight, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
      "        view_23: f16[2, 64, 64, 320] = torch.ops.aten.view.default(add_2, [2, 64, 64, 320]);  add_2 = None\n",
      "        permute_9: f16[2, 320, 64, 64] = torch.ops.aten.permute.default(view_23, [0, 3, 1, 2]);  view_23 = None\n",
      "        clone_8: f16[2, 320, 64, 64] = torch.ops.aten.clone.default(permute_9, memory_format = torch.contiguous_format);  permute_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:224, code: hidden_states = self.proj_out(hidden_states)\n",
      "        convolution_1: f16[2, 320, 64, 64] = torch.ops.aten.convolution.default(clone_8, arg24_1, arg25_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  clone_8 = arg24_1 = arg25_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:231, code: output = hidden_states + residual\n",
      "        add_3: f16[2, 320, 64, 64] = torch.ops.aten.add.Tensor(convolution_1, arg26_1);  convolution_1 = arg26_1 = None\n",
      "        return (add_3,)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[640], arg1_1: f16[640], arg2_1: f16[320, 640, 3, 3], arg3_1: f16[320], arg4_1: f16[320, 1280], arg5_1: f16[320], arg6_1: f16[320], arg7_1: f16[320], arg8_1: f16[320, 320, 3, 3], arg9_1: f16[320], arg10_1: f16[320, 640, 1, 1], arg11_1: f16[320], arg12_1: f16[2, 640, 64, 64], arg13_1: f16[2, 1280]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg12_1, arg0_1, arg1_1, 2, 640, 4096, 32, 1e-05);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 640, 64, 64] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu: f16[2, 640, 64, 64] = torch.ops.aten.silu.default(getitem);  getitem = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution: f16[2, 320, 64, 64] = torch.ops.aten.convolution.default(silu, arg2_1, arg3_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:474, code: temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
      "        silu_1: f16[2, 1280] = torch.ops.aten.silu.default(arg13_1);  arg13_1 = None\n",
      "        t: f16[1280, 320] = torch.ops.aten.t.default(arg4_1);  arg4_1 = None\n",
      "        addmm: f16[2, 320] = torch.ops.aten.addmm.default(arg5_1, silu_1, t);  arg5_1 = silu_1 = t = None\n",
      "        slice_1: f16[2, 320] = torch.ops.aten.slice.Tensor(addmm, 0, 0, 9223372036854775807);  addmm = None\n",
      "        slice_2: f16[2, 320] = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807);  slice_1 = None\n",
      "        unsqueeze: f16[2, 320, 1] = torch.ops.aten.unsqueeze.default(slice_2, 2);  slice_2 = None\n",
      "        unsqueeze_1: f16[2, 320, 1, 1] = torch.ops.aten.unsqueeze.default(unsqueeze, 3);  unsqueeze = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:477, code: hidden_states = hidden_states + temb\n",
      "        add: f16[2, 320, 64, 64] = torch.ops.aten.add.Tensor(convolution, unsqueeze_1);  convolution = unsqueeze_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_1 = torch.ops.aten.native_group_norm.default(add, arg6_1, arg7_1, 2, 320, 4096, 32, 1e-05);  add = arg6_1 = arg7_1 = None\n",
      "        getitem_3: f16[2, 320, 64, 64] = native_group_norm_1[0]\n",
      "        getitem_4: f16[2, 32] = native_group_norm_1[1]\n",
      "        getitem_5: f16[2, 32] = native_group_norm_1[2];  native_group_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_2: f16[2, 320, 64, 64] = torch.ops.aten.silu.default(getitem_3);  getitem_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_1: f16[2, 320, 64, 64] = torch.ops.aten.convolution.default(silu_2, arg8_1, arg9_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_2 = arg8_1 = arg9_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:491, code: input_tensor = self.conv_shortcut(input_tensor)\n",
      "        convolution_2: f16[2, 320, 64, 64] = torch.ops.aten.convolution.default(arg12_1, arg10_1, arg11_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  arg12_1 = arg10_1 = arg11_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_1: f16[2, 320, 64, 64] = torch.ops.aten.add.Tensor(convolution_2, convolution_1);  convolution_2 = convolution_1 = None\n",
      "        div: f16[2, 320, 64, 64] = torch.ops.aten.div.Tensor(add_1, 1.0);  add_1 = None\n",
      "        return (div,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[320], arg1_1: f16[320], arg2_1: f16[320, 320, 1, 1], arg3_1: f16[320], arg4_1: f16[320], arg5_1: f16[320], arg6_1: f16[320, 320], arg7_1: f16[320, 320], arg8_1: f16[320, 320], arg9_1: f16[320, 320], arg10_1: f16[320], arg11_1: f16[320], arg12_1: f16[320], arg13_1: f16[320, 320], arg14_1: f16[320, 768], arg15_1: f16[320, 768], arg16_1: f16[320, 320], arg17_1: f16[320], arg18_1: f16[320], arg19_1: f16[320], arg20_1: f16[2560, 320], arg21_1: f16[2560], arg22_1: f16[320, 1280], arg23_1: f16[320], arg24_1: f16[320, 320, 1, 1], arg25_1: f16[320], arg26_1: f16[2, 320, 64, 64], arg27_1: f16[2, 77, 768]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:202, code: hidden_states = self.norm(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(arg26_1, arg0_1, arg1_1, 2, 320, 4096, 32, 1e-06);  arg0_1 = arg1_1 = None\n",
      "        getitem: f16[2, 320, 64, 64] = native_group_norm[0]\n",
      "        getitem_1: f16[2, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[2, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:204, code: hidden_states = self.proj_in(hidden_states)\n",
      "        convolution: f16[2, 320, 64, 64] = torch.ops.aten.convolution.default(getitem, arg2_1, arg3_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  getitem = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:206, code: hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * weight, inner_dim)\n",
      "        permute: f16[2, 64, 64, 320] = torch.ops.aten.permute.default(convolution, [0, 2, 3, 1]);  convolution = None\n",
      "        view: f16[2, 4096, 320] = torch.ops.aten.view.default(permute, [2, 4096, 320]);  permute = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:482, code: self.norm1(hidden_states, timestep) if self.use_ada_layer_norm else self.norm1(hidden_states)\n",
      "        native_layer_norm = torch.ops.aten.native_layer_norm.default(view, [320], arg4_1, arg5_1, 1e-05);  arg4_1 = arg5_1 = None\n",
      "        getitem_3: f16[2, 4096, 320] = native_layer_norm[0]\n",
      "        getitem_4: f32[2, 4096, 1] = native_layer_norm[1]\n",
      "        getitem_5: f32[2, 4096, 1] = native_layer_norm[2];  native_layer_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t: f16[320, 320] = torch.ops.aten.t.default(arg6_1);  arg6_1 = None\n",
      "        view_1: f16[8192, 320] = torch.ops.aten.view.default(getitem_3, [8192, 320])\n",
      "        mm: f16[8192, 320] = torch.ops.aten.mm.default(view_1, t);  view_1 = t = None\n",
      "        _unsafe_view: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(mm, [2, 4096, 320]);  mm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_2: f16[2, 4096, 8, 40] = torch.ops.aten.view.default(_unsafe_view, [2, 4096, 8, 40]);  _unsafe_view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_1: f16[2, 8, 4096, 40] = torch.ops.aten.permute.default(view_2, [0, 2, 1, 3]);  view_2 = None\n",
      "        clone: f16[2, 8, 4096, 40] = torch.ops.aten.clone.default(permute_1, memory_format = torch.contiguous_format);  permute_1 = None\n",
      "        _unsafe_view_1: f16[16, 4096, 40] = torch.ops.aten._unsafe_view.default(clone, [16, 4096, 40]);  clone = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_1: f16[320, 320] = torch.ops.aten.t.default(arg7_1);  arg7_1 = None\n",
      "        view_3: f16[8192, 320] = torch.ops.aten.view.default(getitem_3, [8192, 320])\n",
      "        mm_1: f16[8192, 320] = torch.ops.aten.mm.default(view_3, t_1);  view_3 = t_1 = None\n",
      "        _unsafe_view_2: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(mm_1, [2, 4096, 320]);  mm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_2: f16[320, 320] = torch.ops.aten.t.default(arg8_1);  arg8_1 = None\n",
      "        view_4: f16[8192, 320] = torch.ops.aten.view.default(getitem_3, [8192, 320]);  getitem_3 = None\n",
      "        mm_2: f16[8192, 320] = torch.ops.aten.mm.default(view_4, t_2);  view_4 = t_2 = None\n",
      "        _unsafe_view_3: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(mm_2, [2, 4096, 320]);  mm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_5: f16[2, 4096, 8, 40] = torch.ops.aten.view.default(_unsafe_view_2, [2, 4096, 8, 40]);  _unsafe_view_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_2: f16[2, 8, 4096, 40] = torch.ops.aten.permute.default(view_5, [0, 2, 1, 3]);  view_5 = None\n",
      "        clone_1: f16[2, 8, 4096, 40] = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None\n",
      "        _unsafe_view_4: f16[16, 4096, 40] = torch.ops.aten._unsafe_view.default(clone_1, [16, 4096, 40]);  clone_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_6: f16[2, 4096, 8, 40] = torch.ops.aten.view.default(_unsafe_view_3, [2, 4096, 8, 40]);  _unsafe_view_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_3: f16[2, 8, 4096, 40] = torch.ops.aten.permute.default(view_6, [0, 2, 1, 3]);  view_6 = None\n",
      "        clone_2: f16[2, 8, 4096, 40] = torch.ops.aten.clone.default(permute_3, memory_format = torch.contiguous_format);  permute_3 = None\n",
      "        _unsafe_view_5: f16[16, 4096, 40] = torch.ops.aten._unsafe_view.default(clone_2, [16, 4096, 40]);  clone_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty: f16[16, 4096, 4096] = torch.ops.aten.empty.memory_format([16, 4096, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose: f16[16, 40, 4096] = torch.ops.aten.transpose.int(_unsafe_view_4, -1, -2);  _unsafe_view_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm: f16[16, 4096, 4096] = torch.ops.aten.baddbmm.default(empty, _unsafe_view_1, transpose, beta = 0, alpha = 0.15811388300841897);  empty = _unsafe_view_1 = transpose = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax: f16[16, 4096, 4096] = torch.ops.aten._softmax.default(baddbmm, -1, False);  baddbmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm: f16[16, 4096, 40] = torch.ops.aten.bmm.default(_softmax, _unsafe_view_5);  _softmax = _unsafe_view_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_7: f16[2, 8, 4096, 40] = torch.ops.aten.view.default(bmm, [2, 8, 4096, 40]);  bmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_4: f16[2, 4096, 8, 40] = torch.ops.aten.permute.default(view_7, [0, 2, 1, 3]);  view_7 = None\n",
      "        clone_3: f16[2, 4096, 8, 40] = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None\n",
      "        _unsafe_view_6: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(clone_3, [2, 4096, 320]);  clone_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_3: f16[320, 320] = torch.ops.aten.t.default(arg9_1);  arg9_1 = None\n",
      "        view_8: f16[8192, 320] = torch.ops.aten.view.default(_unsafe_view_6, [8192, 320]);  _unsafe_view_6 = None\n",
      "        addmm: f16[8192, 320] = torch.ops.aten.addmm.default(arg10_1, view_8, t_3);  arg10_1 = view_8 = t_3 = None\n",
      "        view_9: f16[2, 4096, 320] = torch.ops.aten.view.default(addmm, [2, 4096, 320]);  addmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:490, code: hidden_states = self.attn1(norm_hidden_states, attention_mask=attention_mask) + hidden_states\n",
      "        add: f16[2, 4096, 320] = torch.ops.aten.add.Tensor(view_9, view);  view_9 = view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:495, code: self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)\n",
      "        native_layer_norm_1 = torch.ops.aten.native_layer_norm.default(add, [320], arg11_1, arg12_1, 1e-05);  arg11_1 = arg12_1 = None\n",
      "        getitem_6: f16[2, 4096, 320] = native_layer_norm_1[0]\n",
      "        getitem_7: f32[2, 4096, 1] = native_layer_norm_1[1]\n",
      "        getitem_8: f32[2, 4096, 1] = native_layer_norm_1[2];  native_layer_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:600, code: query = self.to_q(hidden_states)\n",
      "        t_4: f16[320, 320] = torch.ops.aten.t.default(arg13_1);  arg13_1 = None\n",
      "        view_10: f16[8192, 320] = torch.ops.aten.view.default(getitem_6, [8192, 320]);  getitem_6 = None\n",
      "        mm_3: f16[8192, 320] = torch.ops.aten.mm.default(view_10, t_4);  view_10 = t_4 = None\n",
      "        _unsafe_view_7: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(mm_3, [2, 4096, 320]);  mm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_11: f16[2, 4096, 8, 40] = torch.ops.aten.view.default(_unsafe_view_7, [2, 4096, 8, 40]);  _unsafe_view_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_5: f16[2, 8, 4096, 40] = torch.ops.aten.permute.default(view_11, [0, 2, 1, 3]);  view_11 = None\n",
      "        clone_4: f16[2, 8, 4096, 40] = torch.ops.aten.clone.default(permute_5, memory_format = torch.contiguous_format);  permute_5 = None\n",
      "        _unsafe_view_8: f16[16, 4096, 40] = torch.ops.aten._unsafe_view.default(clone_4, [16, 4096, 40]);  clone_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:619, code: key = self.to_k(encoder_hidden_states)\n",
      "        t_5: f16[768, 320] = torch.ops.aten.t.default(arg14_1);  arg14_1 = None\n",
      "        view_12: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768])\n",
      "        mm_4: f16[154, 320] = torch.ops.aten.mm.default(view_12, t_5);  view_12 = t_5 = None\n",
      "        _unsafe_view_9: f16[2, 77, 320] = torch.ops.aten._unsafe_view.default(mm_4, [2, 77, 320]);  mm_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:620, code: value = self.to_v(encoder_hidden_states)\n",
      "        t_6: f16[768, 320] = torch.ops.aten.t.default(arg15_1);  arg15_1 = None\n",
      "        view_13: f16[154, 768] = torch.ops.aten.view.default(arg27_1, [154, 768]);  arg27_1 = None\n",
      "        mm_5: f16[154, 320] = torch.ops.aten.mm.default(view_13, t_6);  view_13 = t_6 = None\n",
      "        _unsafe_view_10: f16[2, 77, 320] = torch.ops.aten._unsafe_view.default(mm_5, [2, 77, 320]);  mm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_14: f16[2, 77, 8, 40] = torch.ops.aten.view.default(_unsafe_view_9, [2, 77, 8, 40]);  _unsafe_view_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_6: f16[2, 8, 77, 40] = torch.ops.aten.permute.default(view_14, [0, 2, 1, 3]);  view_14 = None\n",
      "        clone_5: f16[2, 8, 77, 40] = torch.ops.aten.clone.default(permute_6, memory_format = torch.contiguous_format);  permute_6 = None\n",
      "        _unsafe_view_11: f16[16, 77, 40] = torch.ops.aten._unsafe_view.default(clone_5, [16, 77, 40]);  clone_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:575, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_15: f16[2, 77, 8, 40] = torch.ops.aten.view.default(_unsafe_view_10, [2, 77, 8, 40]);  _unsafe_view_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:576, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_7: f16[2, 8, 77, 40] = torch.ops.aten.permute.default(view_15, [0, 2, 1, 3]);  view_15 = None\n",
      "        clone_6: f16[2, 8, 77, 40] = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None\n",
      "        _unsafe_view_12: f16[16, 77, 40] = torch.ops.aten._unsafe_view.default(clone_6, [16, 77, 40]);  clone_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:655, code: torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
      "        empty_1: f16[16, 4096, 77] = torch.ops.aten.empty.memory_format([16, 4096, 77], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:657, code: key.transpose(-1, -2),\n",
      "        transpose_1: f16[16, 40, 77] = torch.ops.aten.transpose.int(_unsafe_view_11, -1, -2);  _unsafe_view_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:654, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm_1: f16[16, 4096, 77] = torch.ops.aten.baddbmm.default(empty_1, _unsafe_view_8, transpose_1, beta = 0, alpha = 0.15811388300841897);  empty_1 = _unsafe_view_8 = transpose_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:668, code: attention_probs = attention_scores.softmax(dim=-1)\n",
      "        _softmax_1: f16[16, 4096, 77] = torch.ops.aten._softmax.default(baddbmm_1, -1, False);  baddbmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:674, code: hidden_states = torch.bmm(attention_probs, value)\n",
      "        bmm_1: f16[16, 4096, 40] = torch.ops.aten.bmm.default(_softmax_1, _unsafe_view_12);  _softmax_1 = _unsafe_view_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:582, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_16: f16[2, 8, 4096, 40] = torch.ops.aten.view.default(bmm_1, [2, 8, 4096, 40]);  bmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:583, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_8: f16[2, 4096, 8, 40] = torch.ops.aten.permute.default(view_16, [0, 2, 1, 3]);  view_16 = None\n",
      "        clone_7: f16[2, 4096, 8, 40] = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None\n",
      "        _unsafe_view_13: f16[2, 4096, 320] = torch.ops.aten._unsafe_view.default(clone_7, [2, 4096, 320]);  clone_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:643, code: hidden_states = self.to_out[0](hidden_states)\n",
      "        t_7: f16[320, 320] = torch.ops.aten.t.default(arg16_1);  arg16_1 = None\n",
      "        view_17: f16[8192, 320] = torch.ops.aten.view.default(_unsafe_view_13, [8192, 320]);  _unsafe_view_13 = None\n",
      "        addmm_1: f16[8192, 320] = torch.ops.aten.addmm.default(arg17_1, view_17, t_7);  arg17_1 = view_17 = t_7 = None\n",
      "        view_18: f16[2, 4096, 320] = torch.ops.aten.view.default(addmm_1, [2, 4096, 320]);  addmm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:498, code: self.attn2(\n",
      "        add_1: f16[2, 4096, 320] = torch.ops.aten.add.Tensor(view_18, add);  view_18 = add = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        native_layer_norm_2 = torch.ops.aten.native_layer_norm.default(add_1, [320], arg18_1, arg19_1, 1e-05);  arg18_1 = arg19_1 = None\n",
      "        getitem_9: f16[2, 4096, 320] = native_layer_norm_2[0]\n",
      "        getitem_10: f32[2, 4096, 1] = native_layer_norm_2[1]\n",
      "        getitem_11: f32[2, 4096, 1] = native_layer_norm_2[2];  native_layer_norm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:820, code: hidden_states, gate = self.proj(hidden_states).chunk(2, dim=-1)\n",
      "        t_8: f16[320, 2560] = torch.ops.aten.t.default(arg20_1);  arg20_1 = None\n",
      "        view_19: f16[8192, 320] = torch.ops.aten.view.default(getitem_9, [8192, 320]);  getitem_9 = None\n",
      "        addmm_2: f16[8192, 2560] = torch.ops.aten.addmm.default(arg21_1, view_19, t_8);  arg21_1 = view_19 = t_8 = None\n",
      "        view_20: f16[2, 4096, 2560] = torch.ops.aten.view.default(addmm_2, [2, 4096, 2560]);  addmm_2 = None\n",
      "        split = torch.ops.aten.split.Tensor(view_20, 1280, -1);  view_20 = None\n",
      "        getitem_12: f16[2, 4096, 1280] = split[0]\n",
      "        getitem_13: f16[2, 4096, 1280] = split[1];  split = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:815, code: return F.gelu(gate)\n",
      "        gelu: f16[2, 4096, 1280] = torch.ops.aten.gelu.default(getitem_13);  getitem_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:821, code: return hidden_states * self.gelu(gate)\n",
      "        mul: f16[2, 4096, 1280] = torch.ops.aten.mul.Tensor(getitem_12, gelu);  getitem_12 = gelu = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:774, code: hidden_states = module(hidden_states)\n",
      "        t_9: f16[1280, 320] = torch.ops.aten.t.default(arg22_1);  arg22_1 = None\n",
      "        view_21: f16[8192, 1280] = torch.ops.aten.view.default(mul, [8192, 1280]);  mul = None\n",
      "        addmm_3: f16[8192, 320] = torch.ops.aten.addmm.default(arg23_1, view_21, t_9);  arg23_1 = view_21 = t_9 = None\n",
      "        view_22: f16[2, 4096, 320] = torch.ops.aten.view.default(addmm_3, [2, 4096, 320]);  addmm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:505, code: hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
      "        add_2: f16[2, 4096, 320] = torch.ops.aten.add.Tensor(view_22, add_1);  view_22 = add_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:222, code: hidden_states.reshape(batch, height, weight, inner_dim).permute(0, 3, 1, 2).contiguous()\n",
      "        view_23: f16[2, 64, 64, 320] = torch.ops.aten.view.default(add_2, [2, 64, 64, 320]);  add_2 = None\n",
      "        permute_9: f16[2, 320, 64, 64] = torch.ops.aten.permute.default(view_23, [0, 3, 1, 2]);  view_23 = None\n",
      "        clone_8: f16[2, 320, 64, 64] = torch.ops.aten.clone.default(permute_9, memory_format = torch.contiguous_format);  permute_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:224, code: hidden_states = self.proj_out(hidden_states)\n",
      "        convolution_1: f16[2, 320, 64, 64] = torch.ops.aten.convolution.default(clone_8, arg24_1, arg25_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  clone_8 = arg24_1 = arg25_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:231, code: output = hidden_states + residual\n",
      "        add_3: f16[2, 320, 64, 64] = torch.ops.aten.add.Tensor(convolution_1, arg26_1);  convolution_1 = arg26_1 = None\n",
      "        return (add_3,)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: i64[]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:314, code: prev_timestep = timestep - self.config.num_train_timesteps // self.num_inference_steps\n",
      "        sub: i64[] = torch.ops.aten.sub.Tensor(arg0_1, 100);  arg0_1 = None\n",
      "        return (sub,)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: i64[], arg1_1: f32[1000]):\n",
      "        return (arg0_1, arg1_1)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f32[], arg1_1: i64[]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:372, code: alpha_prod_t_prev = self.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.final_alpha_cumprod\n",
      "        ge: b8[] = torch.ops.aten.ge.Scalar(arg1_1, 0);  arg1_1 = None\n",
      "        return (arg0_1, ge)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: i64[], arg1_1: f32[1000]):\n",
      "        return (arg0_1, arg1_1)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f32[], arg1_1: f16[1, 4, 64, 64], arg2_1: f16[1, 4, 64, 64], arg3_1: f32[]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:373, code: beta_prod_t = 1 - alpha_prod_t\n",
      "        rsub: f32[] = torch.ops.aten.rsub.Scalar(arg3_1, 1)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:374, code: beta_prod_t_prev = 1 - alpha_prod_t_prev\n",
      "        rsub_1: f32[] = torch.ops.aten.rsub.Scalar(arg0_1, 1)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:387, code: sample_coeff = (alpha_prod_t_prev / alpha_prod_t) ** (0.5)\n",
      "        div: f32[] = torch.ops.aten.div.Tensor(arg0_1, arg3_1)\n",
      "        pow_1: f32[] = torch.ops.aten.pow.Tensor_Scalar(div, 0.5);  div = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:390, code: model_output_denom_coeff = alpha_prod_t * beta_prod_t_prev ** (0.5) + (\n",
      "        pow_2: f32[] = torch.ops.aten.pow.Tensor_Scalar(rsub_1, 0.5);  rsub_1 = None\n",
      "        mul: f32[] = torch.ops.aten.mul.Tensor(arg3_1, pow_2);  pow_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:391, code: alpha_prod_t * beta_prod_t * alpha_prod_t_prev\n",
      "        mul_1: f32[] = torch.ops.aten.mul.Tensor(arg3_1, rsub);  rsub = None\n",
      "        mul_2: f32[] = torch.ops.aten.mul.Tensor(mul_1, arg0_1);  mul_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:390, code: model_output_denom_coeff = alpha_prod_t * beta_prod_t_prev ** (0.5) + (\n",
      "        pow_3: f32[] = torch.ops.aten.pow.Tensor_Scalar(mul_2, 0.5);  mul_2 = None\n",
      "        add: f32[] = torch.ops.aten.add.Tensor(mul, pow_3);  mul = pow_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:396, code: sample_coeff * sample - (alpha_prod_t_prev - alpha_prod_t) * model_output / model_output_denom_coeff\n",
      "        mul_3: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(pow_1, arg1_1);  pow_1 = arg1_1 = None\n",
      "        sub: f32[] = torch.ops.aten.sub.Tensor(arg0_1, arg3_1);  arg0_1 = arg3_1 = None\n",
      "        mul_4: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(sub, arg2_1);  sub = arg2_1 = None\n",
      "        div_1: f16[1, 4, 64, 64] = torch.ops.aten.div.Tensor(mul_4, add);  mul_4 = add = None\n",
      "        sub_1: f16[1, 4, 64, 64] = torch.ops.aten.sub.Tensor(mul_3, div_1);  mul_3 = div_1 = None\n",
      "        return (sub_1,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      " 10%|█         | 1/10 [00:15<02:19, 15.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1, 4, 64, 64], arg1_1: i64[], arg2_1: f16[1, 4, 64, 64]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:314, code: prev_timestep = timestep - self.config.num_train_timesteps // self.num_inference_steps\n",
      "        sub: i64[] = torch.ops.aten.sub.Tensor(arg1_1, 100)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:321, code: timestep = timestep + self.config.num_train_timesteps // self.num_inference_steps\n",
      "        add: i64[] = torch.ops.aten.add.Tensor(arg1_1, 100);  arg1_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:327, code: model_output = (model_output + self.ets[-1]) / 2\n",
      "        add_1: f16[1, 4, 64, 64] = torch.ops.aten.add.Tensor(arg0_1, arg2_1);  arg0_1 = arg2_1 = None\n",
      "        div: f16[1, 4, 64, 64] = torch.ops.aten.div.Tensor(add_1, 2);  add_1 = None\n",
      "        return (add, div)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      " 20%|██        | 2/10 [00:15<00:51,  6.47s/it]/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1, 4, 64, 64], arg1_1: i64[], arg2_1: f16[1, 4, 64, 64]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:314, code: prev_timestep = timestep - self.config.num_train_timesteps // self.num_inference_steps\n",
      "        sub: i64[] = torch.ops.aten.sub.Tensor(arg1_1, 100);  arg1_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:331, code: model_output = (3 * self.ets[-1] - self.ets[-2]) / 2\n",
      "        mul: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg0_1, 3);  arg0_1 = None\n",
      "        sub_1: f16[1, 4, 64, 64] = torch.ops.aten.sub.Tensor(mul, arg2_1);  mul = arg2_1 = None\n",
      "        div: f16[1, 4, 64, 64] = torch.ops.aten.div.Tensor(sub_1, 2);  sub_1 = None\n",
      "        return (sub, div)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1, 4, 64, 64], arg1_1: i64[], arg2_1: f16[1, 4, 64, 64], arg3_1: f16[1, 4, 64, 64]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:314, code: prev_timestep = timestep - self.config.num_train_timesteps // self.num_inference_steps\n",
      "        sub: i64[] = torch.ops.aten.sub.Tensor(arg1_1, 100);  arg1_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:333, code: model_output = (23 * self.ets[-1] - 16 * self.ets[-2] + 5 * self.ets[-3]) / 12\n",
      "        mul: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg0_1, 23);  arg0_1 = None\n",
      "        mul_1: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg3_1, 16);  arg3_1 = None\n",
      "        sub_1: f16[1, 4, 64, 64] = torch.ops.aten.sub.Tensor(mul, mul_1);  mul = mul_1 = None\n",
      "        mul_2: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg2_1, 5);  arg2_1 = None\n",
      "        add: f16[1, 4, 64, 64] = torch.ops.aten.add.Tensor(sub_1, mul_2);  sub_1 = mul_2 = None\n",
      "        div: f16[1, 4, 64, 64] = torch.ops.aten.div.Tensor(add, 12);  add = None\n",
      "        return (sub, div)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      " 30%|███       | 3/10 [00:15<00:25,  3.60s/it]/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      " 40%|████      | 4/10 [00:15<00:13,  2.25s/it]/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1, 4, 64, 64], arg1_1: i64[], arg2_1: f16[1, 4, 64, 64], arg3_1: f16[1, 4, 64, 64], arg4_1: f16[1, 4, 64, 64]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:314, code: prev_timestep = timestep - self.config.num_train_timesteps // self.num_inference_steps\n",
      "        sub: i64[] = torch.ops.aten.sub.Tensor(arg1_1, 100);  arg1_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:335, code: model_output = (1 / 24) * (55 * self.ets[-1] - 59 * self.ets[-2] + 37 * self.ets[-3] - 9 * self.ets[-4])\n",
      "        mul: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg0_1, 55);  arg0_1 = None\n",
      "        mul_1: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg4_1, 59);  arg4_1 = None\n",
      "        sub_1: f16[1, 4, 64, 64] = torch.ops.aten.sub.Tensor(mul, mul_1);  mul = mul_1 = None\n",
      "        mul_2: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg3_1, 37);  arg3_1 = None\n",
      "        add: f16[1, 4, 64, 64] = torch.ops.aten.add.Tensor(sub_1, mul_2);  sub_1 = mul_2 = None\n",
      "        mul_3: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg2_1, 9);  arg2_1 = None\n",
      "        sub_2: f16[1, 4, 64, 64] = torch.ops.aten.sub.Tensor(add, mul_3);  add = mul_3 = None\n",
      "        mul_4: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(sub_2, 0.041666666666666664);  sub_2 = None\n",
      "        return (sub, mul_4)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1, 4, 64, 64], arg1_1: i64[], arg2_1: f16[1, 4, 64, 64], arg3_1: f16[1, 4, 64, 64], arg4_1: f16[1, 4, 64, 64]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:314, code: prev_timestep = timestep - self.config.num_train_timesteps // self.num_inference_steps\n",
      "        sub: i64[] = torch.ops.aten.sub.Tensor(arg1_1, 100);  arg1_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:335, code: model_output = (1 / 24) * (55 * self.ets[-1] - 59 * self.ets[-2] + 37 * self.ets[-3] - 9 * self.ets[-4])\n",
      "        mul: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg0_1, 55);  arg0_1 = None\n",
      "        mul_1: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg4_1, 59);  arg4_1 = None\n",
      "        sub_1: f16[1, 4, 64, 64] = torch.ops.aten.sub.Tensor(mul, mul_1);  mul = mul_1 = None\n",
      "        mul_2: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg3_1, 37);  arg3_1 = None\n",
      "        add: f16[1, 4, 64, 64] = torch.ops.aten.add.Tensor(sub_1, mul_2);  sub_1 = mul_2 = None\n",
      "        mul_3: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg2_1, 9);  arg2_1 = None\n",
      "        sub_2: f16[1, 4, 64, 64] = torch.ops.aten.sub.Tensor(add, mul_3);  add = mul_3 = None\n",
      "        mul_4: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(sub_2, 0.041666666666666664);  sub_2 = None\n",
      "        return (sub, mul_4)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      " 50%|█████     | 5/10 [00:16<00:07,  1.50s/it]/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      " 60%|██████    | 6/10 [00:16<00:04,  1.05s/it]/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1, 4, 64, 64], arg1_1: i64[], arg2_1: f16[1, 4, 64, 64], arg3_1: f16[1, 4, 64, 64], arg4_1: f16[1, 4, 64, 64]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:314, code: prev_timestep = timestep - self.config.num_train_timesteps // self.num_inference_steps\n",
      "        sub: i64[] = torch.ops.aten.sub.Tensor(arg1_1, 100);  arg1_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:335, code: model_output = (1 / 24) * (55 * self.ets[-1] - 59 * self.ets[-2] + 37 * self.ets[-3] - 9 * self.ets[-4])\n",
      "        mul: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg0_1, 55);  arg0_1 = None\n",
      "        mul_1: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg4_1, 59);  arg4_1 = None\n",
      "        sub_1: f16[1, 4, 64, 64] = torch.ops.aten.sub.Tensor(mul, mul_1);  mul = mul_1 = None\n",
      "        mul_2: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg3_1, 37);  arg3_1 = None\n",
      "        add: f16[1, 4, 64, 64] = torch.ops.aten.add.Tensor(sub_1, mul_2);  sub_1 = mul_2 = None\n",
      "        mul_3: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg2_1, 9);  arg2_1 = None\n",
      "        sub_2: f16[1, 4, 64, 64] = torch.ops.aten.sub.Tensor(add, mul_3);  add = mul_3 = None\n",
      "        mul_4: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(sub_2, 0.041666666666666664);  sub_2 = None\n",
      "        return (sub, mul_4)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1, 4, 64, 64], arg1_1: i64[], arg2_1: f16[1, 4, 64, 64], arg3_1: f16[1, 4, 64, 64], arg4_1: f16[1, 4, 64, 64]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:314, code: prev_timestep = timestep - self.config.num_train_timesteps // self.num_inference_steps\n",
      "        sub: i64[] = torch.ops.aten.sub.Tensor(arg1_1, 100);  arg1_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:335, code: model_output = (1 / 24) * (55 * self.ets[-1] - 59 * self.ets[-2] + 37 * self.ets[-3] - 9 * self.ets[-4])\n",
      "        mul: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg0_1, 55);  arg0_1 = None\n",
      "        mul_1: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg4_1, 59);  arg4_1 = None\n",
      "        sub_1: f16[1, 4, 64, 64] = torch.ops.aten.sub.Tensor(mul, mul_1);  mul = mul_1 = None\n",
      "        mul_2: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg3_1, 37);  arg3_1 = None\n",
      "        add: f16[1, 4, 64, 64] = torch.ops.aten.add.Tensor(sub_1, mul_2);  sub_1 = mul_2 = None\n",
      "        mul_3: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg2_1, 9);  arg2_1 = None\n",
      "        sub_2: f16[1, 4, 64, 64] = torch.ops.aten.sub.Tensor(add, mul_3);  add = mul_3 = None\n",
      "        mul_4: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(sub_2, 0.041666666666666664);  sub_2 = None\n",
      "        return (sub, mul_4)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      " 70%|███████   | 7/10 [00:16<00:02,  1.30it/s]/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      " 80%|████████  | 8/10 [00:16<00:01,  1.72it/s]/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1, 4, 64, 64], arg1_1: i64[], arg2_1: f16[1, 4, 64, 64], arg3_1: f16[1, 4, 64, 64], arg4_1: f16[1, 4, 64, 64]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:314, code: prev_timestep = timestep - self.config.num_train_timesteps // self.num_inference_steps\n",
      "        sub: i64[] = torch.ops.aten.sub.Tensor(arg1_1, 100);  arg1_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:335, code: model_output = (1 / 24) * (55 * self.ets[-1] - 59 * self.ets[-2] + 37 * self.ets[-3] - 9 * self.ets[-4])\n",
      "        mul: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg0_1, 55);  arg0_1 = None\n",
      "        mul_1: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg4_1, 59);  arg4_1 = None\n",
      "        sub_1: f16[1, 4, 64, 64] = torch.ops.aten.sub.Tensor(mul, mul_1);  mul = mul_1 = None\n",
      "        mul_2: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg3_1, 37);  arg3_1 = None\n",
      "        add: f16[1, 4, 64, 64] = torch.ops.aten.add.Tensor(sub_1, mul_2);  sub_1 = mul_2 = None\n",
      "        mul_3: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg2_1, 9);  arg2_1 = None\n",
      "        sub_2: f16[1, 4, 64, 64] = torch.ops.aten.sub.Tensor(add, mul_3);  add = mul_3 = None\n",
      "        mul_4: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(sub_2, 0.041666666666666664);  sub_2 = None\n",
      "        return (sub, mul_4)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1, 4, 64, 64], arg1_1: i64[], arg2_1: f16[1, 4, 64, 64], arg3_1: f16[1, 4, 64, 64], arg4_1: f16[1, 4, 64, 64]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:314, code: prev_timestep = timestep - self.config.num_train_timesteps // self.num_inference_steps\n",
      "        sub: i64[] = torch.ops.aten.sub.Tensor(arg1_1, 100);  arg1_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:335, code: model_output = (1 / 24) * (55 * self.ets[-1] - 59 * self.ets[-2] + 37 * self.ets[-3] - 9 * self.ets[-4])\n",
      "        mul: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg0_1, 55);  arg0_1 = None\n",
      "        mul_1: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg4_1, 59);  arg4_1 = None\n",
      "        sub_1: f16[1, 4, 64, 64] = torch.ops.aten.sub.Tensor(mul, mul_1);  mul = mul_1 = None\n",
      "        mul_2: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg3_1, 37);  arg3_1 = None\n",
      "        add: f16[1, 4, 64, 64] = torch.ops.aten.add.Tensor(sub_1, mul_2);  sub_1 = mul_2 = None\n",
      "        mul_3: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg2_1, 9);  arg2_1 = None\n",
      "        sub_2: f16[1, 4, 64, 64] = torch.ops.aten.sub.Tensor(add, mul_3);  add = mul_3 = None\n",
      "        mul_4: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(sub_2, 0.041666666666666664);  sub_2 = None\n",
      "        return (sub, mul_4)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      " 90%|█████████ | 9/10 [00:16<00:00,  2.20it/s]/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1, 4, 64, 64], arg1_1: i64[], arg2_1: f16[1, 4, 64, 64], arg3_1: f16[1, 4, 64, 64], arg4_1: f16[1, 4, 64, 64]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:314, code: prev_timestep = timestep - self.config.num_train_timesteps // self.num_inference_steps\n",
      "        sub: i64[] = torch.ops.aten.sub.Tensor(arg1_1, 100);  arg1_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:335, code: model_output = (1 / 24) * (55 * self.ets[-1] - 59 * self.ets[-2] + 37 * self.ets[-3] - 9 * self.ets[-4])\n",
      "        mul: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg0_1, 55);  arg0_1 = None\n",
      "        mul_1: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg4_1, 59);  arg4_1 = None\n",
      "        sub_1: f16[1, 4, 64, 64] = torch.ops.aten.sub.Tensor(mul, mul_1);  mul = mul_1 = None\n",
      "        mul_2: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg3_1, 37);  arg3_1 = None\n",
      "        add: f16[1, 4, 64, 64] = torch.ops.aten.add.Tensor(sub_1, mul_2);  sub_1 = mul_2 = None\n",
      "        mul_3: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg2_1, 9);  arg2_1 = None\n",
      "        sub_2: f16[1, 4, 64, 64] = torch.ops.aten.sub.Tensor(add, mul_3);  add = mul_3 = None\n",
      "        mul_4: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(sub_2, 0.041666666666666664);  sub_2 = None\n",
      "        return (sub, mul_4)\n",
      "        \n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1, 4, 64, 64], arg1_1: f16[1, 4, 64, 64], arg2_1: f32[], arg3_1: f32[]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:373, code: beta_prod_t = 1 - alpha_prod_t\n",
      "        rsub: f32[] = torch.ops.aten.rsub.Scalar(arg2_1, 1)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:374, code: beta_prod_t_prev = 1 - alpha_prod_t_prev\n",
      "        rsub_1: f32[] = torch.ops.aten.rsub.Scalar(arg3_1, 1)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:387, code: sample_coeff = (alpha_prod_t_prev / alpha_prod_t) ** (0.5)\n",
      "        div: f32[] = torch.ops.aten.div.Tensor(arg3_1, arg2_1)\n",
      "        pow_1: f32[] = torch.ops.aten.pow.Tensor_Scalar(div, 0.5);  div = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:390, code: model_output_denom_coeff = alpha_prod_t * beta_prod_t_prev ** (0.5) + (\n",
      "        pow_2: f32[] = torch.ops.aten.pow.Tensor_Scalar(rsub_1, 0.5);  rsub_1 = None\n",
      "        mul: f32[] = torch.ops.aten.mul.Tensor(arg2_1, pow_2);  pow_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:391, code: alpha_prod_t * beta_prod_t * alpha_prod_t_prev\n",
      "        mul_1: f32[] = torch.ops.aten.mul.Tensor(arg2_1, rsub);  rsub = None\n",
      "        mul_2: f32[] = torch.ops.aten.mul.Tensor(mul_1, arg3_1);  mul_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:390, code: model_output_denom_coeff = alpha_prod_t * beta_prod_t_prev ** (0.5) + (\n",
      "        pow_3: f32[] = torch.ops.aten.pow.Tensor_Scalar(mul_2, 0.5);  mul_2 = None\n",
      "        add: f32[] = torch.ops.aten.add.Tensor(mul, pow_3);  mul = pow_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:396, code: sample_coeff * sample - (alpha_prod_t_prev - alpha_prod_t) * model_output / model_output_denom_coeff\n",
      "        mul_3: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(pow_1, arg0_1);  pow_1 = arg0_1 = None\n",
      "        sub: f32[] = torch.ops.aten.sub.Tensor(arg3_1, arg2_1);  arg3_1 = arg2_1 = None\n",
      "        mul_4: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(sub, arg1_1);  sub = arg1_1 = None\n",
      "        div_1: f16[1, 4, 64, 64] = torch.ops.aten.div.Tensor(mul_4, add);  mul_4 = add = None\n",
      "        sub_1: f16[1, 4, 64, 64] = torch.ops.aten.sub.Tensor(mul_3, div_1);  mul_3 = div_1 = None\n",
      "        return (sub_1,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[1, 4, 64, 64]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:340, code: latents = 1 / 0.18215 * latents\n",
      "        mul: f16[1, 4, 64, 64] = torch.ops.aten.mul.Tensor(arg0_1, 5.489980785067252);  arg0_1 = None\n",
      "        return (mul,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f16[4, 4, 1, 1], arg1_1: f16[4], arg2_1: f16[512, 4, 3, 3], arg3_1: f16[512], arg4_1: f16[512], arg5_1: f16[512], arg6_1: f16[512, 512, 3, 3], arg7_1: f16[512], arg8_1: f16[512], arg9_1: f16[512], arg10_1: f16[512, 512, 3, 3], arg11_1: f16[512], arg12_1: f16[512], arg13_1: f16[512], arg14_1: f16[512, 512], arg15_1: f16[512], arg16_1: f16[512, 512], arg17_1: f16[512], arg18_1: f16[512, 512], arg19_1: f16[512], arg20_1: f16[512, 512], arg21_1: f16[512], arg22_1: f16[512], arg23_1: f16[512], arg24_1: f16[512, 512, 3, 3], arg25_1: f16[512], arg26_1: f16[512], arg27_1: f16[512], arg28_1: f16[512, 512, 3, 3], arg29_1: f16[512], arg30_1: f16[512], arg31_1: f16[512], arg32_1: f16[512, 512, 3, 3], arg33_1: f16[512], arg34_1: f16[512], arg35_1: f16[512], arg36_1: f16[512, 512, 3, 3], arg37_1: f16[512], arg38_1: f16[512], arg39_1: f16[512], arg40_1: f16[512, 512, 3, 3], arg41_1: f16[512], arg42_1: f16[512], arg43_1: f16[512], arg44_1: f16[512, 512, 3, 3], arg45_1: f16[512], arg46_1: f16[512], arg47_1: f16[512], arg48_1: f16[512, 512, 3, 3], arg49_1: f16[512], arg50_1: f16[512], arg51_1: f16[512], arg52_1: f16[512, 512, 3, 3], arg53_1: f16[512], arg54_1: f16[512, 512, 3, 3], arg55_1: f16[512], arg56_1: f16[512], arg57_1: f16[512], arg58_1: f16[512, 512, 3, 3], arg59_1: f16[512], arg60_1: f16[512], arg61_1: f16[512], arg62_1: f16[512, 512, 3, 3], arg63_1: f16[512], arg64_1: f16[512], arg65_1: f16[512], arg66_1: f16[512, 512, 3, 3], arg67_1: f16[512], arg68_1: f16[512], arg69_1: f16[512], arg70_1: f16[512, 512, 3, 3], arg71_1: f16[512], arg72_1: f16[512], arg73_1: f16[512], arg74_1: f16[512, 512, 3, 3], arg75_1: f16[512], arg76_1: f16[512], arg77_1: f16[512], arg78_1: f16[512, 512, 3, 3], arg79_1: f16[512], arg80_1: f16[512, 512, 3, 3], arg81_1: f16[512], arg82_1: f16[512], arg83_1: f16[512], arg84_1: f16[256, 512, 3, 3], arg85_1: f16[256], arg86_1: f16[256], arg87_1: f16[256], arg88_1: f16[256, 256, 3, 3], arg89_1: f16[256], arg90_1: f16[256, 512, 1, 1], arg91_1: f16[256], arg92_1: f16[256], arg93_1: f16[256], arg94_1: f16[256, 256, 3, 3], arg95_1: f16[256], arg96_1: f16[256], arg97_1: f16[256], arg98_1: f16[256, 256, 3, 3], arg99_1: f16[256], arg100_1: f16[256], arg101_1: f16[256], arg102_1: f16[256, 256, 3, 3], arg103_1: f16[256], arg104_1: f16[256], arg105_1: f16[256], arg106_1: f16[256, 256, 3, 3], arg107_1: f16[256], arg108_1: f16[256, 256, 3, 3], arg109_1: f16[256], arg110_1: f16[256], arg111_1: f16[256], arg112_1: f16[128, 256, 3, 3], arg113_1: f16[128], arg114_1: f16[128], arg115_1: f16[128], arg116_1: f16[128, 128, 3, 3], arg117_1: f16[128], arg118_1: f16[128, 256, 1, 1], arg119_1: f16[128], arg120_1: f16[128], arg121_1: f16[128], arg122_1: f16[128, 128, 3, 3], arg123_1: f16[128], arg124_1: f16[128], arg125_1: f16[128], arg126_1: f16[128, 128, 3, 3], arg127_1: f16[128], arg128_1: f16[128], arg129_1: f16[128], arg130_1: f16[128, 128, 3, 3], arg131_1: f16[128], arg132_1: f16[128], arg133_1: f16[128], arg134_1: f16[128, 128, 3, 3], arg135_1: f16[128], arg136_1: f16[128], arg137_1: f16[128], arg138_1: f16[3, 128, 3, 3], arg139_1: f16[3], arg140_1: f16[1, 4, 64, 64]):\n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/vae.py:576, code: z = self.post_quant_conv(z)\n",
      "        convolution: f16[1, 4, 64, 64] = torch.ops.aten.convolution.default(arg140_1, arg0_1, arg1_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  arg140_1 = arg0_1 = arg1_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/vae.py:210, code: sample = self.conv_in(sample)\n",
      "        convolution_1: f16[1, 512, 64, 64] = torch.ops.aten.convolution.default(convolution, arg2_1, arg3_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  convolution = arg2_1 = arg3_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm = torch.ops.aten.native_group_norm.default(convolution_1, arg4_1, arg5_1, 1, 512, 4096, 32, 1e-06);  arg4_1 = arg5_1 = None\n",
      "        getitem: f16[1, 512, 64, 64] = native_group_norm[0]\n",
      "        getitem_1: f16[1, 32] = native_group_norm[1]\n",
      "        getitem_2: f16[1, 32] = native_group_norm[2];  native_group_norm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu: f16[1, 512, 64, 64] = torch.ops.aten.silu.default(getitem);  getitem = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution_2: f16[1, 512, 64, 64] = torch.ops.aten.convolution.default(silu, arg6_1, arg7_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu = arg6_1 = arg7_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_1 = torch.ops.aten.native_group_norm.default(convolution_2, arg8_1, arg9_1, 1, 512, 4096, 32, 1e-06);  convolution_2 = arg8_1 = arg9_1 = None\n",
      "        getitem_3: f16[1, 512, 64, 64] = native_group_norm_1[0]\n",
      "        getitem_4: f16[1, 32] = native_group_norm_1[1]\n",
      "        getitem_5: f16[1, 32] = native_group_norm_1[2];  native_group_norm_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_1: f16[1, 512, 64, 64] = torch.ops.aten.silu.default(getitem_3);  getitem_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_3: f16[1, 512, 64, 64] = torch.ops.aten.convolution.default(silu_1, arg10_1, arg11_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_1 = arg10_1 = arg11_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add: f16[1, 512, 64, 64] = torch.ops.aten.add.Tensor(convolution_1, convolution_3);  convolution_1 = convolution_3 = None\n",
      "        div: f16[1, 512, 64, 64] = torch.ops.aten.div.Tensor(add, 1);  add = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:333, code: hidden_states = self.group_norm(hidden_states)\n",
      "        native_group_norm_2 = torch.ops.aten.native_group_norm.default(div, arg12_1, arg13_1, 1, 512, 4096, 32, 1e-06);  arg12_1 = arg13_1 = None\n",
      "        getitem_6: f16[1, 512, 64, 64] = native_group_norm_2[0]\n",
      "        getitem_7: f16[1, 32] = native_group_norm_2[1]\n",
      "        getitem_8: f16[1, 32] = native_group_norm_2[2];  native_group_norm_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:335, code: hidden_states = hidden_states.view(batch, channel, height * width).transpose(1, 2)\n",
      "        view: f16[1, 512, 4096] = torch.ops.aten.view.default(getitem_6, [1, 512, 4096]);  getitem_6 = None\n",
      "        transpose: f16[1, 4096, 512] = torch.ops.aten.transpose.int(view, 1, 2);  view = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:338, code: query_proj = self.query(hidden_states)\n",
      "        t: f16[512, 512] = torch.ops.aten.t.default(arg14_1);  arg14_1 = None\n",
      "        expand: f16[1, 4096, 512] = torch.ops.aten.expand.default(transpose, [1, 4096, 512])\n",
      "        view_1: f16[1, 4096, 512] = torch.ops.aten.view.default(expand, [1, 4096, 512]);  expand = None\n",
      "        expand_1: f16[1, 512, 512] = torch.ops.aten.expand.default(t, [1, 512, 512]);  t = None\n",
      "        view_2: f16[1, 512, 512] = torch.ops.aten.view.default(expand_1, [1, 512, 512]);  expand_1 = None\n",
      "        bmm: f16[1, 4096, 512] = torch.ops.aten.bmm.default(view_1, view_2);  view_1 = view_2 = None\n",
      "        _unsafe_view: f16[1, 4096, 512] = torch.ops.aten._unsafe_view.default(bmm, [1, 4096, 512]);  bmm = None\n",
      "        add_1: f16[1, 4096, 512] = torch.ops.aten.add.Tensor(_unsafe_view, arg15_1);  _unsafe_view = arg15_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:339, code: key_proj = self.key(hidden_states)\n",
      "        t_1: f16[512, 512] = torch.ops.aten.t.default(arg16_1);  arg16_1 = None\n",
      "        expand_2: f16[1, 4096, 512] = torch.ops.aten.expand.default(transpose, [1, 4096, 512])\n",
      "        view_3: f16[1, 4096, 512] = torch.ops.aten.view.default(expand_2, [1, 4096, 512]);  expand_2 = None\n",
      "        expand_3: f16[1, 512, 512] = torch.ops.aten.expand.default(t_1, [1, 512, 512]);  t_1 = None\n",
      "        view_4: f16[1, 512, 512] = torch.ops.aten.view.default(expand_3, [1, 512, 512]);  expand_3 = None\n",
      "        bmm_1: f16[1, 4096, 512] = torch.ops.aten.bmm.default(view_3, view_4);  view_3 = view_4 = None\n",
      "        _unsafe_view_1: f16[1, 4096, 512] = torch.ops.aten._unsafe_view.default(bmm_1, [1, 4096, 512]);  bmm_1 = None\n",
      "        add_2: f16[1, 4096, 512] = torch.ops.aten.add.Tensor(_unsafe_view_1, arg17_1);  _unsafe_view_1 = arg17_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:340, code: value_proj = self.value(hidden_states)\n",
      "        t_2: f16[512, 512] = torch.ops.aten.t.default(arg18_1);  arg18_1 = None\n",
      "        expand_4: f16[1, 4096, 512] = torch.ops.aten.expand.default(transpose, [1, 4096, 512]);  transpose = None\n",
      "        view_5: f16[1, 4096, 512] = torch.ops.aten.view.default(expand_4, [1, 4096, 512]);  expand_4 = None\n",
      "        expand_5: f16[1, 512, 512] = torch.ops.aten.expand.default(t_2, [1, 512, 512]);  t_2 = None\n",
      "        view_6: f16[1, 512, 512] = torch.ops.aten.view.default(expand_5, [1, 512, 512]);  expand_5 = None\n",
      "        bmm_2: f16[1, 4096, 512] = torch.ops.aten.bmm.default(view_5, view_6);  view_5 = view_6 = None\n",
      "        _unsafe_view_2: f16[1, 4096, 512] = torch.ops.aten._unsafe_view.default(bmm_2, [1, 4096, 512]);  bmm_2 = None\n",
      "        add_3: f16[1, 4096, 512] = torch.ops.aten.add.Tensor(_unsafe_view_2, arg19_1);  _unsafe_view_2 = arg19_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:317, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_7: f16[1, 4096, 1, 512] = torch.ops.aten.view.default(add_1, [1, 4096, 1, 512]);  add_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:318, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute: f16[1, 1, 4096, 512] = torch.ops.aten.permute.default(view_7, [0, 2, 1, 3]);  view_7 = None\n",
      "        view_8: f16[1, 4096, 512] = torch.ops.aten.view.default(permute, [1, 4096, 512]);  permute = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:317, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_9: f16[1, 4096, 1, 512] = torch.ops.aten.view.default(add_2, [1, 4096, 1, 512]);  add_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:318, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_1: f16[1, 1, 4096, 512] = torch.ops.aten.permute.default(view_9, [0, 2, 1, 3]);  view_9 = None\n",
      "        view_10: f16[1, 4096, 512] = torch.ops.aten.view.default(permute_1, [1, 4096, 512]);  permute_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:317, code: tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
      "        view_11: f16[1, 4096, 1, 512] = torch.ops.aten.view.default(add_3, [1, 4096, 1, 512]);  add_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:318, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
      "        permute_2: f16[1, 1, 4096, 512] = torch.ops.aten.permute.default(view_11, [0, 2, 1, 3]);  view_11 = None\n",
      "        view_12: f16[1, 4096, 512] = torch.ops.aten.view.default(permute_2, [1, 4096, 512]);  permute_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:354, code: torch.empty(\n",
      "        empty: f16[1, 4096, 4096] = torch.ops.aten.empty.memory_format([1, 4096, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:362, code: key_proj.transpose(-1, -2),\n",
      "        transpose_1: f16[1, 512, 4096] = torch.ops.aten.transpose.int(view_10, -1, -2);  view_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:353, code: attention_scores = torch.baddbmm(\n",
      "        baddbmm: f16[1, 4096, 4096] = torch.ops.aten.baddbmm.default(empty, view_8, transpose_1, beta = 0, alpha = 0.044194173824159216);  empty = view_8 = transpose_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:366, code: attention_probs = torch.softmax(attention_scores.float(), dim=-1).type(attention_scores.dtype)\n",
      "        _to_copy: f32[1, 4096, 4096] = torch.ops.aten._to_copy.default(baddbmm, dtype = torch.float32);  baddbmm = None\n",
      "        _softmax: f32[1, 4096, 4096] = torch.ops.aten._softmax.default(_to_copy, -1, False);  _to_copy = None\n",
      "        _to_copy_1: f16[1, 4096, 4096] = torch.ops.aten._to_copy.default(_softmax, dtype = torch.float16, device = device(type='cuda', index=0));  _softmax = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:367, code: hidden_states = torch.bmm(attention_probs, value_proj)\n",
      "        bmm_3: f16[1, 4096, 512] = torch.ops.aten.bmm.default(_to_copy_1, view_12);  _to_copy_1 = view_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:324, code: tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
      "        view_13: f16[1, 1, 4096, 512] = torch.ops.aten.view.default(bmm_3, [1, 1, 4096, 512]);  bmm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:325, code: tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
      "        permute_3: f16[1, 4096, 1, 512] = torch.ops.aten.permute.default(view_13, [0, 2, 1, 3]);  view_13 = None\n",
      "        view_14: f16[1, 4096, 512] = torch.ops.aten.view.default(permute_3, [1, 4096, 512]);  permute_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:373, code: hidden_states = self.proj_attn(hidden_states)\n",
      "        t_3: f16[512, 512] = torch.ops.aten.t.default(arg20_1);  arg20_1 = None\n",
      "        view_15: f16[4096, 512] = torch.ops.aten.view.default(view_14, [4096, 512]);  view_14 = None\n",
      "        addmm: f16[4096, 512] = torch.ops.aten.addmm.default(arg21_1, view_15, t_3);  arg21_1 = view_15 = t_3 = None\n",
      "        view_16: f16[1, 4096, 512] = torch.ops.aten.view.default(addmm, [1, 4096, 512]);  addmm = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:375, code: hidden_states = hidden_states.transpose(-1, -2).reshape(batch, channel, height, width)\n",
      "        transpose_2: f16[1, 512, 4096] = torch.ops.aten.transpose.int(view_16, -1, -2);  view_16 = None\n",
      "        view_17: f16[1, 512, 64, 64] = torch.ops.aten.view.default(transpose_2, [1, 512, 64, 64]);  transpose_2 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/attention.py:378, code: hidden_states = (hidden_states + residual) / self.rescale_output_factor\n",
      "        add_4: f16[1, 512, 64, 64] = torch.ops.aten.add.Tensor(view_17, div);  view_17 = div = None\n",
      "        div_1: f16[1, 512, 64, 64] = torch.ops.aten.div.Tensor(add_4, 1);  add_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        clone: f16[1, 512, 64, 64] = torch.ops.aten.clone.default(div_1, memory_format = torch.contiguous_format)\n",
      "        native_group_norm_3 = torch.ops.aten.native_group_norm.default(clone, arg22_1, arg23_1, 1, 512, 4096, 32, 1e-06);  clone = arg22_1 = arg23_1 = None\n",
      "        getitem_9: f16[1, 512, 64, 64] = native_group_norm_3[0]\n",
      "        getitem_10: f16[1, 32] = native_group_norm_3[1]\n",
      "        getitem_11: f16[1, 32] = native_group_norm_3[2];  native_group_norm_3 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_2: f16[1, 512, 64, 64] = torch.ops.aten.silu.default(getitem_9);  getitem_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution_4: f16[1, 512, 64, 64] = torch.ops.aten.convolution.default(silu_2, arg24_1, arg25_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_2 = arg24_1 = arg25_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_4 = torch.ops.aten.native_group_norm.default(convolution_4, arg26_1, arg27_1, 1, 512, 4096, 32, 1e-06);  convolution_4 = arg26_1 = arg27_1 = None\n",
      "        getitem_12: f16[1, 512, 64, 64] = native_group_norm_4[0]\n",
      "        getitem_13: f16[1, 32] = native_group_norm_4[1]\n",
      "        getitem_14: f16[1, 32] = native_group_norm_4[2];  native_group_norm_4 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_3: f16[1, 512, 64, 64] = torch.ops.aten.silu.default(getitem_12);  getitem_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_5: f16[1, 512, 64, 64] = torch.ops.aten.convolution.default(silu_3, arg28_1, arg29_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_3 = arg28_1 = arg29_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_5: f16[1, 512, 64, 64] = torch.ops.aten.add.Tensor(div_1, convolution_5);  div_1 = convolution_5 = None\n",
      "        div_2: f16[1, 512, 64, 64] = torch.ops.aten.div.Tensor(add_5, 1);  add_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        clone_1: f16[1, 512, 64, 64] = torch.ops.aten.clone.default(div_2, memory_format = torch.contiguous_format)\n",
      "        native_group_norm_5 = torch.ops.aten.native_group_norm.default(clone_1, arg30_1, arg31_1, 1, 512, 4096, 32, 1e-06);  clone_1 = arg30_1 = arg31_1 = None\n",
      "        getitem_15: f16[1, 512, 64, 64] = native_group_norm_5[0]\n",
      "        getitem_16: f16[1, 32] = native_group_norm_5[1]\n",
      "        getitem_17: f16[1, 32] = native_group_norm_5[2];  native_group_norm_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_4: f16[1, 512, 64, 64] = torch.ops.aten.silu.default(getitem_15);  getitem_15 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution_6: f16[1, 512, 64, 64] = torch.ops.aten.convolution.default(silu_4, arg32_1, arg33_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_4 = arg32_1 = arg33_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_6 = torch.ops.aten.native_group_norm.default(convolution_6, arg34_1, arg35_1, 1, 512, 4096, 32, 1e-06);  convolution_6 = arg34_1 = arg35_1 = None\n",
      "        getitem_18: f16[1, 512, 64, 64] = native_group_norm_6[0]\n",
      "        getitem_19: f16[1, 32] = native_group_norm_6[1]\n",
      "        getitem_20: f16[1, 32] = native_group_norm_6[2];  native_group_norm_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_5: f16[1, 512, 64, 64] = torch.ops.aten.silu.default(getitem_18);  getitem_18 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_7: f16[1, 512, 64, 64] = torch.ops.aten.convolution.default(silu_5, arg36_1, arg37_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_5 = arg36_1 = arg37_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_6: f16[1, 512, 64, 64] = torch.ops.aten.add.Tensor(div_2, convolution_7);  div_2 = convolution_7 = None\n",
      "        div_3: f16[1, 512, 64, 64] = torch.ops.aten.div.Tensor(add_6, 1.0);  add_6 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        clone_2: f16[1, 512, 64, 64] = torch.ops.aten.clone.default(div_3, memory_format = torch.contiguous_format)\n",
      "        native_group_norm_7 = torch.ops.aten.native_group_norm.default(clone_2, arg38_1, arg39_1, 1, 512, 4096, 32, 1e-06);  clone_2 = arg38_1 = arg39_1 = None\n",
      "        getitem_21: f16[1, 512, 64, 64] = native_group_norm_7[0]\n",
      "        getitem_22: f16[1, 32] = native_group_norm_7[1]\n",
      "        getitem_23: f16[1, 32] = native_group_norm_7[2];  native_group_norm_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_6: f16[1, 512, 64, 64] = torch.ops.aten.silu.default(getitem_21);  getitem_21 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution_8: f16[1, 512, 64, 64] = torch.ops.aten.convolution.default(silu_6, arg40_1, arg41_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_6 = arg40_1 = arg41_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_8 = torch.ops.aten.native_group_norm.default(convolution_8, arg42_1, arg43_1, 1, 512, 4096, 32, 1e-06);  convolution_8 = arg42_1 = arg43_1 = None\n",
      "        getitem_24: f16[1, 512, 64, 64] = native_group_norm_8[0]\n",
      "        getitem_25: f16[1, 32] = native_group_norm_8[1]\n",
      "        getitem_26: f16[1, 32] = native_group_norm_8[2];  native_group_norm_8 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_7: f16[1, 512, 64, 64] = torch.ops.aten.silu.default(getitem_24);  getitem_24 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_9: f16[1, 512, 64, 64] = torch.ops.aten.convolution.default(silu_7, arg44_1, arg45_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_7 = arg44_1 = arg45_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_7: f16[1, 512, 64, 64] = torch.ops.aten.add.Tensor(div_3, convolution_9);  div_3 = convolution_9 = None\n",
      "        div_4: f16[1, 512, 64, 64] = torch.ops.aten.div.Tensor(add_7, 1.0);  add_7 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        clone_3: f16[1, 512, 64, 64] = torch.ops.aten.clone.default(div_4, memory_format = torch.contiguous_format)\n",
      "        native_group_norm_9 = torch.ops.aten.native_group_norm.default(clone_3, arg46_1, arg47_1, 1, 512, 4096, 32, 1e-06);  clone_3 = arg46_1 = arg47_1 = None\n",
      "        getitem_27: f16[1, 512, 64, 64] = native_group_norm_9[0]\n",
      "        getitem_28: f16[1, 32] = native_group_norm_9[1]\n",
      "        getitem_29: f16[1, 32] = native_group_norm_9[2];  native_group_norm_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_8: f16[1, 512, 64, 64] = torch.ops.aten.silu.default(getitem_27);  getitem_27 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution_10: f16[1, 512, 64, 64] = torch.ops.aten.convolution.default(silu_8, arg48_1, arg49_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_8 = arg48_1 = arg49_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_10 = torch.ops.aten.native_group_norm.default(convolution_10, arg50_1, arg51_1, 1, 512, 4096, 32, 1e-06);  convolution_10 = arg50_1 = arg51_1 = None\n",
      "        getitem_30: f16[1, 512, 64, 64] = native_group_norm_10[0]\n",
      "        getitem_31: f16[1, 32] = native_group_norm_10[1]\n",
      "        getitem_32: f16[1, 32] = native_group_norm_10[2];  native_group_norm_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_9: f16[1, 512, 64, 64] = torch.ops.aten.silu.default(getitem_30);  getitem_30 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_11: f16[1, 512, 64, 64] = torch.ops.aten.convolution.default(silu_9, arg52_1, arg53_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_9 = arg52_1 = arg53_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_8: f16[1, 512, 64, 64] = torch.ops.aten.add.Tensor(div_4, convolution_11);  div_4 = convolution_11 = None\n",
      "        div_5: f16[1, 512, 64, 64] = torch.ops.aten.div.Tensor(add_8, 1.0);  add_8 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:128, code: hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n",
      "        upsample_nearest2d: f16[1, 512, 128, 128] = torch.ops.aten.upsample_nearest2d.default(div_5, [128, 128], 2.0, 2.0);  div_5 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:139, code: hidden_states = self.conv(hidden_states)\n",
      "        convolution_12: f16[1, 512, 128, 128] = torch.ops.aten.convolution.default(upsample_nearest2d, arg54_1, arg55_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  upsample_nearest2d = arg54_1 = arg55_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm_11 = torch.ops.aten.native_group_norm.default(convolution_12, arg56_1, arg57_1, 1, 512, 16384, 32, 1e-06);  arg56_1 = arg57_1 = None\n",
      "        getitem_33: f16[1, 512, 128, 128] = native_group_norm_11[0]\n",
      "        getitem_34: f16[1, 32] = native_group_norm_11[1]\n",
      "        getitem_35: f16[1, 32] = native_group_norm_11[2];  native_group_norm_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_10: f16[1, 512, 128, 128] = torch.ops.aten.silu.default(getitem_33);  getitem_33 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution_13: f16[1, 512, 128, 128] = torch.ops.aten.convolution.default(silu_10, arg58_1, arg59_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_10 = arg58_1 = arg59_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_12 = torch.ops.aten.native_group_norm.default(convolution_13, arg60_1, arg61_1, 1, 512, 16384, 32, 1e-06);  convolution_13 = arg60_1 = arg61_1 = None\n",
      "        getitem_36: f16[1, 512, 128, 128] = native_group_norm_12[0]\n",
      "        getitem_37: f16[1, 32] = native_group_norm_12[1]\n",
      "        getitem_38: f16[1, 32] = native_group_norm_12[2];  native_group_norm_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_11: f16[1, 512, 128, 128] = torch.ops.aten.silu.default(getitem_36);  getitem_36 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_14: f16[1, 512, 128, 128] = torch.ops.aten.convolution.default(silu_11, arg62_1, arg63_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_11 = arg62_1 = arg63_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_9: f16[1, 512, 128, 128] = torch.ops.aten.add.Tensor(convolution_12, convolution_14);  convolution_12 = convolution_14 = None\n",
      "        div_6: f16[1, 512, 128, 128] = torch.ops.aten.div.Tensor(add_9, 1.0);  add_9 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm_13 = torch.ops.aten.native_group_norm.default(div_6, arg64_1, arg65_1, 1, 512, 16384, 32, 1e-06);  arg64_1 = arg65_1 = None\n",
      "        getitem_39: f16[1, 512, 128, 128] = native_group_norm_13[0]\n",
      "        getitem_40: f16[1, 32] = native_group_norm_13[1]\n",
      "        getitem_41: f16[1, 32] = native_group_norm_13[2];  native_group_norm_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_12: f16[1, 512, 128, 128] = torch.ops.aten.silu.default(getitem_39);  getitem_39 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution_15: f16[1, 512, 128, 128] = torch.ops.aten.convolution.default(silu_12, arg66_1, arg67_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_12 = arg66_1 = arg67_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_14 = torch.ops.aten.native_group_norm.default(convolution_15, arg68_1, arg69_1, 1, 512, 16384, 32, 1e-06);  convolution_15 = arg68_1 = arg69_1 = None\n",
      "        getitem_42: f16[1, 512, 128, 128] = native_group_norm_14[0]\n",
      "        getitem_43: f16[1, 32] = native_group_norm_14[1]\n",
      "        getitem_44: f16[1, 32] = native_group_norm_14[2];  native_group_norm_14 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_13: f16[1, 512, 128, 128] = torch.ops.aten.silu.default(getitem_42);  getitem_42 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_16: f16[1, 512, 128, 128] = torch.ops.aten.convolution.default(silu_13, arg70_1, arg71_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_13 = arg70_1 = arg71_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_10: f16[1, 512, 128, 128] = torch.ops.aten.add.Tensor(div_6, convolution_16);  div_6 = convolution_16 = None\n",
      "        div_7: f16[1, 512, 128, 128] = torch.ops.aten.div.Tensor(add_10, 1.0);  add_10 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm_15 = torch.ops.aten.native_group_norm.default(div_7, arg72_1, arg73_1, 1, 512, 16384, 32, 1e-06);  arg72_1 = arg73_1 = None\n",
      "        getitem_45: f16[1, 512, 128, 128] = native_group_norm_15[0]\n",
      "        getitem_46: f16[1, 32] = native_group_norm_15[1]\n",
      "        getitem_47: f16[1, 32] = native_group_norm_15[2];  native_group_norm_15 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_14: f16[1, 512, 128, 128] = torch.ops.aten.silu.default(getitem_45);  getitem_45 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution_17: f16[1, 512, 128, 128] = torch.ops.aten.convolution.default(silu_14, arg74_1, arg75_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_14 = arg74_1 = arg75_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_16 = torch.ops.aten.native_group_norm.default(convolution_17, arg76_1, arg77_1, 1, 512, 16384, 32, 1e-06);  convolution_17 = arg76_1 = arg77_1 = None\n",
      "        getitem_48: f16[1, 512, 128, 128] = native_group_norm_16[0]\n",
      "        getitem_49: f16[1, 32] = native_group_norm_16[1]\n",
      "        getitem_50: f16[1, 32] = native_group_norm_16[2];  native_group_norm_16 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_15: f16[1, 512, 128, 128] = torch.ops.aten.silu.default(getitem_48);  getitem_48 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_18: f16[1, 512, 128, 128] = torch.ops.aten.convolution.default(silu_15, arg78_1, arg79_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_15 = arg78_1 = arg79_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_11: f16[1, 512, 128, 128] = torch.ops.aten.add.Tensor(div_7, convolution_18);  div_7 = convolution_18 = None\n",
      "        div_8: f16[1, 512, 128, 128] = torch.ops.aten.div.Tensor(add_11, 1.0);  add_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:128, code: hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n",
      "        upsample_nearest2d_1: f16[1, 512, 256, 256] = torch.ops.aten.upsample_nearest2d.default(div_8, [256, 256], 2.0, 2.0);  div_8 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:139, code: hidden_states = self.conv(hidden_states)\n",
      "        convolution_19: f16[1, 512, 256, 256] = torch.ops.aten.convolution.default(upsample_nearest2d_1, arg80_1, arg81_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  upsample_nearest2d_1 = arg80_1 = arg81_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm_17 = torch.ops.aten.native_group_norm.default(convolution_19, arg82_1, arg83_1, 1, 512, 65536, 32, 1e-06);  arg82_1 = arg83_1 = None\n",
      "        getitem_51: f16[1, 512, 256, 256] = native_group_norm_17[0]\n",
      "        getitem_52: f16[1, 32] = native_group_norm_17[1]\n",
      "        getitem_53: f16[1, 32] = native_group_norm_17[2];  native_group_norm_17 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_16: f16[1, 512, 256, 256] = torch.ops.aten.silu.default(getitem_51);  getitem_51 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution_20: f16[1, 256, 256, 256] = torch.ops.aten.convolution.default(silu_16, arg84_1, arg85_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_16 = arg84_1 = arg85_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_18 = torch.ops.aten.native_group_norm.default(convolution_20, arg86_1, arg87_1, 1, 256, 65536, 32, 1e-06);  convolution_20 = arg86_1 = arg87_1 = None\n",
      "        getitem_54: f16[1, 256, 256, 256] = native_group_norm_18[0]\n",
      "        getitem_55: f16[1, 32] = native_group_norm_18[1]\n",
      "        getitem_56: f16[1, 32] = native_group_norm_18[2];  native_group_norm_18 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_17: f16[1, 256, 256, 256] = torch.ops.aten.silu.default(getitem_54);  getitem_54 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_21: f16[1, 256, 256, 256] = torch.ops.aten.convolution.default(silu_17, arg88_1, arg89_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_17 = arg88_1 = arg89_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:491, code: input_tensor = self.conv_shortcut(input_tensor)\n",
      "        convolution_22: f16[1, 256, 256, 256] = torch.ops.aten.convolution.default(convolution_19, arg90_1, arg91_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  convolution_19 = arg90_1 = arg91_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_12: f16[1, 256, 256, 256] = torch.ops.aten.add.Tensor(convolution_22, convolution_21);  convolution_22 = convolution_21 = None\n",
      "        div_9: f16[1, 256, 256, 256] = torch.ops.aten.div.Tensor(add_12, 1.0);  add_12 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm_19 = torch.ops.aten.native_group_norm.default(div_9, arg92_1, arg93_1, 1, 256, 65536, 32, 1e-06);  arg92_1 = arg93_1 = None\n",
      "        getitem_57: f16[1, 256, 256, 256] = native_group_norm_19[0]\n",
      "        getitem_58: f16[1, 32] = native_group_norm_19[1]\n",
      "        getitem_59: f16[1, 32] = native_group_norm_19[2];  native_group_norm_19 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_18: f16[1, 256, 256, 256] = torch.ops.aten.silu.default(getitem_57);  getitem_57 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution_23: f16[1, 256, 256, 256] = torch.ops.aten.convolution.default(silu_18, arg94_1, arg95_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_18 = arg94_1 = arg95_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_20 = torch.ops.aten.native_group_norm.default(convolution_23, arg96_1, arg97_1, 1, 256, 65536, 32, 1e-06);  convolution_23 = arg96_1 = arg97_1 = None\n",
      "        getitem_60: f16[1, 256, 256, 256] = native_group_norm_20[0]\n",
      "        getitem_61: f16[1, 32] = native_group_norm_20[1]\n",
      "        getitem_62: f16[1, 32] = native_group_norm_20[2];  native_group_norm_20 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_19: f16[1, 256, 256, 256] = torch.ops.aten.silu.default(getitem_60);  getitem_60 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_24: f16[1, 256, 256, 256] = torch.ops.aten.convolution.default(silu_19, arg98_1, arg99_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_19 = arg98_1 = arg99_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_13: f16[1, 256, 256, 256] = torch.ops.aten.add.Tensor(div_9, convolution_24);  div_9 = convolution_24 = None\n",
      "        div_10: f16[1, 256, 256, 256] = torch.ops.aten.div.Tensor(add_13, 1.0);  add_13 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm_21 = torch.ops.aten.native_group_norm.default(div_10, arg100_1, arg101_1, 1, 256, 65536, 32, 1e-06);  arg100_1 = arg101_1 = None\n",
      "        getitem_63: f16[1, 256, 256, 256] = native_group_norm_21[0]\n",
      "        getitem_64: f16[1, 32] = native_group_norm_21[1]\n",
      "        getitem_65: f16[1, 32] = native_group_norm_21[2];  native_group_norm_21 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_20: f16[1, 256, 256, 256] = torch.ops.aten.silu.default(getitem_63);  getitem_63 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution_25: f16[1, 256, 256, 256] = torch.ops.aten.convolution.default(silu_20, arg102_1, arg103_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_20 = arg102_1 = arg103_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_22 = torch.ops.aten.native_group_norm.default(convolution_25, arg104_1, arg105_1, 1, 256, 65536, 32, 1e-06);  convolution_25 = arg104_1 = arg105_1 = None\n",
      "        getitem_66: f16[1, 256, 256, 256] = native_group_norm_22[0]\n",
      "        getitem_67: f16[1, 32] = native_group_norm_22[1]\n",
      "        getitem_68: f16[1, 32] = native_group_norm_22[2];  native_group_norm_22 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_21: f16[1, 256, 256, 256] = torch.ops.aten.silu.default(getitem_66);  getitem_66 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_26: f16[1, 256, 256, 256] = torch.ops.aten.convolution.default(silu_21, arg106_1, arg107_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_21 = arg106_1 = arg107_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_14: f16[1, 256, 256, 256] = torch.ops.aten.add.Tensor(div_10, convolution_26);  div_10 = convolution_26 = None\n",
      "        div_11: f16[1, 256, 256, 256] = torch.ops.aten.div.Tensor(add_14, 1.0);  add_14 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:128, code: hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n",
      "        upsample_nearest2d_2: f16[1, 256, 512, 512] = torch.ops.aten.upsample_nearest2d.default(div_11, [512, 512], 2.0, 2.0);  div_11 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:139, code: hidden_states = self.conv(hidden_states)\n",
      "        convolution_27: f16[1, 256, 512, 512] = torch.ops.aten.convolution.default(upsample_nearest2d_2, arg108_1, arg109_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  upsample_nearest2d_2 = arg108_1 = arg109_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm_23 = torch.ops.aten.native_group_norm.default(convolution_27, arg110_1, arg111_1, 1, 256, 262144, 32, 1e-06);  arg110_1 = arg111_1 = None\n",
      "        getitem_69: f16[1, 256, 512, 512] = native_group_norm_23[0]\n",
      "        getitem_70: f16[1, 32] = native_group_norm_23[1]\n",
      "        getitem_71: f16[1, 32] = native_group_norm_23[2];  native_group_norm_23 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_22: f16[1, 256, 512, 512] = torch.ops.aten.silu.default(getitem_69);  getitem_69 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution_28: f16[1, 128, 512, 512] = torch.ops.aten.convolution.default(silu_22, arg112_1, arg113_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_22 = arg112_1 = arg113_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_24 = torch.ops.aten.native_group_norm.default(convolution_28, arg114_1, arg115_1, 1, 128, 262144, 32, 1e-06);  convolution_28 = arg114_1 = arg115_1 = None\n",
      "        getitem_72: f16[1, 128, 512, 512] = native_group_norm_24[0]\n",
      "        getitem_73: f16[1, 32] = native_group_norm_24[1]\n",
      "        getitem_74: f16[1, 32] = native_group_norm_24[2];  native_group_norm_24 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_23: f16[1, 128, 512, 512] = torch.ops.aten.silu.default(getitem_72);  getitem_72 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_29: f16[1, 128, 512, 512] = torch.ops.aten.convolution.default(silu_23, arg116_1, arg117_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_23 = arg116_1 = arg117_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:491, code: input_tensor = self.conv_shortcut(input_tensor)\n",
      "        convolution_30: f16[1, 128, 512, 512] = torch.ops.aten.convolution.default(convolution_27, arg118_1, arg119_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  convolution_27 = arg118_1 = arg119_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_15: f16[1, 128, 512, 512] = torch.ops.aten.add.Tensor(convolution_30, convolution_29);  convolution_30 = convolution_29 = None\n",
      "        div_12: f16[1, 128, 512, 512] = torch.ops.aten.div.Tensor(add_15, 1.0);  add_15 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm_25 = torch.ops.aten.native_group_norm.default(div_12, arg120_1, arg121_1, 1, 128, 262144, 32, 1e-06);  arg120_1 = arg121_1 = None\n",
      "        getitem_75: f16[1, 128, 512, 512] = native_group_norm_25[0]\n",
      "        getitem_76: f16[1, 32] = native_group_norm_25[1]\n",
      "        getitem_77: f16[1, 32] = native_group_norm_25[2];  native_group_norm_25 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_24: f16[1, 128, 512, 512] = torch.ops.aten.silu.default(getitem_75);  getitem_75 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution_31: f16[1, 128, 512, 512] = torch.ops.aten.convolution.default(silu_24, arg122_1, arg123_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_24 = arg122_1 = arg123_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_26 = torch.ops.aten.native_group_norm.default(convolution_31, arg124_1, arg125_1, 1, 128, 262144, 32, 1e-06);  convolution_31 = arg124_1 = arg125_1 = None\n",
      "        getitem_78: f16[1, 128, 512, 512] = native_group_norm_26[0]\n",
      "        getitem_79: f16[1, 32] = native_group_norm_26[1]\n",
      "        getitem_80: f16[1, 32] = native_group_norm_26[2];  native_group_norm_26 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_25: f16[1, 128, 512, 512] = torch.ops.aten.silu.default(getitem_78);  getitem_78 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_32: f16[1, 128, 512, 512] = torch.ops.aten.convolution.default(silu_25, arg126_1, arg127_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_25 = arg126_1 = arg127_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_16: f16[1, 128, 512, 512] = torch.ops.aten.add.Tensor(div_12, convolution_32);  div_12 = convolution_32 = None\n",
      "        div_13: f16[1, 128, 512, 512] = torch.ops.aten.div.Tensor(add_16, 1.0);  add_16 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:457, code: hidden_states = self.norm1(hidden_states)\n",
      "        native_group_norm_27 = torch.ops.aten.native_group_norm.default(div_13, arg128_1, arg129_1, 1, 128, 262144, 32, 1e-06);  arg128_1 = arg129_1 = None\n",
      "        getitem_81: f16[1, 128, 512, 512] = native_group_norm_27[0]\n",
      "        getitem_82: f16[1, 32] = native_group_norm_27[1]\n",
      "        getitem_83: f16[1, 32] = native_group_norm_27[2];  native_group_norm_27 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:458, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_26: f16[1, 128, 512, 512] = torch.ops.aten.silu.default(getitem_81);  getitem_81 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:471, code: hidden_states = self.conv1(hidden_states)\n",
      "        convolution_33: f16[1, 128, 512, 512] = torch.ops.aten.convolution.default(silu_26, arg130_1, arg131_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_26 = arg130_1 = arg131_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:479, code: hidden_states = self.norm2(hidden_states)\n",
      "        native_group_norm_28 = torch.ops.aten.native_group_norm.default(convolution_33, arg132_1, arg133_1, 1, 128, 262144, 32, 1e-06);  convolution_33 = arg132_1 = arg133_1 = None\n",
      "        getitem_84: f16[1, 128, 512, 512] = native_group_norm_28[0]\n",
      "        getitem_85: f16[1, 32] = native_group_norm_28[1]\n",
      "        getitem_86: f16[1, 32] = native_group_norm_28[2];  native_group_norm_28 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:485, code: hidden_states = self.nonlinearity(hidden_states)\n",
      "        silu_27: f16[1, 128, 512, 512] = torch.ops.aten.silu.default(getitem_84);  getitem_84 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:488, code: hidden_states = self.conv2(hidden_states)\n",
      "        convolution_34: f16[1, 128, 512, 512] = torch.ops.aten.convolution.default(silu_27, arg134_1, arg135_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_27 = arg134_1 = arg135_1 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/resnet.py:493, code: output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
      "        add_17: f16[1, 128, 512, 512] = torch.ops.aten.add.Tensor(div_13, convolution_34);  div_13 = convolution_34 = None\n",
      "        div_14: f16[1, 128, 512, 512] = torch.ops.aten.div.Tensor(add_17, 1.0);  add_17 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/vae.py:220, code: sample = self.conv_norm_out(sample)\n",
      "        native_group_norm_29 = torch.ops.aten.native_group_norm.default(div_14, arg136_1, arg137_1, 1, 128, 262144, 32, 1e-06);  div_14 = arg136_1 = arg137_1 = None\n",
      "        getitem_87: f16[1, 128, 512, 512] = native_group_norm_29[0]\n",
      "        getitem_88: f16[1, 32] = native_group_norm_29[1]\n",
      "        getitem_89: f16[1, 32] = native_group_norm_29[2];  native_group_norm_29 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/vae.py:221, code: sample = self.conv_act(sample)\n",
      "        silu_28: f16[1, 128, 512, 512] = torch.ops.aten.silu.default(getitem_87);  getitem_87 = None\n",
      "        \n",
      "        # File: /home/xmo/miniconda3/lib/python3.10/site-packages/diffusers/models/vae.py:222, code: sample = self.conv_out(sample)\n",
      "        convolution_35: f16[1, 3, 512, 512] = torch.ops.aten.convolution.default(silu_28, arg138_1, arg139_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  silu_28 = arg138_1 = arg139_1 = None\n",
      "        return (convolution_35,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmo/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1026: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "_ = inference_func(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58eda91e008c55df308adf105736f42b9725f2104dfc205705e3d94c4f08ad92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
